{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexrosulek/Cs50/blob/main/makearticle3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://ollama.ai/install.sh | sh\n",
        "\n",
        "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "!sudo apt-get update && sudo apt-get install -y cuda-drivers\n",
        "\n",
        "import os\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "!nohup ollama serve &\n",
        "\n",
        "\n",
        "\n",
        "!ollama pull gemma3:1b\n",
        "!ollama pull gemma3:4b\n",
        "!pip install ollama\n",
        "!pip install crawl4ai\n",
        "!pip install aiohttp\n",
        "!pip install pillow\n",
        "!pip install beautifulsoup4\n",
        "!pip install wikipedia\n",
        "!pip install googlesearch-python\n",
        "!pip install playwright\n",
        "!playwright install chromium\n",
        "!nohup ollama serve &\n",
        "\n",
        "import asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\n",
        "from urllib.parse import urlparse\n",
        "import asyncio\n",
        "import random\n",
        "import time\n",
        "import aiohttp\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import base64\n",
        "import io\n",
        "import subprocess\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import wikipedia\n",
        "\n",
        "import textwrap\n",
        "\n",
        "import re\n",
        "import json\n",
        "from googlesearch import search"
      ],
      "metadata": {
        "id": "-b9oppEb4cm2",
        "outputId": "87ea43fb-5e50-4b76-d42c-b5ee85bf8fde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13281    0 13281    0     0  39886      0 --:--:-- --:--:-- --:--:-- 40003\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,704 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,844 kB]\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,872 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,155 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [46.3 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,244 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,543 kB]\n",
            "Fetched 20.8 MB in 2s (9,085 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cpp-12 cuda-drivers-570 dctrl-tools dkms fakeroot gcc-12\n",
            "  keyboard-configuration libasan8 libfakeroot libgail-common libgail18\n",
            "  libgcc-12-dev libgtk2.0-0 libgtk2.0-bin libgtk2.0-common libgudev-1.0-0\n",
            "  libjansson4 liblocale-gettext-perl libnvidia-cfg1-570 libnvidia-common-570\n",
            "  libnvidia-compute-570 libnvidia-decode-570 libnvidia-encode-570\n",
            "  libnvidia-extra-570 libnvidia-fbc1-570 libnvidia-gl-570 librsvg2-common\n",
            "  libtsan2 libudev1 libxcvt0 nvidia-compute-utils-570 nvidia-dkms-570\n",
            "  nvidia-driver-570 nvidia-firmware-570-570.133.20 nvidia-kernel-common-570\n",
            "  nvidia-kernel-source-570 nvidia-modprobe nvidia-settings nvidia-utils-570\n",
            "  python3-xkit screen-resolution-extra switcheroo-control systemd-hwe-hwdb\n",
            "  udev xcvt xserver-xorg-core xserver-xorg-video-nvidia-570\n",
            "Suggested packages:\n",
            "  gcc-12-locales cpp-12-doc debtags menu gcc-12-multilib gcc-12-doc gvfs\n",
            "  xfonts-100dpi | xfonts-75dpi xfonts-scalable\n",
            "Recommended packages:\n",
            "  libnvidia-compute-570:i386 libnvidia-decode-570:i386\n",
            "  libnvidia-encode-570:i386 libnvidia-fbc1-570:i386 libnvidia-gl-570:i386\n",
            "The following NEW packages will be installed:\n",
            "  cpp-12 cuda-drivers cuda-drivers-570 dctrl-tools dkms fakeroot gcc-12\n",
            "  keyboard-configuration libasan8 libfakeroot libgail-common libgail18\n",
            "  libgcc-12-dev libgtk2.0-0 libgtk2.0-bin libgtk2.0-common libgudev-1.0-0\n",
            "  libjansson4 liblocale-gettext-perl libnvidia-cfg1-570 libnvidia-common-570\n",
            "  libnvidia-compute-570 libnvidia-decode-570 libnvidia-encode-570\n",
            "  libnvidia-extra-570 libnvidia-fbc1-570 libnvidia-gl-570 librsvg2-common\n",
            "  libtsan2 libxcvt0 nvidia-compute-utils-570 nvidia-dkms-570 nvidia-driver-570\n",
            "  nvidia-firmware-570-570.133.20 nvidia-kernel-common-570\n",
            "  nvidia-kernel-source-570 nvidia-modprobe nvidia-settings nvidia-utils-570\n",
            "  python3-xkit screen-resolution-extra switcheroo-control systemd-hwe-hwdb\n",
            "  udev xcvt xserver-xorg-core xserver-xorg-video-nvidia-570\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 47 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 397 MB of archives.\n",
            "After this operation, 1,168 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-common-570 570.133.20-0ubuntu1 [15.9 kB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-gl-570 570.133.20-0ubuntu1 [162 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblocale-gettext-perl amd64 1.07-4build3 [17.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 keyboard-configuration all 1.205ubuntu3 [206 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 cpp-12 amd64 12.3.0-1ubuntu1~22.04 [10.8 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libasan8 amd64 12.3.0-1ubuntu1~22.04 [2,442 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtsan2 amd64 12.3.0-1ubuntu1~22.04 [2,477 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgcc-12-dev amd64 12.3.0-1ubuntu1~22.04 [2,618 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gcc-12 amd64 12.3.0-1ubuntu1~22.04 [21.7 MB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-kernel-source-570 570.133.20-0ubuntu1 [73.4 MB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 dctrl-tools amd64 2.24-3build2 [66.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 dkms all 2.8.7-2ubuntu2.2 [70.1 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.15 [76.6 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.15 [1,557 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjansson4 amd64 2.13.1-1.1build3 [32.4 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcvt0 amd64 0.1.1-3 [5,494 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-xorg-core amd64 2:21.1.4-2ubuntu1.7~22.04.14 [1,478 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfakeroot amd64 1.28-1ubuntu1 [31.5 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 fakeroot amd64 1.28-1ubuntu1 [60.4 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2.1 [125 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2.1 [2,038 kB]\n",
            "Get:22 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-firmware-570-570.133.20 570.133.20-0ubuntu1 [64.6 MB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail18 amd64 2.24.33-2ubuntu2.1 [15.9 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail-common amd64 2.24.33-2ubuntu2.1 [132 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2.1 [7,936 B]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgudev-1.0-0 amd64 1:237-2build1 [16.3 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-xkit all 0.5.0ubuntu5 [18.5 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 screen-resolution-extra all 0.18.2 [4,396 B]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 switcheroo-control amd64 2.4-3build2 [16.5 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 xcvt amd64 0.1.1-3 [7,140 B]\n",
            "Get:33 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-modprobe 570.133.20-0ubuntu1 [14.9 kB]\n",
            "Get:34 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-kernel-common-570 570.133.20-0ubuntu1 [98.6 kB]\n",
            "Get:35 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-dkms-570 570.133.20-0ubuntu1 [14.9 kB]\n",
            "Get:36 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-decode-570 570.133.20-0ubuntu1 [2,434 kB]\n",
            "Get:37 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-compute-570 570.133.20-0ubuntu1 [44.1 MB]\n",
            "Get:38 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-extra-570 570.133.20-0ubuntu1 [72.7 kB]\n",
            "Get:39 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-compute-utils-570 570.133.20-0ubuntu1 [109 kB]\n",
            "Get:40 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-encode-570 570.133.20-0ubuntu1 [105 kB]\n",
            "Get:41 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-utils-570 570.133.20-0ubuntu1 [522 kB]\n",
            "Get:42 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-cfg1-570 570.133.20-0ubuntu1 [145 kB]\n",
            "Get:43 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  xserver-xorg-video-nvidia-570 570.133.20-0ubuntu1 [1,695 kB]\n",
            "Get:44 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  libnvidia-fbc1-570 570.133.20-0ubuntu1 [98.0 kB]\n",
            "Get:45 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-driver-570 570.133.20-0ubuntu1 [492 kB]\n",
            "Get:46 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-drivers-570 570.133.20-0ubuntu1 [2,550 B]\n",
            "Get:47 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  cuda-drivers 570.133.20-0ubuntu1 [2,498 B]\n",
            "Get:48 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nvidia-settings 570.133.20-0ubuntu1 [959 kB]\n",
            "Fetched 397 MB in 7s (56.0 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package liblocale-gettext-perl.\n",
            "(Reading database ... 126101 files and directories currently installed.)\n",
            "Preparing to unpack .../0-liblocale-gettext-perl_1.07-4build3_amd64.deb ...\n",
            "Unpacking liblocale-gettext-perl (1.07-4build3) ...\n",
            "Selecting previously unselected package keyboard-configuration.\n",
            "Preparing to unpack .../1-keyboard-configuration_1.205ubuntu3_all.deb ...\n",
            "Unpacking keyboard-configuration (1.205ubuntu3) ...\n",
            "Selecting previously unselected package cpp-12.\n",
            "Preparing to unpack .../2-cpp-12_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking cpp-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libasan8:amd64.\n",
            "Preparing to unpack .../3-libasan8_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libasan8:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libtsan2:amd64.\n",
            "Preparing to unpack .../4-libtsan2_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libtsan2:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package libgcc-12-dev:amd64.\n",
            "Preparing to unpack .../5-libgcc-12-dev_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking libgcc-12-dev:amd64 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package gcc-12.\n",
            "Preparing to unpack .../6-gcc-12_12.3.0-1ubuntu1~22.04_amd64.deb ...\n",
            "Unpacking gcc-12 (12.3.0-1ubuntu1~22.04) ...\n",
            "Selecting previously unselected package dctrl-tools.\n",
            "Preparing to unpack .../7-dctrl-tools_2.24-3build2_amd64.deb ...\n",
            "Unpacking dctrl-tools (2.24-3build2) ...\n",
            "Selecting previously unselected package dkms.\n",
            "Preparing to unpack .../8-dkms_2.8.7-2ubuntu2.2_all.deb ...\n",
            "Unpacking dkms (2.8.7-2ubuntu2.2) ...\n",
            "Preparing to unpack .../9-libudev1_249.11-0ubuntu3.15_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.15) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.15) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 126461 files and directories currently installed.)\n",
            "Preparing to unpack .../00-udev_249.11-0ubuntu3.15_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.15) ...\n",
            "Selecting previously unselected package libjansson4:amd64.\n",
            "Preparing to unpack .../01-libjansson4_2.13.1-1.1build3_amd64.deb ...\n",
            "Unpacking libjansson4:amd64 (2.13.1-1.1build3) ...\n",
            "Selecting previously unselected package libnvidia-common-570.\n",
            "Preparing to unpack .../02-libnvidia-common-570_570.133.20-0ubuntu1_all.deb ...\n",
            "Unpacking libnvidia-common-570 (570.133.20-0ubuntu1) ...\n",
            "Selecting previously unselected package libnvidia-gl-570:amd64.\n",
            "Preparing to unpack .../03-libnvidia-gl-570_570.133.20-0ubuntu1_amd64.deb ...\n",
            "dpkg-query: no packages found matching libnvidia-gl-535\n",
            "Unpacking libnvidia-gl-570:amd64 (570.133.20-0ubuntu1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "folder = \"downloaded_images\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "def is_valid_phone(value: str) -> bool:\n",
        "    print(value)\n",
        "    return bool(re.fullmatch(r\"\\d{3}-\\d{3}-\\d{4}\", value.strip()))\n",
        "\n",
        "def is_valid_email(value: str) -> bool:\n",
        "    print(value)\n",
        "    return bool(re.fullmatch(r\"[^@\\s]+@[^@\\s]+\\.[a-zA-Z0-9]+\", value.strip()))\n",
        "\n",
        "def is_valid_url(value: str) -> bool:\n",
        "    print(value)\n",
        "    return value.strip().startswith(\"http\")\n",
        "\n",
        "def is_valid_json(value: str) -> bool:\n",
        "    print(value)\n",
        "    try:\n",
        "        json.loads(value)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def is_non_empty_string(value: str) -> bool:\n",
        "    print(value)\n",
        "    return isinstance(value, str) and len(value.strip()) > 0\n",
        "\n",
        "def is_valid_dict(value) -> bool:\n",
        "    print(value)\n",
        "    return isinstance(value, dict)\n",
        "FIELD_VALIDATORS = {\n",
        "\n",
        "    \"extract_holiday_hours\": is_valid_dict,\n",
        "    \"extract_categories\": is_valid_json,\n",
        "\n",
        "    \"extract_social_media\": is_valid_json,\n",
        "    \"extract_stocked_brands\": is_valid_json,\n",
        "    \"extract_inventory_categories\": is_valid_json,\n",
        "    \"extract_customer_reviews\": is_valid_json,\n",
        "\n",
        "}\n",
        "FIELD_EXTRACTORS = {\n",
        "\n",
        "        \"extract_holiday_hours\": \"From the content, extract holiday hours in this format: {'2024-12-25': 'Closed', '2024-12-31': '10:00 AM - 4:00 PM'}\\nOnly return DICTIONARY, no extra text, prefixes,  marks or quotes, only the dictionary itself. no `, nothing before or after dictionary, ONLY VALID DICTIONARY.\",\n",
        "        \"extract_categories\": \"From the content, extract categories in this format: ['Thrift Store', 'Charity']\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself. no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "           \"extract_social_media\": \"From the content, extract social media links in this format: {'facebook': 'url', 'instagram': 'url'}\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself. no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_stocked_brands\": \"From the content, extract stocked brands in this format: ['Nike', 'Adidas']\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself. no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_inventory_categories\": \"From the content, extract inventory categories in this format: {'Apparel': ['Shirts', 'Hoodies']}\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself.  no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_customer_reviews\": \"From the content, extract customer reviews in this format: [{'user'': 'John', 'comment': 'Great!', 'rating': 5}]\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself. no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "       }\n",
        "import random\n",
        "import wikipedia\n",
        "\n",
        "async def handle_wikipedia(task):\n",
        "    try:\n",
        "        name = task['target'].get(\"name\")\n",
        "        city = task['target'].get(\"city\", \"\")\n",
        "        query = f\"{name} {city}\".strip()\n",
        "        print(f\"📚 Wikipedia lookup for: {query}\")\n",
        "\n",
        "        page = wikipedia.page(query, auto_suggest=True)\n",
        "        content = page.content\n",
        "\n",
        "        # If short, return all\n",
        "        if len(content) <= 2000:\n",
        "            return content\n",
        "\n",
        "        # Otherwise, chunk and pick 4 from middle\n",
        "        chunk_size = 500\n",
        "        chunks = [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]\n",
        "\n",
        "        if len(chunks) <= 6:\n",
        "            selected_chunks = chunks  # Not enough to skip ends\n",
        "        else:\n",
        "            middle_chunks = chunks[2:-2]\n",
        "            selected_chunks = random.sample(middle_chunks, min(4, len(middle_chunks)))\n",
        "\n",
        "        return \"\\n\\n\".join(selected_chunks)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Wikipedia fetch failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "async def handle_google_search(task, crawler):\n",
        "    try:\n",
        "        target = task['target']\n",
        "        query = f\"{target.get('name', '')} {target.get('city', '')} {target.get('state', '')}\".strip()\n",
        "        print(query)\n",
        "        valid_results = ''\n",
        "\n",
        "        results = list(search(query))\n",
        "\n",
        "        # Randomly shuffle and pick half\n",
        "        half_results = random.sample(results, max(1, len(results) // 2))\n",
        "\n",
        "\n",
        "\n",
        "        for url in half_results:\n",
        "            try:\n",
        "                res = requests.get(url, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "                soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "                title = soup.title.string.strip() if soup.title else \"No title\"\n",
        "                meta_tag = soup.find(\"meta\", {\"name\": \"description\"})\n",
        "                description = meta_tag[\"content\"].strip() if meta_tag else \"No description\"\n",
        "\n",
        "                prompt = (\n",
        "                    f\"Is this page about the query '{query}'? Be very strict, no random things this is for a specific thrift store.\\n\\n\"\n",
        "                    f\"Title: {title}\\n\"\n",
        "                    f\"Description: {description}\\n\\n\"\n",
        "                    f\"Return true or false.\"\n",
        "                )\n",
        "\n",
        "                print(\"🔍 running ollama for relevance check\")\n",
        "                result = run_ollama1(prompt)\n",
        "                if \"true\" in result.lower():\n",
        "                    print(f\"✅ Relevant: {url}\")\n",
        "                    content = await getsite(url, crawler)\n",
        "                    if content and content.success:\n",
        "                        markdown = content.markdown.fit_markdown\n",
        "                        if len(markdown) <= 1500:\n",
        "                            valid_results += markdown\n",
        "\n",
        "                        # Chunk and pick 4 from the middle\n",
        "                        chunk_size = 500\n",
        "                        chunks = [markdown[i:i + chunk_size] for i in range(0, len(markdown), chunk_size)]\n",
        "                        if len(chunks) <= 6:\n",
        "                            selected_chunks = chunks\n",
        "                        else:\n",
        "                            selected_chunks = random.sample(chunks[2:-2], min(4, len(chunks[2:-2])))\n",
        "\n",
        "                        valid_results += \"\\n\\n\".join(selected_chunks) + f\"\\n\\n<- EXTRACTED FROM THIS URL: {url}\\n\\n\"\n",
        "            except Exception as inner_err:\n",
        "                print(f\"⚠️ Error processing {url}: {inner_err}\")\n",
        "\n",
        "        if valid_results != '':\n",
        "            return valid_results\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in handle_google_search: {e}\")\n",
        "        return None\n",
        "\n",
        "async def search2(task, crawler):\n",
        "    print(\"getting google results\")\n",
        "    google_results = await handle_google_search(task, crawler)\n",
        "    print(\"getting wiki results\")\n",
        "    wiki_results = await handle_wikipedia(task)\n",
        "\n",
        "    imageandalt = None\n",
        "    result = None\n",
        "    images = None\n",
        "\n",
        "    print(\"done searching\")\n",
        "    mainstring = ''\n",
        "    if google_results:\n",
        "        print(\"got some google results\")\n",
        "        mainstring = google_results\n",
        "        if wiki_results:\n",
        "            print(\"got some wiki results\")\n",
        "            mainstring += wiki_results\n",
        "    if result:\n",
        "        if result.success:\n",
        "\n",
        "            mainstring += result.markdown.fit_markdown\n",
        "    if mainstring == '' or len(mainstring) < 500:\n",
        "        if not imageandalt:\n",
        "            return False, None, None\n",
        "        return True, None, imageandalt\n",
        "    return True, mainstring, imageandalt\n",
        "def resize_image(input_path, size):\n",
        "    try:\n",
        "        img = Image.open(input_path).convert(\"RGB\")\n",
        "        img = img.resize(size, Image.ANTIALIAS)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Resize failed for {input_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def image_to_base64(img: Image.Image) -> str:\n",
        "    try:\n",
        "        buffer = io.BytesIO()\n",
        "        img.save(buffer, format=\"JPEG\")\n",
        "        return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ base64: {e}\")\n",
        "        return None\n",
        "\n",
        "def run_ollama_with_image(prompt: str, image_base64: str, model=\"gemma3:4b\") -> str:\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            \"http://localhost:11434/api/generate\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"prompt\": prompt,\n",
        "                \"images\": [image_base64],\n",
        "                \"stream\": False\n",
        "            },\n",
        "            timeout=256\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        return result.get(\"response\", \"\").strip().lower()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ollama API call failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def run_ollama4(prompt, model=\"gemma3:4b\"):\n",
        "    print(f\"🧠 Running Ollama with model={model}\")\n",
        "    try:\n",
        "        proc = subprocess.run(\n",
        "            [\"ollama\", \"run\", model],\n",
        "            input=prompt.encode(\"utf-8\"),\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            timeout=556\n",
        "        )\n",
        "        stdout = proc.stdout.decode(\"utf-8\").strip()\n",
        "        stderr = proc.stderr.decode(\"utf-8\").strip()\n",
        "        print(f\"Ollama stdout:\\n{stdout}\")\n",
        "        if stderr:\n",
        "            print(f\"Ollama stderr:\\n{stderr}\")\n",
        "        return stdout\n",
        "    except Exception as e:\n",
        "        print(f\"Ollama run failed: {e}\")\n",
        "        return \"\"\n",
        "def run_ollama1(prompt, model=\"gemma3:1b\"):\n",
        "    print(f\"🧠 Running Ollama with model={model}\")\n",
        "    try:\n",
        "        proc = subprocess.run(\n",
        "            [\"ollama\", \"run\", model],\n",
        "            input=prompt.encode(\"utf-8\"),\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            timeout=356\n",
        "        )\n",
        "        stdout = proc.stdout.decode(\"utf-8\").strip()\n",
        "        stderr = proc.stderr.decode(\"utf-8\").strip()\n",
        "        print(f\"Ollama stdout:\\n{stdout}\")\n",
        "        return stdout\n",
        "    except Exception as e:\n",
        "        print(f\"Ollama run failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def validate_images(name,images: list[dict]) -> list[dict] | None:\n",
        "    \"\"\"\n",
        "    Takes [{'filename': ..., 'alt': ...}]\n",
        "    Returns [{'alt': ..., 'base64': ...}] — resized to 1536x1024\n",
        "    Returns None on error.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "\n",
        "        approved = []\n",
        "\n",
        "        for img_info in images:\n",
        "            filename = img_info[\"filename\"]\n",
        "            alt_text = img_info.get(\"alt\") or \"No description\"\n",
        "\n",
        "            # Step 1: Resize to 512x512 for judging\n",
        "            resized_512 = resize_image(filename, (512, 512))\n",
        "            if not resized_512:\n",
        "                continue\n",
        "\n",
        "            img_512_b64 = image_to_base64(resized_512)\n",
        "            if not img_512_b64:\n",
        "                continue\n",
        "\n",
        "            prompt = (\n",
        "                f\"Based on the description: \\\"{alt_text}\\\", \"\n",
        "                f\"is this image good for thrift store {name}? Answer only true or false.\"\n",
        "            )\n",
        "\n",
        "            result = run_ollama_with_image(prompt, img_512_b64)\n",
        "            if \"true\" not in result:\n",
        "                print(f\"🛑 Image rejected: {filename}\")\n",
        "                continue\n",
        "\n",
        "            # Step 2: Resize to 1536x1024 for output\n",
        "            final_img = resize_image(filename, (1536, 1024))\n",
        "            if not final_img:\n",
        "                continue\n",
        "\n",
        "            encoded_final = image_to_base64(final_img)\n",
        "            approved.append({\n",
        "                \"alt\": alt_text,\n",
        "                \"base64\": encoded_final\n",
        "            })\n",
        "\n",
        "        if os.path.exists(folder):\n",
        "            shutil.rmtree(folder)\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "        return approved\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        if os.path.exists(folder):\n",
        "            shutil.rmtree(folder)\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "        print(f\"❌ Error in validate_images: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def normalize_and_validate_url(url: str) -> str | None:\n",
        "    try:\n",
        "        # Normalize: ensure https://\n",
        "        if not url.startswith(\"https://\"):\n",
        "            if url.startswith(\"http://\"):\n",
        "                url = url.replace(\"http://\", \"https://\", 1)\n",
        "            else:\n",
        "                url = \"https://\" + url.lstrip(\"/\")\n",
        "\n",
        "        # Validate\n",
        "        parsed = urlparse(url)\n",
        "        if not parsed.scheme or not parsed.netloc:\n",
        "            return None  # Invalid URL\n",
        "\n",
        "        return url\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error normalizing/validating {url}: {e}\")\n",
        "        return None\n",
        "class CrawlerManager:\n",
        "    def __init__(self):\n",
        "        self.crawler = None\n",
        "\n",
        "    async def start(self):\n",
        "        if self.crawler is None:\n",
        "            self.crawler = AsyncWebCrawler(config=BrowserConfig())\n",
        "            await self.crawler.__aenter__()\n",
        "\n",
        "    async def stop(self):\n",
        "        if self.crawler:\n",
        "            await self.crawler.__aexit__(None, None, None)\n",
        "            self.crawler = None\n",
        "\n",
        "    async def crawl(self, url: str):\n",
        "        if not self.crawler:\n",
        "            raise RuntimeError(\"Crawler not started\")\n",
        "\n",
        "        url = normalize_and_validate_url(url)\n",
        "        if not url:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            run_config = CrawlerRunConfig()\n",
        "            result = await self.crawler.arun(url=url, config=run_config)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"❌ crawl error for {url}: {e}\")\n",
        "            return None\n",
        "async def getsite(url: str, crawler_mgr: CrawlerManager):\n",
        "    return await crawler_mgr.crawl(url)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "async def download_image(session, img, idx):\n",
        "\n",
        "    try:\n",
        "        url = img[\"src\"]\n",
        "        alt = img.get(\"alt\") or img.get(\"desc\", \"No alt/desc\")\n",
        "        ext = url.split('.')[-1].split('?')[0]\n",
        "        filename = f\"downloaded_images/image_{idx}.{ext}\"\n",
        "\n",
        "        async with session.get(url) as resp:\n",
        "            if resp.status == 200:\n",
        "                with open(filename, 'wb') as f:\n",
        "                    f.write(await resp.read())\n",
        "                print(f\"✅ Downloaded: {filename}\")\n",
        "                return {\"filename\": filename, \"alt\": alt}\n",
        "            else:\n",
        "                print(f\"❌ Failed to download {url} (status {resp.status})\")\n",
        "                return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error downloading {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "async def findimages(result):\n",
        "    try:\n",
        "\n",
        "        images = result.media.get(\"images\", [])\n",
        "        if not images:\n",
        "            print(\"No images found.\")\n",
        "            return None\n",
        "\n",
        "        # Pick up to 8 random images\n",
        "        selected_images = random.sample(images, min(8, len(images)))\n",
        "\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            tasks = [\n",
        "                download_image(session, img, idx)\n",
        "                for idx, img in enumerate(selected_images)\n",
        "            ]\n",
        "            results = await asyncio.gather(*tasks)\n",
        "\n",
        "        # Filter out failed downloads\n",
        "        successful = [r for r in results if r]\n",
        "\n",
        "        return successful\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error finding images: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def summarize_large_text(prompt, large_text: str, max_length: int = 4000) -> str | None:\n",
        "    try:\n",
        "        # Split large text into chunks (~1024 chars each)\n",
        "        chunks = textwrap.wrap(large_text, width=2000)\n",
        "        print(len(chunks), 'got this many chunks')\n",
        "        summaries = []\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            prompt = f\"Extract thrift store info. keep facts, keep details about thrift store {prompt}, keep operating hours ect, all details, REMOVE HTML AND JS, u are not doing anything else but extracting/consolidating:\\n\\n{chunk}\"\n",
        "            summary = run_ollama1(prompt)\n",
        "            if summary:\n",
        "                summaries.append(summary)\n",
        "            else:\n",
        "                print(f\"⚠️ Failed to summarize chunk {idx}\")\n",
        "\n",
        "        # Aggregate all summaries\n",
        "        combined = \"\".join(summaries)\n",
        "\n",
        "        if len(combined) > max_length:\n",
        "            print(\"summarizing again\")\n",
        "            chunks = textwrap.wrap(combined, width=2000)\n",
        "\n",
        "            summaries = []\n",
        "            for idx, chunk in enumerate(chunks):\n",
        "                prompt = f\"Extract thrift store info concisely, based on thrift store {prompt}, keep operating hours phone website ect all details, u are not doing anything else but extracting/consolidating, return it back do not tell me how good it is, no other objectives or questions:\\n\\n{chunk}\"\n",
        "\n",
        "                summary = run_ollama1(prompt)\n",
        "                if summary:\n",
        "                    summaries.append(summary)\n",
        "                else:\n",
        "                    print(f\"⚠️ Failed to summarize chunk {idx}\")\n",
        "\n",
        "        combined = \"\".join(summaries)\n",
        "        if len(combined) > max_length:\n",
        "\n",
        "            print(\"summarizing again\")\n",
        "            chunks = textwrap.wrap(combined, width=2000)\n",
        "\n",
        "            summaries = []\n",
        "            for idx, chunk in enumerate(chunks):\n",
        "                prompt = f\"Summarize shortly make shorter than it is. based on thrift store {prompt}, keep all details and facts, u are not doing anything else but summarizing/consolidating, return it back do not tell me how good it is, no other objectives or questions:\\n\\n{chunk}\"\n",
        "\n",
        "                summary = run_ollama1(prompt)\n",
        "                if summary:\n",
        "                    summaries.append(summary)\n",
        "                else:\n",
        "                    print(f\"⚠️ Failed to summarize chunk {idx}\")\n",
        "\n",
        "        combined = \"\\n\".join(summaries)\n",
        "        prompt = f\"You work for nearestdoor.com. This is scraped info from the internet based on thrift store {prompt}, keep all info/details, keep all information/facts/details operating hours phone website ect and turn it into an overview, no other objectives or questions, it is 2025:\\n\\n{combined}\"\n",
        "\n",
        "        summary = run_ollama4(prompt)\n",
        "        if check_if_aggregate_is_good(prompt,summary):\n",
        "            return True, summary\n",
        "        else:\n",
        "            return False, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in summarize_large_text: {e}\")\n",
        "        return False, None\n",
        "def create_aggregate_plan(aggregate: str) -> bool:\n",
        "    try:\n",
        "        prompt = (\n",
        "            \"Determine if this information is enough to generate an 'article', 'faq', 'history' snippet.\\n\\n\"\n",
        "            f\"{aggregate}\\n\\n\"\n",
        "            \"Reply only with the names of sections that can be created: 'article', 'faq', 'history'.\\n \"\n",
        "            \"Do not include a section name if there isn't enough information to generate that section.\"\n",
        "        )\n",
        "\n",
        "        response = run_ollama4(prompt)\n",
        "        response_clean = response.strip().lower()\n",
        "\n",
        "        valid = {\"article\", \"faq\", \"history\"}\n",
        "        sections = [s for s in valid if s in response_clean]\n",
        "\n",
        "        return True, sections if sections else []\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error checking aggregate: {e}\")\n",
        "        return False, None\n",
        "def check_if_aggregate_is_good(prompt, aggregate: str) -> bool:\n",
        "    try:\n",
        "        prompt = (\n",
        "            f\"You are validating an overview of scraped content from multiple sources based on shop {prompt}.\\n\"\n",
        "            f\"{aggregate}\\n\\n\"\n",
        "            f\"Reply only `true` or `false` — is the summary solid and contains useful facts and is about {prompt}? \"\n",
        "        )\n",
        "        result = run_ollama4(prompt).strip().lower()\n",
        "        return \"true\" in result\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error checking aggregate: {e}\")\n",
        "        return False\n",
        "def generate_article_faq_history(prompt, aggregate: str, approved_sections: list[str]) -> dict | None:\n",
        "    def generate_and_validate(section_name: str, section_prompt: str) -> str | None:\n",
        "        raw = run_ollama4(section_prompt).strip()\n",
        "        if validate_section_html(prompt, section_name, raw):\n",
        "            return raw\n",
        "        print(f\"⚠️ {section_name} section failed validation, attempting fix.\")\n",
        "        fixed = fix_section_html(prompt, section_name, raw)\n",
        "        if fixed and validate_section_html(prompt, section_name, fixed):\n",
        "            return fixed\n",
        "        print(f\"❌ {section_name} section discarded after failed validation and fix.\")\n",
        "        return None\n",
        "    try:\n",
        "        results = {}\n",
        "        plan = f\"You will get a summary and write for consumers to read as a nearestdoor.com writer reporting on {prompt}, remove bad info and random questions or info, remove stuff that doesnt make sense, DO NOT INCLUDE EXTRA TEXT, PREFIXES, MARKS, QUOTES OR COMMENTS, ONLY GIVE THE WANTED TEXT NOTHING ELSE, NO HTML, use key words, make it thorough, you work for nearestdoor.com as our writer about thrift stores, you report on thrift stores for nearestdoor.com. must be just text nothing before or after, DO NOT USE ** or *, no asterisks\\n\"\n",
        "        if \"article\" in approved_sections:\n",
        "            article_prompt = (\n",
        "                f\"Using the following plan:\\n\\n{plan}\\n\\n\"\n",
        "                f\"And the following content:\\n\\n{aggregate}\\n\\n\"\n",
        "                f\"Write a detailed, SEO-friendly article about the thrift store {prompt}.\"\n",
        "            )\n",
        "            article = generate_and_validate(\"article\", article_prompt)\n",
        "            if article:\n",
        "                results[\"article\"] = article\n",
        "\n",
        "        if \"faq\" in approved_sections:\n",
        "            faq_prompt = (\n",
        "                f\"Using the following plan:\\n\\n{plan}\\n\\n\"\n",
        "                f\"And the following content:\\n\\n{aggregate}\\n\\n\"\n",
        "                f\"Generate a detailed and useful FAQ section for the thrift store {prompt}.\"\n",
        "            )\n",
        "            faq = generate_and_validate(\"faq\", faq_prompt)\n",
        "            if faq:\n",
        "                results[\"faq\"] = faq\n",
        "\n",
        "\n",
        "        if \"history\" in approved_sections:\n",
        "            history_prompt = (\n",
        "                f\"Using the following plan:\\n\\n{plan}\\n\\n\"\n",
        "                f\"And the following content:\\n\\n{aggregate}\\n\\n\"\n",
        "                f\"Write a backstory/history overview of the thrift store {prompt}:\"\n",
        "            )\n",
        "            history = generate_and_validate(\"history\", history_prompt)\n",
        "            if history:\n",
        "                results[\"history\"] = history\n",
        "\n",
        "        return True, results if results else None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error generating article/faq/history: {e}\")\n",
        "        return False, None\n",
        "\n",
        "\n",
        "import json\n",
        "import re\n",
        "def find_available_fields(aggregate):\n",
        "    try:\n",
        "        print('finding available fields', aggregate)\n",
        "        field_list = list(FIELD_EXTRACTORS.keys())\n",
        "        field_str = ', '.join(field_list)\n",
        "\n",
        "        prompt = (\n",
        "            f\"The following content is scraped and summarized:\\n\\n{aggregate}\\n\\n\"\n",
        "            f\"Which of the following fields can be confidently extracted from it?\\n\"\n",
        "            f\"{field_str}\\n\\n\"\n",
        "            \" Do not do None or example \"\n",
        "            \"Only return a list of field keys that are clearly available.\\n\"\n",
        "            \"If a key is not clearly extractable from the content, do not include it in response.\"\n",
        "        )\n",
        "\n",
        "        raw_response = run_ollama4(prompt).strip()\n",
        "        print(f\"🧠 Raw LLM response:\\n{raw_response}\")\n",
        "\n",
        "        # Normalize and clean up the response text\n",
        "        response_clean = raw_response.lower()\n",
        "\n",
        "        # Now just match by substring presence (don't trust format)\n",
        "        detected = [field for field in field_list if field.lower() in response_clean]\n",
        "\n",
        "        # Always include extract_categories (can always try to infer)\n",
        "        if \"extract_categories\" not in detected:\n",
        "            detected.append(\"extract_categories\")\n",
        "\n",
        "        return True, detected\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error detecting available fields: {e}\")\n",
        "        return False, None\n",
        "\n",
        "\n",
        "def extract_clean_json_structure(text):\n",
        "    try:\n",
        "        match = re.search(r\"(|\\{.*?\\})\", text.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            return None\n",
        "        raw = match.group(0)\n",
        "        json_ready = raw.replace(\"'\", '\"')\n",
        "        return json.loads(json_ready)  # ✅ Returns a real dict or list\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ extract_clean_json_structure failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def extract_fields_from_aggregate(aggregate: str, available_fields: list[str]) -> dict | None:\n",
        "    try:\n",
        "        extracted = {}\n",
        "\n",
        "        for field in available_fields:\n",
        "            prompt = FIELD_EXTRACTORS[field] + \"\\n\\n\" + aggregate\n",
        "            raw_value = run_ollama4(prompt).strip()\n",
        "\n",
        "            # Clean BEFORE validating\n",
        "            cleaned_value = extract_clean_json_structure(raw_value) or raw_value\n",
        "\n",
        "            validator = FIELD_VALIDATORS.get(field)\n",
        "            is_valid = validator(cleaned_value) if validator else True\n",
        "\n",
        "            # If fails local validation, use LLM validator\n",
        "            if not is_valid:\n",
        "                is_valid = check_with_ollama_format_validator(field, cleaned_value)\n",
        "\n",
        "            # If still invalid, try to fix it via LLM\n",
        "            if not is_valid:\n",
        "                print(f\"⚠️ Invalid format for {field}, attempting fix.\")\n",
        "                fixed = attempt_fix_with_ollama(field, cleaned_value)\n",
        "\n",
        "                if fixed and validator(fixed):\n",
        "                    is_valid = check_with_ollama_format_validator(field, fixed)\n",
        "                    if is_valid:\n",
        "                        extracted[field] = fixed\n",
        "                    continue\n",
        "\n",
        "                print(f\"❌ Discarding {field}, failed validation and repair.\")\n",
        "                continue\n",
        "\n",
        "            extracted[field] = cleaned_value\n",
        "            print(\"EXTRACTED\")\n",
        "            print(cleaned_value)\n",
        "\n",
        "        return True, extracted if extracted else None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error extracting fields: {e}\")\n",
        "        return False, None\n",
        "\n",
        "def validate_section_html(prompt,section: str, html: str) -> bool:\n",
        "    try:\n",
        "        prompt = (\n",
        "              \"Reply only with `true` or `false`.\\n\"\n",
        "            f\"You're validating a block of text meant for a '{section}' section about {prompt} for nearestdoor.com.\\n\\n\"\n",
        "            f\"{html}\\n\\n\"\n",
        "            \"No ** or *, no asterisks\\n\"\n",
        "            f\"does the info seem valid and real?\\n\"\n",
        "\n",
        "            \"Article must be written as nearestdoor.com writer\\n \"\n",
        "            \"Does the info contain random questions or off topic parts?\\n\"\n",
        "            \" Does it make sense and are there any comments or prefixes before or after the text? It is written as a nearestdoor.com writer reporting on a shop? must be only the text wanted, nothing before or after\\n\"\n",
        "            \"Reply only with `true` or `false`.\\n\"\n",
        "\n",
        "        )\n",
        "        result = run_ollama4(prompt).strip().lower()\n",
        "        return \"true\" in result\n",
        "    except Exception as e:\n",
        "        print(f\"❌ HTML validation failed for section {section}: {e}\")\n",
        "        return False\n",
        "\n",
        "def check_with_ollama_format_validator(field_name: str, value: str) -> bool:\n",
        "    try:\n",
        "        prompt = (\n",
        "              \"Reply only with `true` or `false`.\\n\"\n",
        "            f\"You're validating the format of this field: `{field_name}`.\\n\\n\"\n",
        "            f\"Content:\\n{value}\\n\\n\"\n",
        "             \" Do not allow None or example \"\n",
        "\n",
        "            f\"does the info seem valid and real?\\n\"\n",
        "            \"Is the format correct for its expected structure (e.g. phone, JSON, email) and has no comments, quotes, marks or prefixes? nothing before or after json, must be valid json, no '', no ` \\n \"\n",
        "            \"will this parse correctly to json?\\n\"\n",
        "            \"Reply only with `true` or `false`.\\n\"\n",
        "        )\n",
        "        result = run_ollama4(prompt).strip().lower()\n",
        "        return \"true\" in result\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ollama format check failed for {field_name}: {e}\")\n",
        "        return False\n",
        "def attempt_fix_with_ollama(field_name: str, value: str) -> str | None:\n",
        "    try:\n",
        "        fix_prompt = (\n",
        "            f\"The following field is not formatted correctly for `{field_name}`.\\n\\n\"\n",
        "            f\"Broken value:\\n{value}\\n\\n\"\n",
        "             \"Output must parse correctly to json.\\n\"\n",
        "                  \" Do not allow None or example \"\n",
        "            f\"Fix it to match the expected format based on this instruction:\\n{FIELD_EXTRACTORS[field_name]}\\n\\n\"\n",
        "            \"Only return the fixed value, no comments or explanation, no marks, quotes, or prefixes. nothing before or after json, must be valid json, no ''\"\n",
        "        )\n",
        "        return run_ollama4(fix_prompt).strip()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ollama fix failed for {field_name}: {e}\")\n",
        "        return None\n",
        "def fix_section_html(prompt, section: str, html: str) -> str | None:\n",
        "    try:\n",
        "        prompt = (\n",
        "            f\"The following {section} section text is incorrect or does not follow required formatting rules.\\n\\n\"\n",
        "            f\"{html}\\n\\n\"\n",
        "            f\"based on {prompt}.\\n\"\n",
        "            \"Article must be written as nearestdoor.com writer \\n\"\n",
        "            \"Fix it based on these rules:\\n\"\n",
        "  \"No ** or *, no asterisks\\n\"\n",
        "\n",
        "            \"must not contain random info or comments, weird info\\n\"\n",
        "            \"- Be SEO-friendly, concise, and structured.\\n\"\n",
        "            \"no '' \\n\"\n",
        "            \"Only text no html\"\n",
        "            \"must be only the section nothing before or after\\n\"\n",
        "            \"Only return the corrected text. No extra commentary, quotes, prefixes, or marks.\"\n",
        "        )\n",
        "        return run_ollama4(prompt).strip()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to fix {section} section: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "API_BASE = \"https://www.nearestdoor.com\"  # Replace with actual server URL\n",
        "CLIENT_ID = \"client001\"\n",
        "HEARTBEAT_INTERVAL = 60  # seconds\n",
        "SHOP_FLOW_STATIC = [\n",
        "    #all expect completed or failed responses and timeout in 4 mins so u will check in heartbeat every 1 min\n",
        "    \"search\",#gives shopwebsite if any and shop name, calls google wiki, and shopwebsite lookup returns all data from summarize and images with alts base64 if so, first is_main, or none stop\n",
        "  \"aggregate\"#sends data from search and shop data, sends back final summary or none stop\n",
        "    \"createplan\",#sends final summary and shop name, expects \"article\", \"faq\", \"history\" in [] if good, or none\n",
        "    \"create\", #sends final summary and the data [] from createplan and shop data, sends     results[\"history\"] faq article, or none\n",
        "    \"generate_meta_tags_from_aggregate\", #send aggregate and shop name, expect return {  \"title\": title \"description\": description or none  }\n",
        "    \"find_available_fields\", #sends aggregate, expects detected = [field for field in FIELD_EXTRACTORS if field.lower() in response] return detected if detected else [] or None\n",
        "    \"extract_fields_from_aggregate\",#sends aggreagate and result from find avilable fields, expects   extracted[field] = raw_value return extracted if extracted else None or none\n",
        "    \"fillintheshop\",#if youve made it this far add content created to the shop variables\n",
        "]\n",
        "async def handle_task(task, crawler):\n",
        "    try:\n",
        "        if not task or not isinstance(task, dict):\n",
        "            print(\"❌ Skipping empty or malformed task.\")\n",
        "            return\n",
        "\n",
        "        task_id = task.get(\"task_id\")\n",
        "        task_type = task.get(\"task_type\")\n",
        "\n",
        "        if not task_id or not isinstance(task_id, int):\n",
        "            print(f\"❌ Invalid task_id: {task_id}\")\n",
        "            return\n",
        "        if not task_type:\n",
        "            print(\"❌ Missing task_type.\")\n",
        "            return\n",
        "\n",
        "        print(f\"▶️ Task: {task_type} ({task_id})\")\n",
        "\n",
        "        result,summary, mainstring, images, extractedfields, foundfields = False, None, None, None, None, None\n",
        "        match task_type:\n",
        "            case \"search\": result, mainstring, images = await search2(task, crawler)\n",
        "            case \"aggregate\": result, summary = summarize_large_text(task['target'].get('name', ''),task['target'].get('largetext', '') )\n",
        "            case \"createplan\": result,aggregateplan =  create_aggregate_plan(task['target'].get('summary', ''))\n",
        "            case \"create\": result,createdinfo =  generate_article_faq_history(task['target'].get('name', ''), task['target'].get('summary', ''),task['target'].get('aggregateplan', ''))\n",
        "\n",
        "            case \"find_available_fields\": result, foundfields = find_available_fields(task['target'].get('summary', ''))\n",
        "            case \"extract_fields_from_aggregate\": result, extractedfields = extract_fields_from_aggregate(task['target'].get('summary', ''), task['target'].get('foundfields', ''))\n",
        "        print(result, task_type, \"extraxted\",extractedfields,\"found\", foundfields)\n",
        "        if result:\n",
        "            print(f\"📤 Submitting result for {task_type} ({task_id})\")\n",
        "            try:\n",
        "                if task_type == 'search':\n",
        "                    res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"mainstring\": mainstring, \"images\": images, \"client_id\": CLIENT_ID})\n",
        "\n",
        "                if task_type == 'aggregate':\n",
        "                    if summary:\n",
        "                        res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"summary\": summary, \"client_id\": CLIENT_ID})\n",
        "                    else:\n",
        "                        print(\"nosummary\")\n",
        "                        res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"fail\", \"client_id\": CLIENT_ID})\n",
        "\n",
        "\n",
        "                if task_type == 'createplan':\n",
        "                    res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"aggregateplan\": aggregateplan, \"client_id\": CLIENT_ID})\n",
        "                if task_type == 'create':\n",
        "                    res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"createdinfo\":createdinfo, \"client_id\": CLIENT_ID})\n",
        "                if task_type == 'find_available_fields':\n",
        "                    res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"foundfields\": foundfields, \"client_id\": CLIENT_ID})\n",
        "                if task_type == 'extract_fields_from_aggregate':\n",
        "                    res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"extractedfields\": extractedfields, \"client_id\": CLIENT_ID})\n",
        "\n",
        "                print(f\"Server responded: {res.status_code} - {res.text}\")\n",
        "                if res.status_code == 200:\n",
        "                    print(f\"✅ Submitted: {task_type}\")\n",
        "                else:\n",
        "                    print(f\"❌ Submit failed: {task_type} - {res.status_code}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Submit exception: {e}\")\n",
        "        else:\n",
        "\n",
        "            print(f\"Submit Failure {task_type}\")\n",
        "            res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"fail\", \"client_id\": CLIENT_ID})\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Task FAILED, Task: {task_type} ({task_id} {e}) \")\n",
        "        return None\n",
        "def get_task():\n",
        "    try:\n",
        "        print(\"📡 Polling for task...\")\n",
        "        r = requests.get(f\"{API_BASE}/next-task\", params={\"client_id\": CLIENT_ID}, timeout=30)\n",
        "        print(f\"API GET response: {r.status_code} - {r.text}\")\n",
        "\n",
        "        if r.status_code == 200:\n",
        "            try:\n",
        "                task = r.json()\n",
        "                if not isinstance(task, dict) or \"task_id\" not in task:\n",
        "                    print(f\"⚠️ Invalid task structure received: {task}\")\n",
        "                    return None\n",
        "                return task\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Failed to parse JSON: {e}\")\n",
        "                return None\n",
        "\n",
        "        elif r.status_code == 204:\n",
        "            print(\"⏳ No available task.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"❌ Unexpected status code: {r.status_code}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Task fetch failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def poll_heartbeat(current_task_id=None):\n",
        "    try:\n",
        "        data = {\"client_id\": CLIENT_ID}\n",
        "        if current_task_id:\n",
        "            data[\"task_id\"] = current_task_id\n",
        "        requests.post(f\"{API_BASE}/heartbeat\", json=data)\n",
        "        print(\"🫀 Heartbeat sent.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Failed heartbeat: {e}\")\n",
        "\n",
        "async def main():\n",
        "    crawler_mgr = CrawlerManager()\n",
        "    await crawler_mgr.start()\n",
        "\n",
        "    last_heartbeat = 0\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            task = get_task()\n",
        "            if task:\n",
        "                now = time.time()\n",
        "                if now - last_heartbeat > HEARTBEAT_INTERVAL:\n",
        "                    poll_heartbeat(task.get(\"task_id\"))\n",
        "                    last_heartbeat = now\n",
        "                await handle_task(task, crawler_mgr)\n",
        "            else:\n",
        "                print(\"⏳ No task available, waiting...\")\n",
        "                await asyncio.sleep(10)\n",
        "    finally:\n",
        "        await crawler_mgr.stop()\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "\n",
        "nest_asyncio.apply()  # Allows re-entry of the running loop in Colab\n",
        "\n",
        "# Then just run the main function like this\n",
        "await main()"
      ],
      "metadata": {
        "id": "W_g_LpJugnI1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}