{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexrosulek/Cs50/blob/main/makearticle2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzG-3nviVQTM",
        "outputId": "f0e32ac0-2248-4581-9bad-c3a02f95aef6",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13281    0 13281    0     0  10582      0 --:--:--  0:00:01 --:--:-- 10582\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to render group...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: systemd is not running\n",
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cuda-drivers is already the newest version (570.124.06-0ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!curl https://ollama.ai/install.sh | sh\n",
        "\n",
        "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "!sudo apt-get update && sudo apt-get install -y cuda-drivers\n",
        "\n",
        "import os\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &\n",
        "\n",
        "\n",
        "!ollama pull gemma3:1b\n",
        "!ollama pull gemma3:4b\n",
        "!pip install ollama"
      ],
      "metadata": {
        "id": "O5toc_VkVffm",
        "outputId": "0b5e1033-d7f2-43ff-d12e-180616ae81a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "\u001b[?2026h\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠹ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠸ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠼ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠴ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠦ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠧ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠇ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠏ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠹ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠸ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠼ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠴ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠦ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠧ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠇ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest \n",
            "pulling 7cd4618c1faf... 100% ▕▏ 815 MB                         \n",
            "pulling e0a42594d802... 100% ▕▏  358 B                         \n",
            "pulling dd084c7d92a3... 100% ▕▏ 8.4 KB                         \n",
            "pulling 3116c5225075... 100% ▕▏   77 B                         \n",
            "pulling 120007c81bf8... 100% ▕▏  492 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\u001b[?2026l\n",
            "\u001b[?2026h\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠹ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠸ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠼ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠴ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠧ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest ⠧ \u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25lpulling manifest \n",
            "pulling aeda25e63ebd... 100% ▕▏ 3.3 GB                         \n",
            "pulling e0a42594d802... 100% ▕▏  358 B                         \n",
            "pulling dd084c7d92a3... 100% ▕▏ 8.4 KB                         \n",
            "pulling 3116c5225075... 100% ▕▏   77 B                         \n",
            "pulling b6ae5839783f... 100% ▕▏  489 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\u001b[?2026l\n",
            "Requirement already satisfied: ollama in /usr/local/lib/python3.11/dist-packages (0.4.8)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from ollama) (2.11.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &\n",
        "!pip install crawl4ai\n",
        "!pip install aiohttp\n",
        "!pip install pillow\n",
        "!pip install beautifulsoup4\n",
        "!pip install wikipedia\n",
        "!pip install googlesearch-python\n",
        "!pip install playwright\n",
        "!playwright install chromium\n",
        "\n",
        "import asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\n",
        "from urllib.parse import urlparse\n",
        "import asyncio\n",
        "import random\n",
        "import time\n",
        "import aiohttp\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import base64\n",
        "import io\n",
        "import subprocess\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import wikipedia\n",
        "\n",
        "import textwrap\n",
        "\n",
        "import re\n",
        "import json\n",
        "from googlesearch import search\n",
        "\n",
        "folder = \"downloaded_images\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "def is_valid_phone(value: str) -> bool:\n",
        "    return bool(re.fullmatch(r\"\\d{3}-\\d{3}-\\d{4}\", value.strip()))\n",
        "\n",
        "def is_valid_email(value: str) -> bool:\n",
        "    return bool(re.fullmatch(r\"[^@\\s]+@[^@\\s]+\\.[a-zA-Z0-9]+\", value.strip()))\n",
        "\n",
        "def is_valid_url(value: str) -> bool:\n",
        "    return value.strip().startswith(\"http\")\n",
        "\n",
        "def is_valid_json(value: str) -> bool:\n",
        "    try:\n",
        "        json.loads(value)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def is_non_empty_string(value: str) -> bool:\n",
        "    return isinstance(value, str) and len(value.strip()) > 0\n",
        "FIELD_VALIDATORS = {\n",
        "    \"extract_phone\": is_valid_phone,\n",
        "    \"extract_email\": is_valid_email,\n",
        "    \"extract_website\": is_valid_url,\n",
        "    \"extract_operating_hours\": is_valid_json,\n",
        "    \"extract_holiday_hours\": is_valid_json,\n",
        "    \"extract_categories\": is_valid_json,\n",
        "    \"extract_delivery_services\": is_valid_json,\n",
        "    \"extract_social_media\": is_valid_json,\n",
        "    \"extract_stocked_brands\": is_valid_json,\n",
        "    \"extract_inventory_categories\": is_valid_json,\n",
        "    \"extract_customer_reviews\": is_valid_json,\n",
        "    \"extract_admission\": is_non_empty_string,\n",
        "    \"extract_date_available\": is_non_empty_string,\n",
        "    \"extract_attendance_amount\": is_non_empty_string,\n",
        "    \"extract_exhibitor_amount\": is_non_empty_string\n",
        "}\n",
        "\n",
        "FIELD_EXTRACTORS = {\n",
        "        \"extract_phone\": 'From the content, extract the phone number in this format: \"727-123-4567\"\\nOnly return the phone string, no extra text, prefixes, marks, or quotes, only the phone number itself.',\n",
        "        \"extract_email\": 'From the content, extract the email address in this format: \"name@example.com\"\\nOnly return the email string, no extra text, prefixes,  marks, or quotes, only the email itself.',\n",
        "        \"extract_website\": 'From the content, extract the website URL in this format: \"https://example.com\"\\nOnly return the URL string, no extra text, prefixes,  marks, or quotes, only the website url itself with https://.',\n",
        "        \"extract_operating_hours\": \"\"\"From the content, extract operating hours in this format, no extra text, prefixes, marks or quotes, only the json itself:\n",
        "{\n",
        "  \"monday\": \"9:00 AM - 5:00 PM\",\n",
        "  \"tuesday\": \"9:00 AM - 5:00 PM\",\n",
        "  \"wednesday\": \"9:00 AM - 5:00 PM\",\n",
        "  \"thursday\": \"9:00 AM - 5:00 PM\",\n",
        "  \"friday\": \"9:00 AM - 5:00 PM\",\n",
        "  \"saturday\": \"Closed\",\n",
        "  \"sunday\": \"Closed\"\n",
        "}\n",
        "Only return JSON, no extra text. no `, nothing before or after json, ONLY VALID JSON.\"\"\",\n",
        "        \"extract_holiday_hours\": \"From the content, extract holiday hours in this format: {'2024-12-25': 'Closed', '2024-12-31': '10:00 AM - 4:00 PM'}\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself. no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_categories\": \"From the content, extract shop categories in this format: ['Church', 'Nonprofit', 'Holy']\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself. no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_delivery_services\": \"From the content, extract delivery services in this format: ['Uber Eats', 'Self Delivery']\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself.  no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_social_media\": \"From the content, extract social media links in this format: {'facebook': 'url', 'instagram': 'url'}\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself. no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_stocked_brands\": \"From the content, extract stocked brands in this format: ['Nike', 'Adidas']\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself. no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_inventory_categories\": \"From the content, extract inventory categories in this format: {'Apparel': ['Shirts', 'Hoodies']}\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself.  no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_customer_reviews\": \"From the content, extract customer reviews in this format: [{'user'': 'John', 'comment': 'Great!', 'rating': 5}]\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself. no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_admission\": \"From the content, extract the admission info as a string. Only return the string, no extra text, prefixes, marks or quotes, only the string admission itself.  no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_date_available\": \"From the content, extract date available as a string. Only return the string, no extra text, prefixes, marks or quotes, only the string date available itself. no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_attendance_amount\": \"From the content, extract attendance amount as a string. Only return the string, no extra text, prefixes, marks or quotes, only the string attendence amount itself. no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_exhibitor_amount\": \"From the content, extract exhibitor amount as a string. Only return the string, no extra text, prefixes, marks or quotes, only the string exhibitor amount itself. no `, nothing before or after json, ONLY VALID JSON. \"\n",
        "    }\n",
        "import random\n",
        "import wikipedia\n",
        "\n",
        "async def handle_wikipedia(task):\n",
        "    try:\n",
        "        name = task['target'].get(\"name\")\n",
        "        city = task['target'].get(\"city\", \"\")\n",
        "        query = f\"{name} {city}\".strip()\n",
        "        print(f\"📚 Wikipedia lookup for: {query}\")\n",
        "\n",
        "        page = wikipedia.page(query, auto_suggest=True)\n",
        "        content = page.content\n",
        "\n",
        "        # If short, return all\n",
        "        if len(content) <= 2000:\n",
        "            return content\n",
        "\n",
        "        # Otherwise, chunk and pick 4 from middle\n",
        "        chunk_size = 500\n",
        "        chunks = [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]\n",
        "\n",
        "        if len(chunks) <= 6:\n",
        "            selected_chunks = chunks  # Not enough to skip ends\n",
        "        else:\n",
        "            middle_chunks = chunks[2:-2]\n",
        "            selected_chunks = random.sample(middle_chunks, min(4, len(middle_chunks)))\n",
        "\n",
        "        return \"\\n\\n\".join(selected_chunks)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Wikipedia fetch failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "async def handle_google_search(task, crawler):\n",
        "    try:\n",
        "        target = task['target']\n",
        "        query = f\"{target.get('name', '')} {target.get('city', '')} {target.get('state', '')}\".strip()\n",
        "        print(query)\n",
        "        valid_results = ''\n",
        "\n",
        "        results = list(search(query))\n",
        "\n",
        "        # Randomly shuffle and pick half\n",
        "        half_results = random.sample(results, max(1, len(results) // 2))\n",
        "\n",
        "\n",
        "\n",
        "        for url in half_results:\n",
        "            try:\n",
        "                res = requests.get(url, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "                soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "                title = soup.title.string.strip() if soup.title else \"No title\"\n",
        "                meta_tag = soup.find(\"meta\", {\"name\": \"description\"})\n",
        "                description = meta_tag[\"content\"].strip() if meta_tag else \"No description\"\n",
        "\n",
        "                prompt = (\n",
        "                    f\"Is this page about the query '{query}'? Be very strict, no random things this is for a specific church.\\n\\n\"\n",
        "                    f\"Title: {title}\\n\"\n",
        "                    f\"Description: {description}\\n\\n\"\n",
        "                    f\"Return true or false.\"\n",
        "                )\n",
        "\n",
        "                print(\"🔍 running ollama for relevance check\")\n",
        "                result = run_ollama1(prompt)\n",
        "                if \"true\" in result.lower():\n",
        "                    print(f\"✅ Relevant: {url}\")\n",
        "                    content = await getsite(url, crawler)\n",
        "                    if content and content.success:\n",
        "                        markdown = content.markdown.fit_markdown\n",
        "                        if len(markdown) <= 1500:\n",
        "                            valid_results += markdown\n",
        "\n",
        "                        # Chunk and pick 4 from the middle\n",
        "                        chunk_size = 500\n",
        "                        chunks = [markdown[i:i + chunk_size] for i in range(0, len(markdown), chunk_size)]\n",
        "                        if len(chunks) <= 6:\n",
        "                            selected_chunks = chunks\n",
        "                        else:\n",
        "                            selected_chunks = random.sample(chunks[2:-2], min(4, len(chunks[2:-2])))\n",
        "\n",
        "                        valid_results += \"\\n\\n\".join(selected_chunks) + f\"\\n\\n<- EXTRACTED FROM THIS URL: {url}\\n\\n\"\n",
        "            except Exception as inner_err:\n",
        "                print(f\"⚠️ Error processing {url}: {inner_err}\")\n",
        "\n",
        "        if valid_results != '':\n",
        "            return valid_results\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in handle_google_search: {e}\")\n",
        "        return None\n",
        "\n",
        "async def search2(task, crawler):\n",
        "    print(\"getting google results\")\n",
        "    google_results = await handle_google_search(task, crawler)\n",
        "    print(\"getting wiki results\")\n",
        "    wiki_results = await handle_wikipedia(task)\n",
        "\n",
        "    imageandalt = None\n",
        "    result = None\n",
        "    images = None\n",
        "    if task['target'].get(\"website\"):\n",
        "        print(\"getting website results\")\n",
        "        print(task['target'].get(\"website\"))\n",
        "\n",
        "        result = await getsite(task['target'].get(\"website\"), crawler)\n",
        "\n",
        "        if result.success:\n",
        "            print(\"got images\")\n",
        "            images = await findimages(result)\n",
        "        if images:\n",
        "            imageandalt = validate_images(task['target'].get(\"name\"),images)\n",
        "            print(\"images validated\")\n",
        "    print(\"done searching\")\n",
        "    mainstring = ''\n",
        "    if google_results:\n",
        "        print(\"got some google results\")\n",
        "        mainstring = google_results\n",
        "        if wiki_results:\n",
        "            print(\"got some wiki results\")\n",
        "            mainstring += wiki_results\n",
        "    if result:\n",
        "        if result.success:\n",
        "\n",
        "            mainstring += result.markdown.fit_markdown\n",
        "    if mainstring == '' or len(mainstring) < 500:\n",
        "        if not imageandalt:\n",
        "            return False, None, None\n",
        "        return True, None, imageandalt\n",
        "    return True, mainstring, imageandalt\n",
        "def resize_image(input_path, size):\n",
        "    try:\n",
        "        img = Image.open(input_path).convert(\"RGB\")\n",
        "        img = img.resize(size, Image.ANTIALIAS)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Resize failed for {input_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def image_to_base64(img: Image.Image) -> str:\n",
        "    try:\n",
        "        buffer = io.BytesIO()\n",
        "        img.save(buffer, format=\"JPEG\")\n",
        "        return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ base64: {e}\")\n",
        "        return None\n",
        "\n",
        "def run_ollama_with_image(prompt: str, image_base64: str, model=\"gemma3:1b\") -> str:\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            \"http://localhost:11434/api/generate\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"prompt\": prompt,\n",
        "                \"images\": [image_base64],\n",
        "                \"stream\": False\n",
        "            },\n",
        "            timeout=256\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        return result.get(\"response\", \"\").strip().lower()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ollama API call failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def run_ollama4(prompt, model=\"gemma3:4b\"):\n",
        "    print(f\"🧠 Running Ollama with model={model}\")\n",
        "    try:\n",
        "        proc = subprocess.run(\n",
        "            [\"ollama\", \"run\", model],\n",
        "            input=prompt.encode(\"utf-8\"),\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            timeout=556\n",
        "        )\n",
        "        stdout = proc.stdout.decode(\"utf-8\").strip()\n",
        "        stderr = proc.stderr.decode(\"utf-8\").strip()\n",
        "        print(f\"Ollama stdout:\\n{stdout}\")\n",
        "        if stderr:\n",
        "            print(f\"Ollama stderr:\\n{stderr}\")\n",
        "        return stdout\n",
        "    except Exception as e:\n",
        "        print(f\"Ollama run failed: {e}\")\n",
        "        return \"\"\n",
        "def run_ollama1(prompt, model=\"gemma3:1b\"):\n",
        "    print(f\"🧠 Running Ollama with model={model}\")\n",
        "    try:\n",
        "        proc = subprocess.run(\n",
        "            [\"ollama\", \"run\", model],\n",
        "            input=prompt.encode(\"utf-8\"),\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            timeout=356\n",
        "        )\n",
        "        stdout = proc.stdout.decode(\"utf-8\").strip()\n",
        "        stderr = proc.stderr.decode(\"utf-8\").strip()\n",
        "        print(f\"Ollama stdout:\\n{stdout}\")\n",
        "        return stdout\n",
        "    except Exception as e:\n",
        "        print(f\"Ollama run failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def validate_images(name,images: list[dict]) -> list[dict] | None:\n",
        "    \"\"\"\n",
        "    Takes [{'filename': ..., 'alt': ...}]\n",
        "    Returns [{'alt': ..., 'base64': ...}] — resized to 1536x1024\n",
        "    Returns None on error.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "\n",
        "        approved = []\n",
        "\n",
        "        for img_info in images:\n",
        "            filename = img_info[\"filename\"]\n",
        "            alt_text = img_info.get(\"alt\") or \"No description\"\n",
        "\n",
        "            # Step 1: Resize to 512x512 for judging\n",
        "            resized_512 = resize_image(filename, (512, 512))\n",
        "            if not resized_512:\n",
        "                continue\n",
        "\n",
        "            img_512_b64 = image_to_base64(resized_512)\n",
        "            if not img_512_b64:\n",
        "                continue\n",
        "\n",
        "            prompt = (\n",
        "                f\"Based on the description: \\\"{alt_text}\\\", \"\n",
        "                f\"is this image good for church {name}? Answer only true or false.\"\n",
        "            )\n",
        "\n",
        "            result = run_ollama_with_image(prompt, img_512_b64)\n",
        "            if \"true\" not in result:\n",
        "                print(f\"🛑 Image rejected: {filename}\")\n",
        "                continue\n",
        "\n",
        "            # Step 2: Resize to 1536x1024 for output\n",
        "            final_img = resize_image(filename, (1536, 1024))\n",
        "            if not final_img:\n",
        "                continue\n",
        "\n",
        "            encoded_final = image_to_base64(final_img)\n",
        "            approved.append({\n",
        "                \"alt\": alt_text,\n",
        "                \"base64\": encoded_final\n",
        "            })\n",
        "\n",
        "        if os.path.exists(folder):\n",
        "            shutil.rmtree(folder)\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "        return approved\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        if os.path.exists(folder):\n",
        "            shutil.rmtree(folder)\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "        print(f\"❌ Error in validate_images: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def normalize_and_validate_url(url: str) -> str | None:\n",
        "    try:\n",
        "        # Normalize: ensure https://\n",
        "        if not url.startswith(\"https://\"):\n",
        "            if url.startswith(\"http://\"):\n",
        "                url = url.replace(\"http://\", \"https://\", 1)\n",
        "            else:\n",
        "                url = \"https://\" + url.lstrip(\"/\")\n",
        "\n",
        "        # Validate\n",
        "        parsed = urlparse(url)\n",
        "        if not parsed.scheme or not parsed.netloc:\n",
        "            return None  # Invalid URL\n",
        "\n",
        "        return url\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error normalizing/validating {url}: {e}\")\n",
        "        return None\n",
        "class CrawlerManager:\n",
        "    def __init__(self):\n",
        "        self.crawler = None\n",
        "\n",
        "    async def start(self):\n",
        "        if self.crawler is None:\n",
        "            self.crawler = AsyncWebCrawler(config=BrowserConfig())\n",
        "            await self.crawler.__aenter__()\n",
        "\n",
        "    async def stop(self):\n",
        "        if self.crawler:\n",
        "            await self.crawler.__aexit__(None, None, None)\n",
        "            self.crawler = None\n",
        "\n",
        "    async def crawl(self, url: str):\n",
        "        if not self.crawler:\n",
        "            raise RuntimeError(\"Crawler not started\")\n",
        "\n",
        "        url = normalize_and_validate_url(url)\n",
        "        if not url:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            run_config = CrawlerRunConfig()\n",
        "            result = await self.crawler.arun(url=url, config=run_config)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"❌ crawl error for {url}: {e}\")\n",
        "            return None\n",
        "async def getsite(url: str, crawler_mgr: CrawlerManager):\n",
        "    return await crawler_mgr.crawl(url)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "async def download_image(session, img, idx):\n",
        "\n",
        "    try:\n",
        "        url = img[\"src\"]\n",
        "        alt = img.get(\"alt\") or img.get(\"desc\", \"No alt/desc\")\n",
        "        ext = url.split('.')[-1].split('?')[0]\n",
        "        filename = f\"downloaded_images/image_{idx}.{ext}\"\n",
        "\n",
        "        async with session.get(url) as resp:\n",
        "            if resp.status == 200:\n",
        "                with open(filename, 'wb') as f:\n",
        "                    f.write(await resp.read())\n",
        "                print(f\"✅ Downloaded: {filename}\")\n",
        "                return {\"filename\": filename, \"alt\": alt}\n",
        "            else:\n",
        "                print(f\"❌ Failed to download {url} (status {resp.status})\")\n",
        "                return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error downloading {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "async def findimages(result):\n",
        "    try:\n",
        "\n",
        "        images = result.media.get(\"images\", [])\n",
        "        if not images:\n",
        "            print(\"No images found.\")\n",
        "            return None\n",
        "\n",
        "        # Pick up to 8 random images\n",
        "        selected_images = random.sample(images, min(8, len(images)))\n",
        "\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            tasks = [\n",
        "                download_image(session, img, idx)\n",
        "                for idx, img in enumerate(selected_images)\n",
        "            ]\n",
        "            results = await asyncio.gather(*tasks)\n",
        "\n",
        "        # Filter out failed downloads\n",
        "        successful = [r for r in results if r]\n",
        "\n",
        "        return successful\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error finding images: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def summarize_large_text(prompt, large_text: str, max_length: int = 3000) -> str | None:\n",
        "    try:\n",
        "        # Split large text into chunks (~1024 chars each)\n",
        "        chunks = textwrap.wrap(large_text, width=1500)\n",
        "        print(len(chunks), 'got this many chunks')\n",
        "        summaries = []\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            prompt = f\"Extract church info. keep facts, keep details about church {prompt}, keep operating hours ect, all details, REMOVE HTML AND JS, u are not doing anything else but extracting/consolidating:\\n\\n{chunk}\"\n",
        "            summary = run_ollama1(prompt)\n",
        "            if summary:\n",
        "                summaries.append(summary)\n",
        "            else:\n",
        "                print(f\"⚠️ Failed to summarize chunk {idx}\")\n",
        "\n",
        "        # Aggregate all summaries\n",
        "        combined = \"\".join(summaries)\n",
        "\n",
        "        if len(combined) > max_length:\n",
        "            print(\"summarizing again\")\n",
        "            chunks = textwrap.wrap(combined, width=2000)\n",
        "\n",
        "            summaries = []\n",
        "            for idx, chunk in enumerate(chunks):\n",
        "                prompt = f\"Extract church info concisely, based on church {prompt}, keep operating hours phone website ect all details, u are not doing anything else but extracting/consolidating, return it back do not tell me how good it is, no other objectives or questions:\\n\\n{chunk}\"\n",
        "\n",
        "                summary = run_ollama1(prompt)\n",
        "                if summary:\n",
        "                    summaries.append(summary)\n",
        "                else:\n",
        "                    print(f\"⚠️ Failed to summarize chunk {idx}\")\n",
        "\n",
        "        combined = \"\".join(summaries)\n",
        "        if len(combined) > max_length:\n",
        "\n",
        "            print(\"summarizing again\")\n",
        "            chunks = textwrap.wrap(combined, width=2000)\n",
        "\n",
        "            summaries = []\n",
        "            for idx, chunk in enumerate(chunks):\n",
        "                prompt = f\"Summarize shortly make shorter than it is. based on church {prompt}, keep all details and facts, u are not doing anything else but summarizing/consolidating, return it back do not tell me how good it is, no other objectives or questions:\\n\\n{chunk}\"\n",
        "\n",
        "                summary = run_ollama1(prompt)\n",
        "                if summary:\n",
        "                    summaries.append(summary)\n",
        "                else:\n",
        "                    print(f\"⚠️ Failed to summarize chunk {idx}\")\n",
        "\n",
        "            combined = \"\\n\".join(summaries)\n",
        "        prompt = f\"You work for nearestdoor.com. This is scraped info from the internet based on church {prompt}, keep all details, keep all information/facts/details operating hours phone website ect and turn it into an overview, no other objectives or questions:\\n\\n{combined}\"\n",
        "\n",
        "        summary = run_ollama4(prompt)\n",
        "        if check_if_aggregate_is_good(prompt,summary):\n",
        "            return True, summary\n",
        "        else:\n",
        "            return False, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in summarize_large_text: {e}\")\n",
        "        return False, None\n",
        "def create_aggregate_plan(aggregate: str) -> bool:\n",
        "    try:\n",
        "        prompt = (\n",
        "            \"Determine if this information is enough to generate an 'article', 'faq', 'history' snippet.\\n\\n\"\n",
        "            f\"{aggregate}\\n\\n\"\n",
        "            \"Reply only with the names of sections that can be created: 'article', 'faq', 'history'.\\n \"\n",
        "            \"Do not include a section name if there isn't enough information to generate that section.\"\n",
        "        )\n",
        "\n",
        "        response = run_ollama4(prompt)\n",
        "        response_clean = response.strip().lower()\n",
        "\n",
        "        valid = {\"article\", \"faq\", \"history\"}\n",
        "        sections = [s for s in valid if s in response_clean]\n",
        "\n",
        "        return True, sections if sections else []\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error checking aggregate: {e}\")\n",
        "        return False, None\n",
        "def check_if_aggregate_is_good(prompt, aggregate: str) -> bool:\n",
        "    try:\n",
        "        prompt = (\n",
        "            f\"You are validating an overview of scraped content from multiple sources based on shop {prompt}.\\n\"\n",
        "            f\"{aggregate}\\n\\n\"\n",
        "            f\"Reply only `true` or `false` — is the summary solid and contains useful facts and is about {prompt}? \"\n",
        "        )\n",
        "        result = run_ollama4(prompt).strip().lower()\n",
        "        return \"true\" in result\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error checking aggregate: {e}\")\n",
        "        return False\n",
        "def generate_article_faq_history(prompt, aggregate: str, approved_sections: list[str]) -> dict | None:\n",
        "    def generate_and_validate(section_name: str, section_prompt: str) -> str | None:\n",
        "        raw = run_ollama4(section_prompt).strip()\n",
        "        if validate_section_html(prompt, section_name, raw):\n",
        "            return raw\n",
        "        print(f\"⚠️ {section_name} section failed validation, attempting fix.\")\n",
        "        fixed = fix_section_html(prompt, section_name, raw)\n",
        "        if fixed and validate_section_html(prompt, section_name, fixed):\n",
        "            return fixed\n",
        "        print(f\"❌ {section_name} section discarded after failed validation and fix.\")\n",
        "        return None\n",
        "    try:\n",
        "        results = {}\n",
        "        plan = \"You will get a summary and write for consumers to read, remove bad info and random questions or info, remove stuff that doesnt make sense, DO NOT INCLUDE EXTRA TEXT, PREFIXES, MARKS, QUOTES OR COMMENTS, ONLY GIVE THE WANTED TEXT NOTHING ELSE, NO HTML, use key words, make it thorough, you work for nearestdoor.com as our writer about churches, you report on churches for nearestdoor.com. must be just text nothing before or after\\n\"\n",
        "        if \"article\" in approved_sections:\n",
        "            article_prompt = (\n",
        "                f\"Using the following plan:\\n\\n{plan}\\n\\n\"\n",
        "                f\"And the following content:\\n\\n{aggregate}\\n\\n\"\n",
        "                f\"Write a detailed, SEO-friendly article about the church {prompt}.\"\n",
        "            )\n",
        "            article = generate_and_validate(\"article\", article_prompt)\n",
        "            if article:\n",
        "                results[\"article\"] = article\n",
        "\n",
        "        if \"faq\" in approved_sections:\n",
        "            faq_prompt = (\n",
        "                f\"Using the following plan:\\n\\n{plan}\\n\\n\"\n",
        "                f\"And the following content:\\n\\n{aggregate}\\n\\n\"\n",
        "                f\"Generate a detailed and useful FAQ section for the church {prompt}.\"\n",
        "            )\n",
        "            faq = generate_and_validate(\"faq\", faq_prompt)\n",
        "            if faq:\n",
        "                results[\"faq\"] = faq\n",
        "\n",
        "\n",
        "        if \"history\" in approved_sections:\n",
        "            history_prompt = (\n",
        "                f\"Using the following plan:\\n\\n{plan}\\n\\n\"\n",
        "                f\"And the following content:\\n\\n{aggregate}\\n\\n\"\n",
        "                f\"Write a backstory/history overview of the church {prompt}:\"\n",
        "            )\n",
        "            history = generate_and_validate(\"history\", history_prompt)\n",
        "            if history:\n",
        "                results[\"history\"] = history\n",
        "\n",
        "        return True, results if results else None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error generating article/faq/history: {e}\")\n",
        "        return False, None\n",
        "\n",
        "\n",
        "import json\n",
        "import re\n",
        "\n",
        "def find_available_fields(aggregate: str) -> list[str] | None:\n",
        "    try:\n",
        "        print('finding available fields', aggregate)\n",
        "        field_list = list(FIELD_EXTRACTORS.keys())\n",
        "        field_str = ', '.join(field_list)\n",
        "\n",
        "        prompt = (\n",
        "            f\"The following content is scraped and summarized:\\n\\n{aggregate}\\n\\n\"\n",
        "            f\"Which of the following fields can be confidently extracted from it?\\n\"\n",
        "            f\"{field_str}\\n\\n\"\n",
        "            \"Only return a list of field keys that are clearly available.\\n\"\n",
        "            \"If a key is not clearly extractable from the content, do not include it in response.\"\n",
        "        )\n",
        "\n",
        "        raw_response = run_ollama4(prompt).strip()\n",
        "        print(f\"🧠 Raw LLM response:\\n{raw_response}\")\n",
        "\n",
        "        # Clean the response AFTER receiving it\n",
        "        cleaned = extract_clean_json_structure(raw_response)\n",
        "        parsed = json.loads(cleaned) if cleaned else []\n",
        "\n",
        "        # If still not parsed cleanly, fallback to basic string match\n",
        "        if not isinstance(parsed, list):\n",
        "            parsed = [field for field in FIELD_EXTRACTORS if field.lower() in raw_response.lower()]\n",
        "\n",
        "        # Always include extract_categories\n",
        "        if \"extract_categories\" not in parsed:\n",
        "            parsed.append(\"extract_categories\")\n",
        "\n",
        "        return True, parsed\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error detecting available fields: {e}\")\n",
        "        return False, None\n",
        "\n",
        "def extract_clean_json_structure(text: str) -> str | None:\n",
        "    try:\n",
        "        match = re.search(r\"(\\[.*?\\]|\\{.*?\\})\", text.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            return None\n",
        "        raw = match.group(0)\n",
        "        json_ready = raw.replace(\"'\", '\"')\n",
        "        parsed = json.loads(json_ready)\n",
        "        if isinstance(parsed, (list, dict)):\n",
        "            return json.dumps(parsed)\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ extract_clean_json_structure failed: {e}\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def extract_fields_from_aggregate(aggregate: str, available_fields: list[str]) -> dict | None:\n",
        "    try:\n",
        "        extracted = {}\n",
        "\n",
        "        for field in available_fields:\n",
        "            prompt = FIELD_EXTRACTORS[field] + \"\\n\\n\" + aggregate\n",
        "            raw_value = run_ollama4(prompt).strip()\n",
        "\n",
        "            # Clean BEFORE validating\n",
        "            cleaned_value = extract_clean_json_structure(raw_value) or raw_value\n",
        "\n",
        "            validator = FIELD_VALIDATORS.get(field)\n",
        "            is_valid = validator(cleaned_value) if validator else True\n",
        "\n",
        "            # If fails local validation, use LLM validator\n",
        "            if not is_valid:\n",
        "                is_valid = check_with_ollama_format_validator(field, cleaned_value)\n",
        "\n",
        "            # If still invalid, try to fix it via LLM\n",
        "            if not is_valid:\n",
        "                print(f\"⚠️ Invalid format for {field}, attempting fix.\")\n",
        "                fixed = attempt_fix_with_ollama(field, cleaned_value)\n",
        "\n",
        "                if fixed and validator(fixed):\n",
        "                    is_valid = check_with_ollama_format_validator(field, fixed)\n",
        "                    if is_valid:\n",
        "                        extracted[field] = fixed\n",
        "                    continue\n",
        "\n",
        "                print(f\"❌ Discarding {field}, failed validation and repair.\")\n",
        "                continue\n",
        "\n",
        "            extracted[field] = cleaned_value\n",
        "            print(\"EXTRACTED\")\n",
        "            print(cleaned_value)\n",
        "\n",
        "        return True, extracted if extracted else None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error extracting fields: {e}\")\n",
        "        return False, None\n",
        "\n",
        "def validate_section_html(prompt,section: str, html: str) -> bool:\n",
        "    try:\n",
        "        prompt = (\n",
        "              \"Reply only with `true` or `false`.\\n\"\n",
        "            f\"You're validating a block of text meant for a '{section}' section about {prompt} for nearestdoor.com.\\n\\n\"\n",
        "            f\"{html}\\n\\n\"\n",
        "\n",
        "            f\"does the info seem valid and real?\\n\"\n",
        "\n",
        "            \"Article must be written as nearestdoor.com writer\\n \"\n",
        "            \"Does the info contain random questions or off topic parts?\\n\"\n",
        "            \" Does it make sense and are there any comments or prefixes before or after the text? It is written as a nearestdoor.com writer reporting on a shop? must be only the text wanted, nothing before or after\\n\"\n",
        "            \"Reply only with `true` or `false`.\\n\"\n",
        "\n",
        "        )\n",
        "        result = run_ollama1(prompt).strip().lower()\n",
        "        return \"true\" in result\n",
        "    except Exception as e:\n",
        "        print(f\"❌ HTML validation failed for section {section}: {e}\")\n",
        "        return False\n",
        "\n",
        "def check_with_ollama_format_validator(field_name: str, value: str) -> bool:\n",
        "    try:\n",
        "        prompt = (\n",
        "              \"Reply only with `true` or `false`.\\n\"\n",
        "            f\"You're validating the format of this field: `{field_name}`.\\n\\n\"\n",
        "            f\"Content:\\n{value}\\n\\n\"\n",
        "\n",
        "            f\"does the info seem valid and real?\\n\"\n",
        "            \"Is the format correct for its expected structure (e.g. phone, JSON, email) and has no comments, quotes, marks or prefixes? nothing before or after json, must be valid json, no '', no ` \\n \"\n",
        "            \"will this parse correctly to json?\\n\"\n",
        "            \"Reply only with `true` or `false`.\\n\"\n",
        "        )\n",
        "        result = run_ollama1(prompt).strip().lower()\n",
        "        return \"true\" in result\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ollama format check failed for {field_name}: {e}\")\n",
        "        return False\n",
        "def attempt_fix_with_ollama(field_name: str, value: str) -> str | None:\n",
        "    try:\n",
        "        fix_prompt = (\n",
        "            f\"The following field is not formatted correctly for `{field_name}`.\\n\\n\"\n",
        "            f\"Broken value:\\n{value}\\n\\n\"\n",
        "             \"Output must parse correctly to json.\\n\"\n",
        "            f\"Fix it to match the expected format based on this instruction:\\n{FIELD_EXTRACTORS[field_name]}\\n\\n\"\n",
        "            \"Only return the fixed value, no comments or explanation, no marks, quotes, or prefixes. nothing before or after json, must be valid json, no ''\"\n",
        "        )\n",
        "        return run_ollama4(fix_prompt).strip()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ollama fix failed for {field_name}: {e}\")\n",
        "        return None\n",
        "def fix_section_html(prompt, section: str, html: str) -> str | None:\n",
        "    try:\n",
        "        prompt = (\n",
        "            f\"The following {section} section text is incorrect or does not follow required formatting rules.\\n\\n\"\n",
        "            f\"{html}\\n\\n\"\n",
        "            f\"based on {prompt}.\\n\"\n",
        "            \"Article must be written as nearestdoor.com writer \\n\"\n",
        "            \"Fix it based on these rules:\\n\"\n",
        "\n",
        "\n",
        "            \"must not contain random info or comments, weird info\\n\"\n",
        "            \"- Be SEO-friendly, concise, and structured.\\n\"\n",
        "            \"no '' \\n\"\n",
        "            \"Only text no html\"\n",
        "            \"must be only the section nothing before or after\\n\"\n",
        "            \"Only return the corrected text. No extra commentary, quotes, prefixes, or marks.\"\n",
        "        )\n",
        "        return run_ollama4(prompt).strip()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to fix {section} section: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "API_BASE = \"https://www.nearestdoor.com\"  # Replace with actual server URL\n",
        "CLIENT_ID = \"client001\"\n",
        "HEARTBEAT_INTERVAL = 60  # seconds\n",
        "SHOP_FLOW_STATIC = [\n",
        "    #all expect completed or failed responses and timeout in 4 mins so u will check in heartbeat every 1 min\n",
        "    \"search\",#gives shopwebsite if any and shop name, calls google wiki, and shopwebsite lookup returns all data from summarize and images with alts base64 if so, first is_main, or none stop\n",
        "  \"aggregate\"#sends data from search and shop data, sends back final summary or none stop\n",
        "    \"createplan\",#sends final summary and shop name, expects \"article\", \"faq\", \"history\" in [] if good, or none\n",
        "    \"create\", #sends final summary and the data [] from createplan and shop data, sends     results[\"history\"] faq article, or none\n",
        "    \"generate_meta_tags_from_aggregate\", #send aggregate and shop name, expect return {  \"title\": title \"description\": description or none  }\n",
        "    \"find_available_fields\", #sends aggregate, expects detected = [field for field in FIELD_EXTRACTORS if field.lower() in response] return detected if detected else [] or None\n",
        "    \"extract_fields_from_aggregate\",#sends aggreagate and result from find avilable fields, expects   extracted[field] = raw_value return extracted if extracted else None or none\n",
        "    \"fillintheshop\",#if youve made it this far add content created to the shop variables\n",
        "]\n",
        "async def handle_task(task, crawler):\n",
        "    try:\n",
        "        if not task or not isinstance(task, dict):\n",
        "            print(\"❌ Skipping empty or malformed task.\")\n",
        "            return\n",
        "\n",
        "        task_id = task.get(\"task_id\")\n",
        "        task_type = task.get(\"task_type\")\n",
        "\n",
        "        if not task_id or not isinstance(task_id, int):\n",
        "            print(f\"❌ Invalid task_id: {task_id}\")\n",
        "            return\n",
        "        if not task_type:\n",
        "            print(\"❌ Missing task_type.\")\n",
        "            return\n",
        "\n",
        "        print(f\"▶️ Task: {task_type} ({task_id})\")\n",
        "\n",
        "        result,summary, mainstring, images, extractedfields, foundfields = False, None, None, None, None, None\n",
        "        match task_type:\n",
        "            case \"search\": result, mainstring, images = await search2(task, crawler)\n",
        "            case \"aggregate\": result, summary = summarize_large_text(task['target'].get('name', ''),task['target'].get('largetext', '') )\n",
        "            case \"createplan\": result,aggregateplan =  create_aggregate_plan(task['target'].get('summary', ''))\n",
        "            case \"create\": result,createdinfo =  generate_article_faq_history(task['target'].get('name', ''), task['target'].get('summary', ''),task['target'].get('aggregateplan', ''))\n",
        "\n",
        "            case \"find_available_fields\": result, foundfields = find_available_fields(task['target'].get('summary', ''))\n",
        "            case \"extract_fields_from_aggregate\": result, extractedfields = extract_fields_from_aggregate(task['target'].get('summary', ''), task['target'].get('foundfields', ''))\n",
        "        print(result, task_type, \"extraxted\",extractedfields,\"found\", foundfields)\n",
        "        if result:\n",
        "            print(f\"📤 Submitting result for {task_type} ({task_id})\")\n",
        "            try:\n",
        "                if task_type == 'search':\n",
        "                    res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"mainstring\": mainstring, \"images\": images, \"client_id\": CLIENT_ID})\n",
        "\n",
        "                if task_type == 'aggregate':\n",
        "                    if summary:\n",
        "                        res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"summary\": summary, \"client_id\": CLIENT_ID})\n",
        "                    else:\n",
        "                        print(\"nosummary\")\n",
        "                        res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"fail\", \"client_id\": CLIENT_ID})\n",
        "\n",
        "\n",
        "                if task_type == 'createplan':\n",
        "                    res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"aggregateplan\": aggregateplan, \"client_id\": CLIENT_ID})\n",
        "                if task_type == 'create':\n",
        "                    res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"createdinfo\":createdinfo, \"client_id\": CLIENT_ID})\n",
        "                if task_type == 'find_available_fields':\n",
        "                    res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"foundfields\": foundfields, \"client_id\": CLIENT_ID})\n",
        "                if task_type == 'extract_fields_from_aggregate':\n",
        "                    res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"extractedfields\": extractedfields, \"client_id\": CLIENT_ID})\n",
        "\n",
        "                print(f\"Server responded: {res.status_code} - {res.text}\")\n",
        "                if res.status_code == 200:\n",
        "                    print(f\"✅ Submitted: {task_type}\")\n",
        "                else:\n",
        "                    print(f\"❌ Submit failed: {task_type} - {res.status_code}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Submit exception: {e}\")\n",
        "        else:\n",
        "\n",
        "            print(f\"Submit Failure {task_type}\")\n",
        "            res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"fail\", \"client_id\": CLIENT_ID})\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Task FAILED, Task: {task_type} ({task_id} {e}) \")\n",
        "        return None\n",
        "def get_task():\n",
        "    try:\n",
        "        print(\"📡 Polling for task...\")\n",
        "        r = requests.get(f\"{API_BASE}/next-task\", params={\"client_id\": CLIENT_ID}, timeout=30)\n",
        "        print(f\"API GET response: {r.status_code} - {r.text}\")\n",
        "\n",
        "        if r.status_code == 200:\n",
        "            try:\n",
        "                task = r.json()\n",
        "                if not isinstance(task, dict) or \"task_id\" not in task:\n",
        "                    print(f\"⚠️ Invalid task structure received: {task}\")\n",
        "                    return None\n",
        "                return task\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Failed to parse JSON: {e}\")\n",
        "                return None\n",
        "\n",
        "        elif r.status_code == 204:\n",
        "            print(\"⏳ No available task.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"❌ Unexpected status code: {r.status_code}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Task fetch failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def poll_heartbeat(current_task_id=None):\n",
        "    try:\n",
        "        data = {\"client_id\": CLIENT_ID}\n",
        "        if current_task_id:\n",
        "            data[\"task_id\"] = current_task_id\n",
        "        requests.post(f\"{API_BASE}/heartbeat\", json=data)\n",
        "        print(\"🫀 Heartbeat sent.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Failed heartbeat: {e}\")\n",
        "\n",
        "async def main():\n",
        "    crawler_mgr = CrawlerManager()\n",
        "    await crawler_mgr.start()\n",
        "\n",
        "    last_heartbeat = 0\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            task = get_task()\n",
        "            if task:\n",
        "                now = time.time()\n",
        "                if now - last_heartbeat > HEARTBEAT_INTERVAL:\n",
        "                    poll_heartbeat(task.get(\"task_id\"))\n",
        "                    last_heartbeat = now\n",
        "                await handle_task(task, crawler_mgr)\n",
        "            else:\n",
        "                print(\"⏳ No task available, waiting...\")\n",
        "                await asyncio.sleep(10)\n",
        "    finally:\n",
        "        await crawler_mgr.stop()\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "await main()\n",
        "\n",
        "# Dictionary of found media (images, videos, audio)\n",
        "\n",
        "\n",
        "#go to shop website if any and scrape html and images, Summarize html and determine good imgs, extract html imgs and alts,\n",
        "# set shop website flag to true, it has been checked\n",
        "#front receive shop website, sends back raw info, main img mainalt, alt imgs altalt,\n",
        "#go to google search and scrape\n",
        "# set googlsearch flag to true, it has been checked\n",
        "#front receive shop name, send back raw info\n",
        "#go to wiki and scrape\n",
        "# set wiki flag to true, it has been checked\n",
        "#front receive shop name, send back raw info\n",
        "\n",
        "#summarize all until below length\n",
        "#front receive chunks sends more chunks back\n",
        "\n",
        "#aggregate\n",
        "\n",
        "# set aggregate flag to true, it has been checked\n",
        "#send all chunks\n",
        "#check if aggregate is good or just stop doing this shop\n",
        "\n",
        "#create aggregate plan\n",
        "#based on aggregate plan reponse make article, faq, history,\n",
        "#based on aggregate + shop name and info create meta title and metadesc\n",
        "#determine which fields can be filled in\n",
        "#run fill in fields create\n",
        "#validate\n",
        "\n",
        "#double check all article faq history meta and other fields\n",
        "\n",
        "#submit all to shop, shop completed"
      ],
      "metadata": {
        "id": "rBsiPCKfTGZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a849e9-e38e-4a60-c4f0-aa02d8f66f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "Requirement already satisfied: crawl4ai in /usr/local/lib/python3.11/dist-packages (0.5.0.post8)\n",
            "Requirement already satisfied: aiosqlite~=0.20 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (0.21.0)\n",
            "Requirement already satisfied: lxml~=5.3 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (5.3.2)\n",
            "Requirement already satisfied: litellm>=1.53.1 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.66.3)\n",
            "Requirement already satisfied: numpy<3,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.0.2)\n",
            "Requirement already satisfied: pillow~=10.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (10.4.0)\n",
            "Requirement already satisfied: playwright>=1.49.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.51.0)\n",
            "Requirement already satisfied: python-dotenv~=1.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.1.0)\n",
            "Requirement already satisfied: requests~=2.26 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4~=4.12 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (4.13.4)\n",
            "Requirement already satisfied: tf-playwright-stealth>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.1.2)\n",
            "Requirement already satisfied: xxhash~=3.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (3.5.0)\n",
            "Requirement already satisfied: rank-bm25~=0.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (0.2.2)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (24.1.0)\n",
            "Requirement already satisfied: colorama~=0.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (0.4.6)\n",
            "Requirement already satisfied: snowballstemmer~=2.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.2.0)\n",
            "Requirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.11.3)\n",
            "Requirement already satisfied: pyOpenSSL>=24.3.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (25.0.0)\n",
            "Requirement already satisfied: psutil>=6.1.1 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (7.0.0)\n",
            "Requirement already satisfied: nltk>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (3.9.1)\n",
            "Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (13.9.4)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.27.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (0.28.1)\n",
            "Requirement already satisfied: fake-useragent>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (8.1.8)\n",
            "Requirement already satisfied: pyperclip>=1.8.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.9.0)\n",
            "Requirement already satisfied: faust-cchardet>=2.1.19 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.1.19)\n",
            "Requirement already satisfied: aiohttp>=3.11.11 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (3.11.15)\n",
            "Requirement already satisfied: humanize>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.11.11->crawl4ai) (1.19.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.11/dist-packages (from aiosqlite~=0.20->crawl4ai) (4.13.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4~=4.12->crawl4ai) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->crawl4ai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->crawl4ai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->crawl4ai) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.2->crawl4ai) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.2->crawl4ai) (0.14.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (8.6.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (4.23.0)\n",
            "Requirement already satisfied: openai>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (1.75.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (0.9.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (0.21.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai) (4.67.1)\n",
            "Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.11/dist-packages (from playwright>=1.49.0->crawl4ai) (12.1.1)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright>=1.49.0->crawl4ai) (3.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->crawl4ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->crawl4ai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->crawl4ai) (0.4.0)\n",
            "Requirement already satisfied: cryptography<45,>=41.0.5 in /usr/local/lib/python3.11/dist-packages (from pyOpenSSL>=24.3.0->crawl4ai) (43.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.26->crawl4ai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.26->crawl4ai) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->crawl4ai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->crawl4ai) (2.18.0)\n",
            "Requirement already satisfied: fake-http-header<0.4.0,>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from tf-playwright-stealth>=1.1.0->crawl4ai) (0.3.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (1.17.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.53.1->crawl4ai) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.53.1->crawl4ai) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.24.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->crawl4ai) (0.1.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm>=1.53.1->crawl4ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm>=1.53.1->crawl4ai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.68.2->litellm>=1.53.1->crawl4ai) (1.3.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->litellm>=1.53.1->crawl4ai) (0.30.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (2.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (6.0.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.19.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (10.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (4.13.2)\n",
            "Requirement already satisfied: googlesearch-python in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (4.13.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (2025.1.31)\n",
            "Requirement already satisfied: playwright in /usr/local/lib/python3.11/dist-packages (1.51.0)\n",
            "Requirement already satisfied: pyee<13,>=12 in /usr/local/lib/python3.11/dist-packages (from playwright) (12.1.1)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from pyee<13,>=12->playwright) (4.13.2)\n",
            "[INIT].... → Crawl4AI 0.5.0.post8\n",
            "📡 Polling for task...\n",
            "API GET response: 200 - {\"task_id\": 622, \"task_type\": \"search\", \"object_type\": \"shop\", \"data\": {}, \"target\": {\"id\": 24820, \"name\": \"Harmony Assembly Of God Paris Arkansas\", \"website\": \"\", \"slug\": \"harmony-assembly-of-god\"}}\n",
            "🫀 Heartbeat sent.\n",
            "▶️ Task: search (622)\n",
            "getting google results\n",
            "Harmony Assembly Of God Paris Arkansas\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "True.\n",
            "✅ Relevant: https://www.usachurches.org/church/harmony-assembly-of-god.htm\n",
            "[FETCH]... ↓ https://www.usachurches.org/church/harmony-assembl... | Status: True | Time: 4.80s\n",
            "[SCRAPE].. ◆ https://www.usachurches.org/church/harmony-assembl... | Time: 0.065s\n",
            "[COMPLETE] ● https://www.usachurches.org/church/harmony-assembl... | Status: True | Total: 4.88s\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "False.\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "False.\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "False.\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "False.\n",
            "getting wiki results\n",
            "📚 Wikipedia lookup for: Harmony Assembly Of God Paris Arkansas\n",
            "Wikipedia fetch failed: Page id \"harmony assembly of god parish arkansas\" does not match any pages. Try another id!\n",
            "done searching\n",
            "got some google results\n",
            "Task FAILED, Task: search (622 cannot access local variable 'extractedfields' where it is not associated with a value) \n",
            "📡 Polling for task...\n",
            "API GET response: 200 - {\"task_id\": 623, \"task_type\": \"search\", \"object_type\": \"shop\", \"data\": {}, \"target\": {\"id\": 24820, \"name\": \"Harmony Assembly Of God Paris Arkansas\", \"website\": \"\", \"slug\": \"harmony-assembly-of-god\"}}\n",
            "▶️ Task: search (623)\n",
            "getting google results\n",
            "Harmony Assembly Of God Paris Arkansas\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "False.\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "False.\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "False.\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "True.\n",
            "✅ Relevant: https://www.usachurches.org/church/harmony-assembly-of-god.htm\n",
            "[FETCH]... ↓ https://www.usachurches.org/church/harmony-assembl... | Status: True | Time: 2.57s\n",
            "[SCRAPE].. ◆ https://www.usachurches.org/church/harmony-assembl... | Time: 0.043s\n",
            "[COMPLETE] ● https://www.usachurches.org/church/harmony-assembl... | Status: True | Total: 2.63s\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "False.\n",
            "getting wiki results\n",
            "📚 Wikipedia lookup for: Harmony Assembly Of God Paris Arkansas\n",
            "Wikipedia fetch failed: Page id \"harmony assembly of god parish arkansas\" does not match any pages. Try another id!\n",
            "done searching\n",
            "got some google results\n",
            "Task FAILED, Task: search (623 cannot access local variable 'extractedfields' where it is not associated with a value) \n",
            "📡 Polling for task...\n",
            "API GET response: 200 - {\"task_id\": 624, \"task_type\": \"search\", \"object_type\": \"shop\", \"data\": {}, \"target\": {\"id\": 24820, \"name\": \"Harmony Assembly Of God Paris Arkansas\", \"website\": \"\", \"slug\": \"harmony-assembly-of-god\"}}\n",
            "▶️ Task: search (624)\n",
            "getting google results\n",
            "Harmony Assembly Of God Paris Arkansas\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "getting wiki results\n",
            "📚 Wikipedia lookup for: Harmony Assembly Of God Paris Arkansas\n",
            "Wikipedia fetch failed: Page id \"harmony assembly of god parish arkansas\" does not match any pages. Try another id!\n",
            "done searching\n",
            "Task FAILED, Task: search (624 cannot access local variable 'extractedfields' where it is not associated with a value) \n",
            "📡 Polling for task...\n",
            "API GET response: 200 - {\"task_id\": 625, \"task_type\": \"search\", \"object_type\": \"shop\", \"data\": {}, \"target\": {\"id\": 24820, \"name\": \"Harmony Assembly Of God Paris Arkansas\", \"website\": \"\", \"slug\": \"harmony-assembly-of-god\"}}\n",
            "▶️ Task: search (625)\n",
            "getting google results\n",
            "Harmony Assembly Of God Paris Arkansas\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "getting wiki results\n",
            "📚 Wikipedia lookup for: Harmony Assembly Of God Paris Arkansas\n",
            "Wikipedia fetch failed: Page id \"harmony assembly of god parish arkansas\" does not match any pages. Try another id!\n",
            "done searching\n",
            "Task FAILED, Task: search (625 cannot access local variable 'extractedfields' where it is not associated with a value) \n",
            "📡 Polling for task...\n",
            "API GET response: 200 - {\"task_id\": 626, \"task_type\": \"search\", \"object_type\": \"shop\", \"data\": {}, \"target\": {\"id\": 24820, \"name\": \"Harmony Assembly Of God Paris Arkansas\", \"website\": \"\", \"slug\": \"harmony-assembly-of-god\"}}\n",
            "▶️ Task: search (626)\n",
            "getting google results\n",
            "Harmony Assembly Of God Paris Arkansas\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "⚠️ Error processing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "getting wiki results\n",
            "📚 Wikipedia lookup for: Harmony Assembly Of God Paris Arkansas\n",
            "Wikipedia fetch failed: Page id \"harmony assembly of god parish arkansas\" does not match any pages. Try another id!\n",
            "done searching\n",
            "Task FAILED, Task: search (626 cannot access local variable 'extractedfields' where it is not associated with a value) \n",
            "📡 Polling for task...\n",
            "API GET response: 200 - {\"task_id\": 627, \"task_type\": \"search\", \"object_type\": \"shop\", \"data\": {}, \"target\": {\"id\": 24820, \"name\": \"Harmony Assembly Of God Paris Arkansas\", \"website\": \"\", \"slug\": \"harmony-assembly-of-god\"}}\n",
            "▶️ Task: search (627)\n",
            "getting google results\n",
            "Harmony Assembly Of God Paris Arkansas\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "getting wiki results\n",
            "📚 Wikipedia lookup for: Harmony Assembly Of God Paris Arkansas\n",
            "Wikipedia fetch failed: Page id \"harmony assembly of god parish arkansas\" does not match any pages. Try another id!\n",
            "done searching\n",
            "Task FAILED, Task: search (627 cannot access local variable 'extractedfields' where it is not associated with a value) \n",
            "📡 Polling for task...\n",
            "API GET response: 200 - {\"task_id\": 628, \"task_type\": \"search\", \"object_type\": \"shop\", \"data\": {}, \"target\": {\"id\": 24820, \"name\": \"Harmony Assembly Of God Paris Arkansas\", \"website\": \"\", \"slug\": \"harmony-assembly-of-god\"}}\n",
            "▶️ Task: search (628)\n",
            "getting google results\n",
            "Harmony Assembly Of God Paris Arkansas\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "⚠️ Error processing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "getting wiki results\n",
            "📚 Wikipedia lookup for: Harmony Assembly Of God Paris Arkansas\n",
            "Wikipedia fetch failed: Page id \"harmony assembly of god parish arkansas\" does not match any pages. Try another id!\n",
            "done searching\n",
            "Task FAILED, Task: search (628 cannot access local variable 'extractedfields' where it is not associated with a value) \n",
            "📡 Polling for task...\n",
            "API GET response: 200 - {\"task_id\": 629, \"task_type\": \"search\", \"object_type\": \"shop\", \"data\": {}, \"target\": {\"id\": 24820, \"name\": \"Harmony Assembly Of God Paris Arkansas\", \"website\": \"\", \"slug\": \"harmony-assembly-of-god\"}}\n",
            "▶️ Task: search (629)\n",
            "getting google results\n",
            "Harmony Assembly Of God Paris Arkansas\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n",
            "⚠️ Error processing : Invalid URL '': No scheme supplied. Perhaps you meant https://?\n",
            "🔍 running ollama for relevance check\n",
            "🧠 Running Ollama with model=gemma3:1b\n",
            "Ollama stdout:\n",
            "\n"
          ]
        }
      ]
    }
  ]
}