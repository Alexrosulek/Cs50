{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexrosulek/Cs50/blob/main/makearticle2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://ollama.ai/install.sh | sh\n",
        "\n",
        "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "!sudo apt-get update && sudo apt-get install -y cuda-drivers\n",
        "\n",
        "import os\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "!nohup ollama serve &\n",
        "\n",
        "\n",
        "\n",
        "!ollama pull gemma3:1b\n",
        "!ollama pull gemma3:4b\n",
        "!pip install ollama\n",
        "!pip install crawl4ai\n",
        "!pip install aiohttp\n",
        "!pip install pillow\n",
        "!pip install beautifulsoup4\n",
        "!pip install wikipedia\n",
        "!pip install googlesearch-python\n",
        "!pip install playwright\n",
        "!playwright install chromium\n",
        "!nohup ollama serve &\n",
        "\n",
        "import asyncio\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig\n",
        "from urllib.parse import urlparse\n",
        "import asyncio\n",
        "import random\n",
        "import time\n",
        "import aiohttp\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import base64\n",
        "import io\n",
        "import subprocess\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import wikipedia\n",
        "\n",
        "import textwrap\n",
        "\n",
        "import re\n",
        "import json\n",
        "from googlesearch import search"
      ],
      "metadata": {
        "id": "-b9oppEb4cm2",
        "outputId": "55d8bf7a-3880-45e9-9ac0-1cfe32b636b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13281    0 13281    0     0  36865      0 --:--:-- --:--:-- --:--:-- 36891\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "###################################################################       93.7%"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "folder = \"downloaded_images\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "def is_valid_phone(value: str) -> bool:\n",
        "    print(value)\n",
        "    return bool(re.fullmatch(r\"\\d{3}-\\d{3}-\\d{4}\", value.strip()))\n",
        "\n",
        "def is_valid_email(value: str) -> bool:\n",
        "    print(value)\n",
        "    return bool(re.fullmatch(r\"[^@\\s]+@[^@\\s]+\\.[a-zA-Z0-9]+\", value.strip()))\n",
        "\n",
        "def is_valid_url(value: str) -> bool:\n",
        "    print(value)\n",
        "    return value.strip().startswith(\"http\")\n",
        "\n",
        "def is_valid_json(value: str) -> bool:\n",
        "    print(value)\n",
        "    try:\n",
        "        json.loads(value)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def is_non_empty_string(value: str) -> bool:\n",
        "    print(value)\n",
        "    return isinstance(value, str) and len(value.strip()) > 0\n",
        "\n",
        "def is_valid_dict(value) -> bool:\n",
        "    print(value)\n",
        "    return isinstance(value, dict)\n",
        "FIELD_VALIDATORS = {\n",
        "    \"extract_phone\": is_valid_phone,\n",
        "    \"extract_email\": is_valid_email,\n",
        "    \"extract_website\": is_valid_url,\n",
        "    \"extract_operating_hours\": is_valid_dict,\n",
        "    \"extract_holiday_hours\": is_valid_dict,\n",
        "    \"extract_categories\": is_valid_json,\n",
        "    \"extract_delivery_services\": is_valid_json,\n",
        "    \"extract_social_media\": is_valid_json,\n",
        "    \"extract_stocked_brands\": is_valid_json,\n",
        "    \"extract_inventory_categories\": is_valid_json,\n",
        "    \"extract_customer_reviews\": is_valid_json,\n",
        "    \"extract_admission\": is_non_empty_string,\n",
        "    \"extract_date_available\": is_non_empty_string,\n",
        "    \"extract_attendance_amount\": is_non_empty_string,\n",
        "    \"extract_exhibitor_amount\": is_non_empty_string\n",
        "}\n",
        "FIELD_EXTRACTORS = {\n",
        "        \"extract_phone\": 'From the content, extract the phone number in this format: \"727-123-4567\"\\nOnly return the phone string, no extra text, prefixes, marks, or quotes, only the phone number itself.',\n",
        "        \"extract_email\": 'From the content, extract the email address in this format: \"name@example.com\"\\nOnly return the email string, no extra text, prefixes,  marks, or quotes, only the email itself.',\n",
        "        \"extract_website\": 'From the content, extract the website URL in this format: \"https://example.com\"\\nOnly return the URL string, no extra text, prefixes,  marks, or quotes, only the website url itself with https://.',\n",
        "        \"extract_operating_hours\": \"\"\"From the content, extract operating hours in this format, no extra text, prefixes, marks or quotes, only the dictionary itself:\n",
        "{\n",
        "  \"monday\": \"9:00 AM - 5:00 PM\",\n",
        "  \"tuesday\": \"9:00 AM - 5:00 PM\",\n",
        "  \"wednesday\": \"9:00 AM - 5:00 PM\",\n",
        "  \"thursday\": \"9:00 AM - 5:00 PM\",\n",
        "  \"friday\": \"9:00 AM - 5:00 PM\",\n",
        "  \"saturday\": \"Closed\",\n",
        "  \"sunday\": \"Closed\"\n",
        "}\n",
        "Only return DICTIONARY, no extra text. no `, nothing before or after dictionary, ONLY VALID DICTIONARY.\"\"\",\n",
        "        \"extract_holiday_hours\": \"From the content, extract holiday hours in this format: {'2024-12-25': 'Closed', '2024-12-31': '10:00 AM - 4:00 PM'}\\nOnly return DICTIONARY, no extra text, prefixes,  marks or quotes, only the dictionary itself. no `, nothing before or after dictionary, ONLY VALID DICTIONARY.\",\n",
        "        \"extract_categories\": \"From the content, extract shop categories in this format: ['Dog trail', 'Dog Park']\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself. no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_delivery_services\": \"From the content, extract delivery services in this format: ['Uber Eats', 'Self Delivery']\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself.  no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_social_media\": \"From the content, extract social media links in this format: {'facebook': 'url', 'instagram': 'url'}\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself. no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_stocked_brands\": \"From the content, extract stocked brands in this format: ['Nike', 'Adidas']\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself. no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_inventory_categories\": \"From the content, extract inventory categories in this format: {'Apparel': ['Shirts', 'Hoodies']}\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself.  no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_customer_reviews\": \"From the content, extract customer reviews in this format: [{'user'': 'John', 'comment': 'Great!', 'rating': 5}]\\nOnly return JSON, no extra text, prefixes,  marks or quotes, only the json itself. no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_admission\": \"From the content, extract the admission info as a string. Only return the string, no extra text, prefixes, marks or quotes, only the string admission itself.  no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_date_available\": \"From the content, extract date available as a string. Only return the string, no extra text, prefixes, marks or quotes, only the string date available itself. no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_attendance_amount\": \"From the content, extract attendance amount as a string. Only return the string, no extra text, prefixes, marks or quotes, only the string attendence amount itself. no `, nothing before or after json, ONLY VALID JSON.\",\n",
        "        \"extract_exhibitor_amount\": \"From the content, extract exhibitor amount as a string. Only return the string, no extra text, prefixes, marks or quotes, only the string exhibitor amount itself. no `, nothing before or after json, ONLY VALID JSON. \"\n",
        "    }\n",
        "import random\n",
        "import wikipedia\n",
        "\n",
        "async def handle_wikipedia(task):\n",
        "    try:\n",
        "        name = task['target'].get(\"name\")\n",
        "        city = task['target'].get(\"city\", \"\")\n",
        "        query = f\"{name} {city}\".strip()\n",
        "        print(f\"📚 Wikipedia lookup for: {query}\")\n",
        "\n",
        "        page = wikipedia.page(query, auto_suggest=True)\n",
        "        content = page.content\n",
        "\n",
        "        # If short, return all\n",
        "        if len(content) <= 2000:\n",
        "            return content\n",
        "\n",
        "        # Otherwise, chunk and pick 4 from middle\n",
        "        chunk_size = 500\n",
        "        chunks = [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]\n",
        "\n",
        "        if len(chunks) <= 6:\n",
        "            selected_chunks = chunks  # Not enough to skip ends\n",
        "        else:\n",
        "            middle_chunks = chunks[2:-2]\n",
        "            selected_chunks = random.sample(middle_chunks, min(4, len(middle_chunks)))\n",
        "\n",
        "        return \"\\n\\n\".join(selected_chunks)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Wikipedia fetch failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "async def handle_google_search(task, crawler):\n",
        "    try:\n",
        "        target = task['target']\n",
        "        query = f\"{target.get('name', '')} {target.get('city', '')} {target.get('state', '')}\".strip()\n",
        "        print(query)\n",
        "        valid_results = ''\n",
        "\n",
        "        results = list(search(query))\n",
        "\n",
        "        # Randomly shuffle and pick half\n",
        "        half_results = random.sample(results, max(1, len(results) // 2))\n",
        "\n",
        "\n",
        "\n",
        "        for url in half_results:\n",
        "            try:\n",
        "                res = requests.get(url, timeout=10, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
        "                soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "                title = soup.title.string.strip() if soup.title else \"No title\"\n",
        "                meta_tag = soup.find(\"meta\", {\"name\": \"description\"})\n",
        "                description = meta_tag[\"content\"].strip() if meta_tag else \"No description\"\n",
        "\n",
        "                prompt = (\n",
        "                    f\"Is this page about the query '{query}'? Be very strict, no random things this is for a specific dog park.\\n\\n\"\n",
        "                    f\"Title: {title}\\n\"\n",
        "                    f\"Description: {description}\\n\\n\"\n",
        "                    f\"Return true or false.\"\n",
        "                )\n",
        "\n",
        "                print(\"🔍 running ollama for relevance check\")\n",
        "                result = run_ollama1(prompt)\n",
        "                if \"true\" in result.lower():\n",
        "                    print(f\"✅ Relevant: {url}\")\n",
        "                    content = await getsite(url, crawler)\n",
        "                    if content and content.success:\n",
        "                        markdown = content.markdown.fit_markdown\n",
        "                        if len(markdown) <= 1500:\n",
        "                            valid_results += markdown\n",
        "\n",
        "                        # Chunk and pick 4 from the middle\n",
        "                        chunk_size = 500\n",
        "                        chunks = [markdown[i:i + chunk_size] for i in range(0, len(markdown), chunk_size)]\n",
        "                        if len(chunks) <= 6:\n",
        "                            selected_chunks = chunks\n",
        "                        else:\n",
        "                            selected_chunks = random.sample(chunks[2:-2], min(4, len(chunks[2:-2])))\n",
        "\n",
        "                        valid_results += \"\\n\\n\".join(selected_chunks) + f\"\\n\\n<- EXTRACTED FROM THIS URL: {url}\\n\\n\"\n",
        "            except Exception as inner_err:\n",
        "                print(f\"⚠️ Error processing {url}: {inner_err}\")\n",
        "\n",
        "        if valid_results != '':\n",
        "            return valid_results\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in handle_google_search: {e}\")\n",
        "        return None\n",
        "\n",
        "async def search2(task, crawler):\n",
        "    print(\"getting google results\")\n",
        "    google_results = await handle_google_search(task, crawler)\n",
        "    print(\"getting wiki results\")\n",
        "    wiki_results = await handle_wikipedia(task)\n",
        "\n",
        "    imageandalt = None\n",
        "    result = None\n",
        "    images = None\n",
        "\n",
        "    print(\"done searching\")\n",
        "    mainstring = ''\n",
        "    if google_results:\n",
        "        print(\"got some google results\")\n",
        "        mainstring = google_results\n",
        "        if wiki_results:\n",
        "            print(\"got some wiki results\")\n",
        "            mainstring += wiki_results\n",
        "    if result:\n",
        "        if result.success:\n",
        "\n",
        "            mainstring += result.markdown.fit_markdown\n",
        "    if mainstring == '' or len(mainstring) < 500:\n",
        "        if not imageandalt:\n",
        "            return False, None, None\n",
        "        return True, None, imageandalt\n",
        "    return True, mainstring, imageandalt\n",
        "def resize_image(input_path, size):\n",
        "    try:\n",
        "        img = Image.open(input_path).convert(\"RGB\")\n",
        "        img = img.resize(size, Image.ANTIALIAS)\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Resize failed for {input_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def image_to_base64(img: Image.Image) -> str:\n",
        "    try:\n",
        "        buffer = io.BytesIO()\n",
        "        img.save(buffer, format=\"JPEG\")\n",
        "        return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ base64: {e}\")\n",
        "        return None\n",
        "\n",
        "def run_ollama_with_image(prompt: str, image_base64: str, model=\"gemma3:4b\") -> str:\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            \"http://localhost:11434/api/generate\",\n",
        "            json={\n",
        "                \"model\": model,\n",
        "                \"prompt\": prompt,\n",
        "                \"images\": [image_base64],\n",
        "                \"stream\": False\n",
        "            },\n",
        "            timeout=256\n",
        "        )\n",
        "        response.raise_for_status()\n",
        "        result = response.json()\n",
        "        return result.get(\"response\", \"\").strip().lower()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ollama API call failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def run_ollama4(prompt, model=\"gemma3:4b\"):\n",
        "    print(f\"🧠 Running Ollama with model={model}\")\n",
        "    try:\n",
        "        proc = subprocess.run(\n",
        "            [\"ollama\", \"run\", model],\n",
        "            input=prompt.encode(\"utf-8\"),\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            timeout=556\n",
        "        )\n",
        "        stdout = proc.stdout.decode(\"utf-8\").strip()\n",
        "        stderr = proc.stderr.decode(\"utf-8\").strip()\n",
        "        print(f\"Ollama stdout:\\n{stdout}\")\n",
        "        if stderr:\n",
        "            print(f\"Ollama stderr:\\n{stderr}\")\n",
        "        return stdout\n",
        "    except Exception as e:\n",
        "        print(f\"Ollama run failed: {e}\")\n",
        "        return \"\"\n",
        "def run_ollama1(prompt, model=\"gemma3:1b\"):\n",
        "    print(f\"🧠 Running Ollama with model={model}\")\n",
        "    try:\n",
        "        proc = subprocess.run(\n",
        "            [\"ollama\", \"run\", model],\n",
        "            input=prompt.encode(\"utf-8\"),\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            timeout=356\n",
        "        )\n",
        "        stdout = proc.stdout.decode(\"utf-8\").strip()\n",
        "        stderr = proc.stderr.decode(\"utf-8\").strip()\n",
        "        print(f\"Ollama stdout:\\n{stdout}\")\n",
        "        return stdout\n",
        "    except Exception as e:\n",
        "        print(f\"Ollama run failed: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def validate_images(name,images: list[dict]) -> list[dict] | None:\n",
        "    \"\"\"\n",
        "    Takes [{'filename': ..., 'alt': ...}]\n",
        "    Returns [{'alt': ..., 'base64': ...}] — resized to 1536x1024\n",
        "    Returns None on error.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "\n",
        "        approved = []\n",
        "\n",
        "        for img_info in images:\n",
        "            filename = img_info[\"filename\"]\n",
        "            alt_text = img_info.get(\"alt\") or \"No description\"\n",
        "\n",
        "            # Step 1: Resize to 512x512 for judging\n",
        "            resized_512 = resize_image(filename, (512, 512))\n",
        "            if not resized_512:\n",
        "                continue\n",
        "\n",
        "            img_512_b64 = image_to_base64(resized_512)\n",
        "            if not img_512_b64:\n",
        "                continue\n",
        "\n",
        "            prompt = (\n",
        "                f\"Based on the description: \\\"{alt_text}\\\", \"\n",
        "                f\"is this image good for dog park {name}? Answer only true or false.\"\n",
        "            )\n",
        "\n",
        "            result = run_ollama_with_image(prompt, img_512_b64)\n",
        "            if \"true\" not in result:\n",
        "                print(f\"🛑 Image rejected: {filename}\")\n",
        "                continue\n",
        "\n",
        "            # Step 2: Resize to 1536x1024 for output\n",
        "            final_img = resize_image(filename, (1536, 1024))\n",
        "            if not final_img:\n",
        "                continue\n",
        "\n",
        "            encoded_final = image_to_base64(final_img)\n",
        "            approved.append({\n",
        "                \"alt\": alt_text,\n",
        "                \"base64\": encoded_final\n",
        "            })\n",
        "\n",
        "        if os.path.exists(folder):\n",
        "            shutil.rmtree(folder)\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "        return approved\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        if os.path.exists(folder):\n",
        "            shutil.rmtree(folder)\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "        print(f\"❌ Error in validate_images: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def normalize_and_validate_url(url: str) -> str | None:\n",
        "    try:\n",
        "        # Normalize: ensure https://\n",
        "        if not url.startswith(\"https://\"):\n",
        "            if url.startswith(\"http://\"):\n",
        "                url = url.replace(\"http://\", \"https://\", 1)\n",
        "            else:\n",
        "                url = \"https://\" + url.lstrip(\"/\")\n",
        "\n",
        "        # Validate\n",
        "        parsed = urlparse(url)\n",
        "        if not parsed.scheme or not parsed.netloc:\n",
        "            return None  # Invalid URL\n",
        "\n",
        "        return url\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error normalizing/validating {url}: {e}\")\n",
        "        return None\n",
        "class CrawlerManager:\n",
        "    def __init__(self):\n",
        "        self.crawler = None\n",
        "\n",
        "    async def start(self):\n",
        "        if self.crawler is None:\n",
        "            self.crawler = AsyncWebCrawler(config=BrowserConfig())\n",
        "            await self.crawler.__aenter__()\n",
        "\n",
        "    async def stop(self):\n",
        "        if self.crawler:\n",
        "            await self.crawler.__aexit__(None, None, None)\n",
        "            self.crawler = None\n",
        "\n",
        "    async def crawl(self, url: str):\n",
        "        if not self.crawler:\n",
        "            raise RuntimeError(\"Crawler not started\")\n",
        "\n",
        "        url = normalize_and_validate_url(url)\n",
        "        if not url:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            run_config = CrawlerRunConfig()\n",
        "            result = await self.crawler.arun(url=url, config=run_config)\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f\"❌ crawl error for {url}: {e}\")\n",
        "            return None\n",
        "async def getsite(url: str, crawler_mgr: CrawlerManager):\n",
        "    return await crawler_mgr.crawl(url)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "async def download_image(session, img, idx):\n",
        "\n",
        "    try:\n",
        "        url = img[\"src\"]\n",
        "        alt = img.get(\"alt\") or img.get(\"desc\", \"No alt/desc\")\n",
        "        ext = url.split('.')[-1].split('?')[0]\n",
        "        filename = f\"downloaded_images/image_{idx}.{ext}\"\n",
        "\n",
        "        async with session.get(url) as resp:\n",
        "            if resp.status == 200:\n",
        "                with open(filename, 'wb') as f:\n",
        "                    f.write(await resp.read())\n",
        "                print(f\"✅ Downloaded: {filename}\")\n",
        "                return {\"filename\": filename, \"alt\": alt}\n",
        "            else:\n",
        "                print(f\"❌ Failed to download {url} (status {resp.status})\")\n",
        "                return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error downloading {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "async def findimages(result):\n",
        "    try:\n",
        "\n",
        "        images = result.media.get(\"images\", [])\n",
        "        if not images:\n",
        "            print(\"No images found.\")\n",
        "            return None\n",
        "\n",
        "        # Pick up to 8 random images\n",
        "        selected_images = random.sample(images, min(8, len(images)))\n",
        "\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            tasks = [\n",
        "                download_image(session, img, idx)\n",
        "                for idx, img in enumerate(selected_images)\n",
        "            ]\n",
        "            results = await asyncio.gather(*tasks)\n",
        "\n",
        "        # Filter out failed downloads\n",
        "        successful = [r for r in results if r]\n",
        "\n",
        "        return successful\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error finding images: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def summarize_large_text(prompt, large_text: str, max_length: int = 4000) -> str | None:\n",
        "    try:\n",
        "        # Split large text into chunks (~1024 chars each)\n",
        "        chunks = textwrap.wrap(large_text, width=2000)\n",
        "        print(len(chunks), 'got this many chunks')\n",
        "        summaries = []\n",
        "        for idx, chunk in enumerate(chunks):\n",
        "            prompt = f\"Extract dog park info. keep facts, keep details about dog park {prompt}, keep operating hours ect, all details, REMOVE HTML AND JS, u are not doing anything else but extracting/consolidating:\\n\\n{chunk}\"\n",
        "            summary = run_ollama1(prompt)\n",
        "            if summary:\n",
        "                summaries.append(summary)\n",
        "            else:\n",
        "                print(f\"⚠️ Failed to summarize chunk {idx}\")\n",
        "\n",
        "        # Aggregate all summaries\n",
        "        combined = \"\".join(summaries)\n",
        "\n",
        "        if len(combined) > max_length:\n",
        "            print(\"summarizing again\")\n",
        "            chunks = textwrap.wrap(combined, width=2000)\n",
        "\n",
        "            summaries = []\n",
        "            for idx, chunk in enumerate(chunks):\n",
        "                prompt = f\"Extract dog park info concisely, based on dog park {prompt}, keep operating hours phone website ect all details, u are not doing anything else but extracting/consolidating, return it back do not tell me how good it is, no other objectives or questions:\\n\\n{chunk}\"\n",
        "\n",
        "                summary = run_ollama1(prompt)\n",
        "                if summary:\n",
        "                    summaries.append(summary)\n",
        "                else:\n",
        "                    print(f\"⚠️ Failed to summarize chunk {idx}\")\n",
        "\n",
        "        combined = \"\".join(summaries)\n",
        "        if len(combined) > max_length:\n",
        "\n",
        "            print(\"summarizing again\")\n",
        "            chunks = textwrap.wrap(combined, width=2000)\n",
        "\n",
        "            summaries = []\n",
        "            for idx, chunk in enumerate(chunks):\n",
        "                prompt = f\"Summarize shortly make shorter than it is. based on dog park {prompt}, keep all details and facts, u are not doing anything else but summarizing/consolidating, return it back do not tell me how good it is, no other objectives or questions:\\n\\n{chunk}\"\n",
        "\n",
        "                summary = run_ollama1(prompt)\n",
        "                if summary:\n",
        "                    summaries.append(summary)\n",
        "                else:\n",
        "                    print(f\"⚠️ Failed to summarize chunk {idx}\")\n",
        "\n",
        "        combined = \"\\n\".join(summaries)\n",
        "        prompt = f\"You work for nearestdoor.com. This is scraped info from the internet based on dog park {prompt}, keep all info/details, keep all information/facts/details operating hours phone website ect and turn it into an overview, no other objectives or questions, it is 2025:\\n\\n{combined}\"\n",
        "\n",
        "        summary = run_ollama4(prompt)\n",
        "        if check_if_aggregate_is_good(prompt,summary):\n",
        "            return True, summary\n",
        "        else:\n",
        "            return False, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in summarize_large_text: {e}\")\n",
        "        return False, None\n",
        "def create_aggregate_plan(aggregate: str) -> bool:\n",
        "    try:\n",
        "        prompt = (\n",
        "            \"Determine if this information is enough to generate an 'article', 'faq', 'history' snippet.\\n\\n\"\n",
        "            f\"{aggregate}\\n\\n\"\n",
        "            \"Reply only with the names of sections that can be created: 'article', 'faq', 'history'.\\n \"\n",
        "            \"Do not include a section name if there isn't enough information to generate that section.\"\n",
        "        )\n",
        "\n",
        "        response = run_ollama4(prompt)\n",
        "        response_clean = response.strip().lower()\n",
        "\n",
        "        valid = {\"article\", \"faq\", \"history\"}\n",
        "        sections = [s for s in valid if s in response_clean]\n",
        "\n",
        "        return True, sections if sections else []\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error checking aggregate: {e}\")\n",
        "        return False, None\n",
        "def check_if_aggregate_is_good(prompt, aggregate: str) -> bool:\n",
        "    try:\n",
        "        prompt = (\n",
        "            f\"You are validating an overview of scraped content from multiple sources based on shop {prompt}.\\n\"\n",
        "            f\"{aggregate}\\n\\n\"\n",
        "            f\"Reply only `true` or `false` — is the summary solid and contains useful facts and is about {prompt}? \"\n",
        "        )\n",
        "        result = run_ollama4(prompt).strip().lower()\n",
        "        return \"true\" in result\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error checking aggregate: {e}\")\n",
        "        return False\n",
        "def generate_article_faq_history(prompt, aggregate: str, approved_sections: list[str]) -> dict | None:\n",
        "    def generate_and_validate(section_name: str, section_prompt: str) -> str | None:\n",
        "        raw = run_ollama4(section_prompt).strip()\n",
        "        if validate_section_html(prompt, section_name, raw):\n",
        "            return raw\n",
        "        print(f\"⚠️ {section_name} section failed validation, attempting fix.\")\n",
        "        fixed = fix_section_html(prompt, section_name, raw)\n",
        "        if fixed and validate_section_html(prompt, section_name, fixed):\n",
        "            return fixed\n",
        "        print(f\"❌ {section_name} section discarded after failed validation and fix.\")\n",
        "        return None\n",
        "    try:\n",
        "        results = {}\n",
        "        plan = f\"You will get a summary and write for consumers to read as a nearestdoor.com writer reporting on {prompt}, remove bad info and random questions or info, remove stuff that doesnt make sense, DO NOT INCLUDE EXTRA TEXT, PREFIXES, MARKS, QUOTES OR COMMENTS, ONLY GIVE THE WANTED TEXT NOTHING ELSE, NO HTML, use key words, make it thorough, you work for nearestdoor.com as our writer about dog parks, you report on dog parks for nearestdoor.com. must be just text nothing before or after, DO NOT USE ** or *, no asterisks\\n\"\n",
        "        if \"article\" in approved_sections:\n",
        "            article_prompt = (\n",
        "                f\"Using the following plan:\\n\\n{plan}\\n\\n\"\n",
        "                f\"And the following content:\\n\\n{aggregate}\\n\\n\"\n",
        "                f\"Write a detailed, SEO-friendly article about the dog park {prompt}.\"\n",
        "            )\n",
        "            article = generate_and_validate(\"article\", article_prompt)\n",
        "            if article:\n",
        "                results[\"article\"] = article\n",
        "\n",
        "        if \"faq\" in approved_sections:\n",
        "            faq_prompt = (\n",
        "                f\"Using the following plan:\\n\\n{plan}\\n\\n\"\n",
        "                f\"And the following content:\\n\\n{aggregate}\\n\\n\"\n",
        "                f\"Generate a detailed and useful FAQ section for the dog park {prompt}.\"\n",
        "            )\n",
        "            faq = generate_and_validate(\"faq\", faq_prompt)\n",
        "            if faq:\n",
        "                results[\"faq\"] = faq\n",
        "\n",
        "\n",
        "        if \"history\" in approved_sections:\n",
        "            history_prompt = (\n",
        "                f\"Using the following plan:\\n\\n{plan}\\n\\n\"\n",
        "                f\"And the following content:\\n\\n{aggregate}\\n\\n\"\n",
        "                f\"Write a backstory/history overview of the dog park {prompt}:\"\n",
        "            )\n",
        "            history = generate_and_validate(\"history\", history_prompt)\n",
        "            if history:\n",
        "                results[\"history\"] = history\n",
        "\n",
        "        return True, results if results else None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error generating article/faq/history: {e}\")\n",
        "        return False, None\n",
        "\n",
        "\n",
        "import json\n",
        "import re\n",
        "def find_available_fields(aggregate):\n",
        "    try:\n",
        "        print('finding available fields', aggregate)\n",
        "        field_list = list(FIELD_EXTRACTORS.keys())\n",
        "        field_str = ', '.join(field_list)\n",
        "\n",
        "        prompt = (\n",
        "            f\"The following content is scraped and summarized:\\n\\n{aggregate}\\n\\n\"\n",
        "            f\"Which of the following fields can be confidently extracted from it?\\n\"\n",
        "            f\"{field_str}\\n\\n\"\n",
        "            \"Only return a list of field keys that are clearly available.\\n\"\n",
        "            \"If a key is not clearly extractable from the content, do not include it in response.\"\n",
        "        )\n",
        "\n",
        "        raw_response = run_ollama4(prompt).strip()\n",
        "        print(f\"🧠 Raw LLM response:\\n{raw_response}\")\n",
        "\n",
        "        # Normalize and clean up the response text\n",
        "        response_clean = raw_response.lower()\n",
        "\n",
        "        # Now just match by substring presence (don't trust format)\n",
        "        detected = [field for field in field_list if field.lower() in response_clean]\n",
        "\n",
        "        # Always include extract_categories (can always try to infer)\n",
        "        if \"extract_categories\" not in detected:\n",
        "            detected.append(\"extract_categories\")\n",
        "\n",
        "        return True, detected\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error detecting available fields: {e}\")\n",
        "        return False, None\n",
        "\n",
        "\n",
        "def extract_clean_json_structure(text):\n",
        "    try:\n",
        "        match = re.search(r\"(|\\{.*?\\})\", text.strip(), re.DOTALL)\n",
        "        if not match:\n",
        "            return None\n",
        "        raw = match.group(0)\n",
        "        json_ready = raw.replace(\"'\", '\"')\n",
        "        return json.loads(json_ready)  # ✅ Returns a real dict or list\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ extract_clean_json_structure failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "def extract_fields_from_aggregate(aggregate: str, available_fields: list[str]) -> dict | None:\n",
        "    try:\n",
        "        extracted = {}\n",
        "\n",
        "        for field in available_fields:\n",
        "            prompt = FIELD_EXTRACTORS[field] + \"\\n\\n\" + aggregate\n",
        "            raw_value = run_ollama4(prompt).strip()\n",
        "\n",
        "            # Clean BEFORE validating\n",
        "            cleaned_value = extract_clean_json_structure(raw_value) or raw_value\n",
        "\n",
        "            validator = FIELD_VALIDATORS.get(field)\n",
        "            is_valid = validator(cleaned_value) if validator else True\n",
        "\n",
        "            # If fails local validation, use LLM validator\n",
        "            if not is_valid:\n",
        "                is_valid = check_with_ollama_format_validator(field, cleaned_value)\n",
        "\n",
        "            # If still invalid, try to fix it via LLM\n",
        "            if not is_valid:\n",
        "                print(f\"⚠️ Invalid format for {field}, attempting fix.\")\n",
        "                fixed = attempt_fix_with_ollama(field, cleaned_value)\n",
        "\n",
        "                if fixed and validator(fixed):\n",
        "                    is_valid = check_with_ollama_format_validator(field, fixed)\n",
        "                    if is_valid:\n",
        "                        extracted[field] = fixed\n",
        "                    continue\n",
        "\n",
        "                print(f\"❌ Discarding {field}, failed validation and repair.\")\n",
        "                continue\n",
        "\n",
        "            extracted[field] = cleaned_value\n",
        "            print(\"EXTRACTED\")\n",
        "            print(cleaned_value)\n",
        "\n",
        "        return True, extracted if extracted else None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error extracting fields: {e}\")\n",
        "        return False, None\n",
        "\n",
        "def validate_section_html(prompt,section: str, html: str) -> bool:\n",
        "    try:\n",
        "        prompt = (\n",
        "              \"Reply only with `true` or `false`.\\n\"\n",
        "            f\"You're validating a block of text meant for a '{section}' section about {prompt} for nearestdoor.com.\\n\\n\"\n",
        "            f\"{html}\\n\\n\"\n",
        "            \"No ** or *, no asterisks\\n\"\n",
        "            f\"does the info seem valid and real?\\n\"\n",
        "\n",
        "            \"Article must be written as nearestdoor.com writer\\n \"\n",
        "            \"Does the info contain random questions or off topic parts?\\n\"\n",
        "            \" Does it make sense and are there any comments or prefixes before or after the text? It is written as a nearestdoor.com writer reporting on a shop? must be only the text wanted, nothing before or after\\n\"\n",
        "            \"Reply only with `true` or `false`.\\n\"\n",
        "\n",
        "        )\n",
        "        result = run_ollama4(prompt).strip().lower()\n",
        "        return \"true\" in result\n",
        "    except Exception as e:\n",
        "        print(f\"❌ HTML validation failed for section {section}: {e}\")\n",
        "        return False\n",
        "\n",
        "def check_with_ollama_format_validator(field_name: str, value: str) -> bool:\n",
        "    try:\n",
        "        prompt = (\n",
        "              \"Reply only with `true` or `false`.\\n\"\n",
        "            f\"You're validating the format of this field: `{field_name}`.\\n\\n\"\n",
        "            f\"Content:\\n{value}\\n\\n\"\n",
        "\n",
        "            f\"does the info seem valid and real?\\n\"\n",
        "            \"Is the format correct for its expected structure (e.g. phone, JSON, email) and has no comments, quotes, marks or prefixes? nothing before or after json, must be valid json, no '', no ` \\n \"\n",
        "            \"will this parse correctly to json?\\n\"\n",
        "            \"Reply only with `true` or `false`.\\n\"\n",
        "        )\n",
        "        result = run_ollama4(prompt).strip().lower()\n",
        "        return \"true\" in result\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ollama format check failed for {field_name}: {e}\")\n",
        "        return False\n",
        "def attempt_fix_with_ollama(field_name: str, value: str) -> str | None:\n",
        "    try:\n",
        "        fix_prompt = (\n",
        "            f\"The following field is not formatted correctly for `{field_name}`.\\n\\n\"\n",
        "            f\"Broken value:\\n{value}\\n\\n\"\n",
        "             \"Output must parse correctly to json.\\n\"\n",
        "            f\"Fix it to match the expected format based on this instruction:\\n{FIELD_EXTRACTORS[field_name]}\\n\\n\"\n",
        "            \"Only return the fixed value, no comments or explanation, no marks, quotes, or prefixes. nothing before or after json, must be valid json, no ''\"\n",
        "        )\n",
        "        return run_ollama4(fix_prompt).strip()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Ollama fix failed for {field_name}: {e}\")\n",
        "        return None\n",
        "def fix_section_html(prompt, section: str, html: str) -> str | None:\n",
        "    try:\n",
        "        prompt = (\n",
        "            f\"The following {section} section text is incorrect or does not follow required formatting rules.\\n\\n\"\n",
        "            f\"{html}\\n\\n\"\n",
        "            f\"based on {prompt}.\\n\"\n",
        "            \"Article must be written as nearestdoor.com writer \\n\"\n",
        "            \"Fix it based on these rules:\\n\"\n",
        "  \"No ** or *, no asterisks\\n\"\n",
        "\n",
        "            \"must not contain random info or comments, weird info\\n\"\n",
        "            \"- Be SEO-friendly, concise, and structured.\\n\"\n",
        "            \"no '' \\n\"\n",
        "            \"Only text no html\"\n",
        "            \"must be only the section nothing before or after\\n\"\n",
        "            \"Only return the corrected text. No extra commentary, quotes, prefixes, or marks.\"\n",
        "        )\n",
        "        return run_ollama4(prompt).strip()\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to fix {section} section: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "API_BASE = \"https://www.nearestdoor.com\"  # Replace with actual server URL\n",
        "CLIENT_ID = \"client001\"\n",
        "HEARTBEAT_INTERVAL = 60  # seconds\n",
        "SHOP_FLOW_STATIC = [\n",
        "    #all expect completed or failed responses and timeout in 4 mins so u will check in heartbeat every 1 min\n",
        "    \"search\",#gives shopwebsite if any and shop name, calls google wiki, and shopwebsite lookup returns all data from summarize and images with alts base64 if so, first is_main, or none stop\n",
        "  \"aggregate\"#sends data from search and shop data, sends back final summary or none stop\n",
        "    \"createplan\",#sends final summary and shop name, expects \"article\", \"faq\", \"history\" in [] if good, or none\n",
        "    \"create\", #sends final summary and the data [] from createplan and shop data, sends     results[\"history\"] faq article, or none\n",
        "    \"generate_meta_tags_from_aggregate\", #send aggregate and shop name, expect return {  \"title\": title \"description\": description or none  }\n",
        "    \"find_available_fields\", #sends aggregate, expects detected = [field for field in FIELD_EXTRACTORS if field.lower() in response] return detected if detected else [] or None\n",
        "    \"extract_fields_from_aggregate\",#sends aggreagate and result from find avilable fields, expects   extracted[field] = raw_value return extracted if extracted else None or none\n",
        "    \"fillintheshop\",#if youve made it this far add content created to the shop variables\n",
        "]\n",
        "async def handle_task(task, crawler):\n",
        "    try:\n",
        "        if not task or not isinstance(task, dict):\n",
        "            print(\"❌ Skipping empty or malformed task.\")\n",
        "            return\n",
        "\n",
        "        task_id = task.get(\"task_id\")\n",
        "        task_type = task.get(\"task_type\")\n",
        "\n",
        "        if not task_id or not isinstance(task_id, int):\n",
        "            print(f\"❌ Invalid task_id: {task_id}\")\n",
        "            return\n",
        "        if not task_type:\n",
        "            print(\"❌ Missing task_type.\")\n",
        "            return\n",
        "\n",
        "        print(f\"▶️ Task: {task_type} ({task_id})\")\n",
        "\n",
        "        result,summary, mainstring, images, extractedfields, foundfields = False, None, None, None, None, None\n",
        "        match task_type:\n",
        "            case \"search\": result, mainstring, images = await search2(task, crawler)\n",
        "            case \"aggregate\": result, summary = summarize_large_text(task['target'].get('name', ''),task['target'].get('largetext', '') )\n",
        "            case \"createplan\": result,aggregateplan =  create_aggregate_plan(task['target'].get('summary', ''))\n",
        "            case \"create\": result,createdinfo =  generate_article_faq_history(task['target'].get('name', ''), task['target'].get('summary', ''),task['target'].get('aggregateplan', ''))\n",
        "\n",
        "            case \"find_available_fields\": result, foundfields = find_available_fields(task['target'].get('summary', ''))\n",
        "            case \"extract_fields_from_aggregate\": result, extractedfields = extract_fields_from_aggregate(task['target'].get('summary', ''), task['target'].get('foundfields', ''))\n",
        "        print(result, task_type, \"extraxted\",extractedfields,\"found\", foundfields)\n",
        "        if result:\n",
        "            print(f\"📤 Submitting result for {task_type} ({task_id})\")\n",
        "            try:\n",
        "                if task_type == 'search':\n",
        "                    res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"mainstring\": mainstring, \"images\": images, \"client_id\": CLIENT_ID})\n",
        "\n",
        "                if task_type == 'aggregate':\n",
        "                    if summary:\n",
        "                        res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"summary\": summary, \"client_id\": CLIENT_ID})\n",
        "                    else:\n",
        "                        print(\"nosummary\")\n",
        "                        res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"fail\", \"client_id\": CLIENT_ID})\n",
        "\n",
        "\n",
        "                if task_type == 'createplan':\n",
        "                    res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"aggregateplan\": aggregateplan, \"client_id\": CLIENT_ID})\n",
        "                if task_type == 'create':\n",
        "                    res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"createdinfo\":createdinfo, \"client_id\": CLIENT_ID})\n",
        "                if task_type == 'find_available_fields':\n",
        "                    res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"foundfields\": foundfields, \"client_id\": CLIENT_ID})\n",
        "                if task_type == 'extract_fields_from_aggregate':\n",
        "                    res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"success\", \"extractedfields\": extractedfields, \"client_id\": CLIENT_ID})\n",
        "\n",
        "                print(f\"Server responded: {res.status_code} - {res.text}\")\n",
        "                if res.status_code == 200:\n",
        "                    print(f\"✅ Submitted: {task_type}\")\n",
        "                else:\n",
        "                    print(f\"❌ Submit failed: {task_type} - {res.status_code}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Submit exception: {e}\")\n",
        "        else:\n",
        "\n",
        "            print(f\"Submit Failure {task_type}\")\n",
        "            res = requests.post(f\"{API_BASE}/submit/{task_id}\", json={\"status\": \"fail\", \"client_id\": CLIENT_ID})\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Task FAILED, Task: {task_type} ({task_id} {e}) \")\n",
        "        return None\n",
        "def get_task():\n",
        "    try:\n",
        "        print(\"📡 Polling for task...\")\n",
        "        r = requests.get(f\"{API_BASE}/next-task\", params={\"client_id\": CLIENT_ID}, timeout=30)\n",
        "        print(f\"API GET response: {r.status_code} - {r.text}\")\n",
        "\n",
        "        if r.status_code == 200:\n",
        "            try:\n",
        "                task = r.json()\n",
        "                if not isinstance(task, dict) or \"task_id\" not in task:\n",
        "                    print(f\"⚠️ Invalid task structure received: {task}\")\n",
        "                    return None\n",
        "                return task\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Failed to parse JSON: {e}\")\n",
        "                return None\n",
        "\n",
        "        elif r.status_code == 204:\n",
        "            print(\"⏳ No available task.\")\n",
        "            return None\n",
        "\n",
        "        print(f\"❌ Unexpected status code: {r.status_code}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Task fetch failed: {e}\")\n",
        "        return None\n",
        "\n",
        "def poll_heartbeat(current_task_id=None):\n",
        "    try:\n",
        "        data = {\"client_id\": CLIENT_ID}\n",
        "        if current_task_id:\n",
        "            data[\"task_id\"] = current_task_id\n",
        "        requests.post(f\"{API_BASE}/heartbeat\", json=data)\n",
        "        print(\"🫀 Heartbeat sent.\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Failed heartbeat: {e}\")\n",
        "\n",
        "async def main():\n",
        "    crawler_mgr = CrawlerManager()\n",
        "    await crawler_mgr.start()\n",
        "\n",
        "    last_heartbeat = 0\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            task = get_task()\n",
        "            if task:\n",
        "                now = time.time()\n",
        "                if now - last_heartbeat > HEARTBEAT_INTERVAL:\n",
        "                    poll_heartbeat(task.get(\"task_id\"))\n",
        "                    last_heartbeat = now\n",
        "                await handle_task(task, crawler_mgr)\n",
        "            else:\n",
        "                print(\"⏳ No task available, waiting...\")\n",
        "                await asyncio.sleep(10)\n",
        "    finally:\n",
        "        await crawler_mgr.stop()\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "\n",
        "nest_asyncio.apply()  # Allows re-entry of the running loop in Colab\n",
        "\n",
        "# Then just run the main function like this\n",
        "await main()"
      ],
      "metadata": {
        "id": "W_g_LpJugnI1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}