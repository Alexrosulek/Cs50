{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexrosulek/Cs50/blob/main/makearticle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzG-3nviVQTM",
        "outputId": "1c36f53b-5b09-4bf5-8e07-1adc36479638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 13269    0 13269    0     0  61602      0 --:--:-- --:--:-- --:--:-- 61716\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "####################################################################################           90.7%"
          ]
        }
      ],
      "source": [
        "!curl https://ollama.ai/install.sh | sh\n",
        "\n",
        "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "!sudo apt-get update && sudo apt-get install -y cuda-drivers\n",
        "\n",
        "import os\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &\n",
        "\n",
        "!ollama pull deepseek-r1:32b\n",
        "\n",
        "!ollama pull deepseek-r1:14b\n",
        "!ollama pull deepseek-r1:7b\n",
        "!ollama pull qwen2.5:3b\n",
        "!pip install ollama\n",
        "#!unzip -q /content/processed_shops.zip"
      ],
      "metadata": {
        "id": "O5toc_VkVffm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import asyncio\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import hashlib\n",
        "from urllib.parse import urlparse\n",
        "from pathlib import Path\n",
        "import ollama\n",
        "import httpx\n",
        "import requests\n",
        "import asyncio\n",
        "########################################################################\n",
        "# Configuration\n",
        "########################################################################\n",
        "MODEL_SEMAPHORE = asyncio.Semaphore(1)  # Ensures only one model runs at a time\n",
        "CHECKPOINT_FILE = \"progress.checkpoint2\"\n",
        "BASE_INPUT_DIR = \"processed_shops\"      # Root directory containing input shop data\n",
        "BASE_OUTPUT_DIR = \"generated_articles\"  # Root directory for generated articles\n",
        "STATES = [\"Florida\"]                    # Target states to process\n",
        "MIN_CONTENT_LENGTH = 1000                # Minimum combined content length to process\n",
        "CHUNK_SIZE = 8000\n",
        "                 # Maximum text chunk size for summarization\n",
        "SHOPDD = False\n",
        "########################################################################\n",
        "# AI Client Classes\n",
        "########################################################################\n",
        "\n",
        "\n",
        "class QwenClient:\n",
        "    async def fast_summarize(self, text):\n",
        "        \"\"\"Use Qwen (0.5b) for business/SEO-focused summarization.\"\"\"\n",
        "        async with MODEL_SEMAPHORE:\n",
        "            prompt = (\n",
        "                \"Your task is to extract key service details from this text\\n\"\n",
        "\n",
        "\n",
        "                \"STRICTLY IGNORE:\\n\"\n",
        "                \"- Business name/address (already known do not worry about)\\n\"\n",
        "\n",
        "                \"- Marketing fluff ('best in town')\\n\"\n",
        "                \"- Customer reviews\\n\"\n",
        "                \"- Repeated information\\n\\n\"\n",
        "                \"FORMAT REQUIREMENTS:\\n\"\n",
        "                \"- Bullet points only\\n\"\n",
        "                \"No extra spaceing\\n\"\n",
        "                \"- No complete sentences\\n\"\n",
        "                \"- Preserve exact numbers/special & unique info\\n\\n\"\n",
        "                \"Keep all concrete facts.\\n\"\n",
        "                \"No one is talking to you, you are extracting ALL info.\\n\"\n",
        "              \"REMOVE COOKIE AND PRIVACY POLICY\"\n",
        "                \"If error or empty or unsure don't respond. \"\n",
        "                f\"RAW INPUT TEXT:\\n{text}\"\n",
        "            )\n",
        "\n",
        "            try:\n",
        "\n",
        "                response = ollama.chat(\n",
        "                        model='qwen2.5:3b',\n",
        "                        messages=[{'role': 'user', 'content': prompt}]\n",
        "                    )\n",
        "\n",
        "                response = response['message']['content'].strip()\n",
        "\n",
        "\n",
        "                # Debug line for youâ€”feel free to remove if you want fewer logs:\n",
        "                print(f\"[AI] QwenClient.fast_summarize response: {response}\")\n",
        "\n",
        "                # Fallback if response is empty\n",
        "                if not response:\n",
        "                    print(\"Empty summary response, returning partial original text.\")\n",
        "                    return text[: CHUNK_SIZE // 4]  # integer slice\n",
        "\n",
        "                return response\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Qwen error: {e}\")\n",
        "                # Fallback return if something goes wrong\n",
        "                SHOPDD = True\n",
        "                return text[: CHUNK_SIZE // 4]  # integer slice\n",
        "\n",
        "\n",
        "async def chunked_summarize(qwen_client, text, chunk_size=CHUNK_SIZE):\n",
        "    \"\"\"\n",
        "    Splits 'text' into chunk_size blocks, summarizes each,\n",
        "    then combines those summaries into one final summary.\n",
        "    This produces a single, concise summary without repeated partials.\n",
        "    \"\"\"\n",
        "\n",
        "    # If it's already small enough, just return it.\n",
        "    if len(text) <= chunk_size:\n",
        "\n",
        "        summary = await qwen_client.fast_summarize(text)\n",
        "        print(f\"Text fits in one chunk (length={len(text)}). No further chunking needed.\")\n",
        "        return summary\n",
        "\n",
        "    split_size = chunk_size // 2\n",
        "    chunks = [text[i : i + split_size] for i in range(0, len(text), split_size)]\n",
        "    print(f\"Split into {len(chunks)} chunks ~{split_size} chars each.\")\n",
        "\n",
        "    # Step B: Summarize each chunk individually\n",
        "    summarized_chunks = []\n",
        "    for idx, chunk in enumerate(chunks, start=1):\n",
        "        print(\"starting\", idx)\n",
        "        summary = await qwen_client.fast_summarize(chunk)\n",
        "        if not summary:\n",
        "            print(\"nosum\")\n",
        "            # Fallback if the model returns empty\n",
        "            summary = chunk[: split_size // 4]\n",
        "        summarized_chunks.append(summary)\n",
        "\n",
        "    # Step C: Join all chunk summaries\n",
        "    combined_summary = \"\\n\".join(summarized_chunks)\n",
        "\n",
        "    summary = await qwen_client.fast_summarize(combined_summary)\n",
        "\n",
        "    return summary\n",
        "\n",
        "class DeepSeekClient:\n",
        "    async def seo_analysis(self, text):\n",
        "        \"\"\"Use DeepSeek 1.5b for SEO analysis.\"\"\"\n",
        "        async with MODEL_SEMAPHORE:\n",
        "            prompt = f\"Analyze this text for SEO for keywords, not for the webpage but for us to make an article about this webpage for findthatshop.com; Keep aligned with the shop info and ensure the writer know that he must write an article using this info about the shop specifically for findthatshop.com, and not to mention seo. most likely to be typed in google search. no emojis. You are a writer for findthatshop.com writing what these shops have. shortly suggest seo for us to rank for their keywords ect:\\n{text}\"\n",
        "\n",
        "            try:\n",
        "\n",
        "                response = ollama.chat(\n",
        "                        model='deepseek-r1:7b',\n",
        "                        messages=[{'role': 'user', 'content': prompt}]\n",
        "                    )\n",
        "\n",
        "                response = response['message']['content'].strip()\n",
        "\n",
        "                clean_response = self._clean_output(response)\n",
        "                print(f\"[AI] DeepSeekClient.seo_analysis response: {clean_response}\")\n",
        "                return clean_response\n",
        "            except Exception as e:\n",
        "                print(f\"DeepSeek SEO error: {e}\")\n",
        "                return \"\"\n",
        "\n",
        "    async def generate_article(self, prompt):\n",
        "        \"\"\"Use DeepSeek 7b for article generation.\"\"\"\n",
        "        async with MODEL_SEMAPHORE:\n",
        "            try:\n",
        "                response = ollama.chat(\n",
        "                        model='deepseek-r1:14b',\n",
        "                        messages=[{'role': 'user', 'content': prompt}]\n",
        "                    )\n",
        "\n",
        "                response = response['message']['content'].strip()\n",
        "\n",
        "                clean_response = self._clean_output(response)\n",
        "                print(f\"[AI] DeepSeekClient.generate_article response: {clean_response}\")\n",
        "                return clean_response\n",
        "            except Exception as e:\n",
        "                print(f\"DeepSeek Article error: {e}\")\n",
        "                return \"\"\n",
        "\n",
        "\n",
        "\n",
        "    async def validate_article(self, article):\n",
        "        \"\"\"Validation logic remains same\"\"\"\n",
        "        async with MODEL_SEMAPHORE:\n",
        "            prompt = (\n",
        "                \"You are an article checker. Please check the following article for these requirements. The article must be a 1-3 paragraphs, \"\n",
        " \"YOU ARE WORK FOR FINDTHATSHOP.COM CHECKING ARTICLES WRITTEN ABOUT WHAT SHOPS HAVE TO OFFER OFF GIVEN WEBSITE INFO WE GATHERED ON THEM FOR THE MAIN PAGE OF FINDTHATSHOP.COM!\\n\"\n",
        "                \"MAKE SURE IT IS FRIENDY CONSUMER LANGUAGE READABLE ARTICLE ABOUT THE SPECIFIC SHOP SERVICES\"\n",
        "                \"IF ITS ABOUT SOME HOLIDAY OR DEAL OR ISNT WHAT A READER WOULD EXPECT ON A REPORT ABOUT A SHOPS SERVICES SAY 'BAD'\"\n",
        "                \"Article must be a article about a shop not anything else or somthing random.\"\n",
        "                  \"ENSURE THAT IT IS A PROPER ARTICLE BASED ON A SHOP. \"\n",
        "                  \"DENY ARTICLES WITH BAD FORMATTING OR HTML. \"\n",
        "\n",
        "                \"INVALIDATE COOKIE AND PRIVACY POLICY ARTICLES\"\n",
        "                \"must be aligned with writing from findthatshop.com about this shops details. \"\n",
        "                \"NO markdown formatting, no *, extra commentary, or additional text beyond the core article content. NO EXTRA COMMENTS BEFORE OR AFTER THE ARTICLE, JUST THE ARTICLE. \"\n",
        "                \"article must not mention seo tactics or anything bad or anything caused by its ai generation which is noticable and bad for a reader, must be consumer readable article.\"\n",
        "                \"must be based on a shop\"\n",
        "                  \"FORBIDDEN PHRASES:\\n\"\n",
        "    \"- 'Here is the article'\\n\"\n",
        "    \"- 'As a business'\\n\"\n",
        "    \"- 'In conclusion'\\n\"\n",
        "    \"- 'We recommend'\\n\\n\"\n",
        "\n",
        "    \"- 'Deals\"\n",
        "    \"- 'deal saturday'\\n\\n\"\n",
        "\n",
        "                \"ONLY If the article meets these criteria, you MUST say 'VALID', IF GOOD SAY exactly 'VALID' ELSE SAY 'BAD'.  \"\n",
        "                \"\\n\\n\"\n",
        "                f\"Article:\\n{article}\\n\"\n",
        "            )\n",
        "            try:\n",
        "                response = ollama.chat(\n",
        "                    model='deepseek-r1:14b',\n",
        "                    messages=[{'role': 'user', 'content': prompt}]\n",
        "                )\n",
        "                clean_response = self._clean_output(response['message']['content'].strip())\n",
        "                return \"valid\" in clean_response.lower(), clean_response\n",
        "            except Exception as e:\n",
        "                print(f\"Validation error: {e}\")\n",
        "                return False, \"validation error\"\n",
        "\n",
        "    async def fixer(self, article, validation_response):\n",
        "        \"\"\"Targeted article repair based on validation feedback\"\"\"\n",
        "        async with MODEL_SEMAPHORE:\n",
        "            prompt = (\n",
        "                \"ARTICLE REPAIR PROTOCOL\\n\\n\"\n",
        "\n",
        " \"YOU WORK FOR FINDTHATSHOP.COM WRITING ABOUT WHAT SHOPS HAVE TO OFFER OFF GIVEN WEBSITE INFO WE GATHERED ON THEM FOR THE MAIN PAGE OF FINDTHATSHOP.COM!\\n\"\n",
        "                \"MAKE SURE IT IS FRIENDY CONSUMER LANGUAGE READABLE ARTICLE ABOUT THE SPECIFIC SHOP SERVICES\"\n",
        "                \"FIX THESE SPECIFIC ISSUES:\\n\"\n",
        "                f\"{validation_response}\\n\\n\"\n",
        "                \"REPAIR RULES:\\n\"\n",
        "                \"MAKE SURE IT IS CONSUMER LANGUAGE READABLE ARTICLE\"\n",
        "                \" MOST IMPORTANT! Remove ALL text before/after article content, RETURN NOTHING BUT THE ARTICLE\\n\"\n",
        "                \"ENSURE THAT IT IS A PROPER ARTICLE BASED ON A SHOP\"\n",
        "\n",
        "                \" Preserve all business details\\n\"\n",
        "                \" Use natural consumer language\\n\\n\"\n",
        "                  \"FORBIDDEN PHRASES:\\n\"\n",
        "    \"- 'Here is the article'\\n\"\n",
        "    \"- 'As a business'\\n\"\n",
        "    \"- 'In conclusion'\\n\"\n",
        "    \"- 'We recommend'\\n\\n\"\n",
        "\n",
        "    \"- 'deals'\\n\\n\"\n",
        "                \"BAD EXAMPLE TO AVOID:\\n\"\n",
        "                \"'Here's your article: ...'\\n\\n\"\n",
        "                \"GOOD EXAMPLE FORMAT:\\n\"\n",
        "                \"'ABC Shop at 123 Main St offers... Their services include...'\\n\\n\"\n",
        "                \"DEFECTIVE ARTICLE:\\n\"\n",
        "                f\"{article}\\n\\n\"\n",
        "                \"RETURN ONLY THE FIXED ARTICLE, NO EXTRA REMARKS OR COMMENTS, DO NOT SAY 'HERE IS YOUR ARTICLE', GIVE ONLY THE ARTICLE!\"\n",
        "            )\n",
        "            try:\n",
        "                response = ollama.chat(\n",
        "                    model='deepseek-r1:14b',\n",
        "                    messages=[{'role': 'user', 'content': prompt}]\n",
        "                )\n",
        "                return self._clean_output(response['message']['content'].strip())\n",
        "            except Exception as e:\n",
        "                print(f\"Fixer error: {e}\")\n",
        "                return article\n",
        "\n",
        "\n",
        "\n",
        "    def _clean_output(self, text):\n",
        "        \"\"\"Clean model output.\"\"\"\n",
        "        return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n",
        "\n",
        "########################################################################\n",
        "# Helper Functions\n",
        "########################################################################\n",
        "def clean_filename(name):\n",
        "    \"\"\"Clean a string to be a valid filename.\"\"\"\n",
        "    return re.sub(r'[\\\\/*?:\"<>|]', \"\", name).strip()\n",
        "\n",
        "async def save_checkpoint(index):\n",
        "    \"\"\"Save current shop index to a checkpoint file.\"\"\"\n",
        "    with open(CHECKPOINT_FILE, \"w\") as f:\n",
        "        f.write(str(index))\n",
        "\n",
        "async def load_checkpoint():\n",
        "    \"\"\"Load the current shop index from a checkpoint file.\"\"\"\n",
        "    try:\n",
        "        with open(CHECKPOINT_FILE, \"r\") as f:\n",
        "            return int(f.read().strip())\n",
        "    except FileNotFoundError:\n",
        "        return 0\n",
        "\n",
        "def validate_shop_data(shop_data):\n",
        "    \"\"\"Validate required fields in shop data and content length.\"\"\"\n",
        "    required_metadata = ['name', 'city', 'state', 'website']\n",
        "    meta = shop_data.get('metadata', {})\n",
        "\n",
        "    missing = [field for field in required_metadata if not meta.get(field)]\n",
        "    if missing:\n",
        "        return False, f\"Missing metadata fields: {missing}\"\n",
        "\n",
        "    content = shop_data.get('content', '')\n",
        "    visual_text = ' '.join([v.get('extracted_text', '')\n",
        "                          for v in shop_data.get('visual_analysis', [])])\n",
        "    combined = f\"{meta.get('name', '')} {meta.get('city', '')} {meta.get('state', '')} {content} {visual_text}\"\n",
        "\n",
        "    if len(combined) < MIN_CONTENT_LENGTH:\n",
        "        return False, f\"Content too short ({len(combined)} chars)\"\n",
        "\n",
        "    return True, \"Valid shop data\"\n",
        "\n",
        "########################################################################\n",
        "# Processing Functions\n",
        "########################################################################\n",
        "import math\n",
        "\n",
        "def split_text_evenly_dynamic(text, max_chunk_size):\n",
        "    \"\"\"\n",
        "    Splits the text into an equal number of chunks, where each chunk is\n",
        "    as close as possible to the same length and no chunk exceeds max_chunk_size.\n",
        "    \"\"\"\n",
        "    total_length = len(text)\n",
        "    # Determine the number of chunks needed such that each chunk is at most max_chunk_size.\n",
        "    n_chunks = math.ceil(total_length / max_chunk_size)\n",
        "    # Determine the equal chunk size.\n",
        "    chunk_size = math.ceil(total_length / n_chunks)\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(n_chunks):\n",
        "        start = i * chunk_size\n",
        "        end = start + chunk_size\n",
        "        chunks.append(text[start:end])\n",
        "    return chunks\n",
        "\n",
        "async def process_shop(shop_path, ai_client, index, total):\n",
        "    \"\"\"Process a single shop JSON file.\"\"\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        with open(shop_path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "            shop_data = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {shop_path}: {e}\")\n",
        "        return\n",
        "\n",
        "    is_valid, validation_msg = validate_shop_data(shop_data)\n",
        "    if not is_valid:\n",
        "        print(f\"Skipping {shop_path}: {validation_msg}\")\n",
        "        return\n",
        "\n",
        "    meta = shop_data['metadata']\n",
        "    slug  = meta['slug']\n",
        "    print(meta)\n",
        "    content = shop_data['content']\n",
        "    visual_text = ' '.join([v.get('extracted_text', '')\n",
        "                          for v in shop_data.get('visual_analysis', [])])\n",
        "    output_path = Path(BASE_OUTPUT_DIR) / meta['state'].lower() / meta['city'].lower()\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "    filename = output_path / f\"{clean_filename(meta['name'])}.txt\"\n",
        "    if filename.exists():\n",
        "        print(f\"Skipping {shop_path}: Generated article already exists at {filename}\")\n",
        "        return\n",
        "\n",
        "    combined_text = (\n",
        "        f\"Business Name: {meta['name']}\\n\"\n",
        "        f\"Location: {meta['address']}\\n\"\n",
        "        f\"Services: {content}\\n\"\n",
        "        f\"Additional Details: {visual_text}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        summary = await chunked_summarize(ai_client.qwen, combined_text)\n",
        "        print(f\"[AI] Final summary: {summary}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Summarization failed: {e}\")\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    article_prompt = (\n",
        "    \"ROLE: Professional writer for findthatshop.com  writing about shop services\\n\"\n",
        "    \"TASK: Create article about this shop using ONLY the provided data\\n\\n\"\n",
        "    \"YOU DO NOT GIVE BACK HTML OR SHIT WE WANT AN ARTICLE ON SERVICES BASED ON INFO, NOT SHIT\"\n",
        "    \"KEEP SEO IN MIND FOR MOST LIKELY SEARCHED TERMS FOR ARTICLE FOR FINDTHATSHOP.COM\\n\"\n",
        "    \"INCLUDE TERMS LIKE 'NEAR ME' ECT FOR FINDTHATSHOP.COM AND YOU WRITE IN A STYLE LIKE YOU ARE REPORTING ON THEIR SERVICES PERSPECTIVE AS A FINDTHATSHOP.COM WRITER, YOU WORK FOR FINDTHATSHOP.COM WRITING ABOUT WHAT SHOPS HAVE TO OFFER OFF GIVEN WEBSITE INFO WE GATHERED ON THEM FOR THE MAIN PAGE OF FINDTHATSHOP.COM!\\n\"\n",
        "    \"STRICT RULES:\\n\"\n",
        "\n",
        "                \"MAKE SURE IT IS FRIENDY CONSUMER LANGUAGE READABLE ARTICLE ABOUT THE SPECIFIC SHOP SERVICES\"\n",
        "    \"1. BEGIN IMMEDIATELY WITH ARTICLE CONTENT. \"\n",
        "\n",
        "     \"You must only provide the article wanted and no other extra text, no *. NO EXTRA COMMENTS BEFORE OR AFTER THE ARTICLE, JUST RESPOND WITH ONLY THE ARTICLE.\"\n",
        "    \"2. NO text before/after the article\\n\"\n",
        "    \"3. NO markdown, asterisks, or special formatting\\n\"\n",
        "    \"4. INCLUDE NO URLS\\n\\n\"\n",
        "    \"You write about the shop not the webpage.\"\n",
        "    \"REPORT ON THE SHOP GENERALLY as a findthatshop.com writer\"\n",
        "    \"CONTENT STRUCTURE:\\n\"\n",
        "      \"ENSURE THAT IT IS A PROPER ARTICLE BASED ON A SHOP\"\n",
        "    \"2-3 PARAGRAPHS ON SERVICES IF NOT SERVICES LOCATION ECT, NOT SPECIFIC DEALS OR HOLIDAY STUFF\"\n",
        "  \"Do not say 'explore our' YOU WORK FOR FINDTHATSHOP.COM, they arent our things its theirs and we are reporting\"\n",
        "    \"FORBIDDEN PHRASES:\\n\"\n",
        "    \"- 'Here is the article'\\n\"\n",
        "    \"- 'As a business'\\n\"\n",
        "    \"- 'In conclusion'\\n\"\n",
        "    \"- 'We recommend'\\n\\n\"\n",
        "       \"- 'Deals\"\n",
        "\n",
        "    \"- 'explore our\"\n",
        "    \"- 'deal saturday'\\n\\n\"\n",
        "\n",
        "\n",
        "    \"BAD EXAMPLE (REJECT THIS):\\n\"\n",
        "    \"'Here's your article about Example Shop: They provide...'\\n\\n\"\n",
        "    \"'Here's your article about...' [WRONG START, ONLY PROVIDE THE ARTICLE]\\n\"\n",
        "\n",
        "\n",
        "    \"GOOD EXAMPLE (COPY THIS FORMAT):\\n\"\n",
        "    \"'ABC Auto Repair in downtown Miami offers... Their team specializes in...'\\n\"\n",
        "    \"'123 Main Street offers... Their services include...'\\n\\n\"\n",
        "\n",
        "    \"BUSINESS DATA:\\n\"\n",
        "    f\"Name: {meta['name']}\\n\"\n",
        "    f\"Address: {meta['address']}\\n\"\n",
        "    f\"Services: {summary}\\n\"\n",
        "\n",
        "\n",
        "    \"- Natural language with location keywords\\n\"\n",
        "    \"- Address reader as 'you'\\n\\n\"\n",
        "    \"- No markdown, bullets, or special characters\\n\"\n",
        "    \"IMPERATIVE: RESPOND ONLY WITH THE RAW ARTICLE TEXT - NO OTHER CONTENT OR REMARKS!\"\n",
        ")\n",
        "    ()\n",
        "    try:\n",
        "        article = await ai_client.deepseek.generate_article(article_prompt)\n",
        "        print(f\"[AI] Generated article: {article}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Article generation failed: {e}\")\n",
        "        return\n",
        "\n",
        "    is_valid, validation_msg = await ai_client.deepseek.validate_article(article)\n",
        "    if not is_valid:\n",
        "        print(\"Article failed validation. Attempting fix...\")\n",
        "\n",
        "        article = await ai_client.deepseek.fixer(article, validation_msg)\n",
        "\n",
        "\n",
        "        if not await ai_client.deepseek.validate_article(article):\n",
        "            print(\"final validation failed\")\n",
        "            return\n",
        "\n",
        "    if article:\n",
        "        print(\"ARTICLE VALID\")\n",
        "        print(article)\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(article)\n",
        "        if slug:\n",
        "            print(\"HERE WITH SLUG\")\n",
        "            payload = {\"slug\": slug, \"description\": article}\n",
        "            endpoint = \"https://www.findthatshop.com/e/s/s/update/\"\n",
        "\n",
        "            async with httpx.AsyncClient() as client:\n",
        "\n",
        "                response = await client.post(endpoint, json=payload, timeout = 50)\n",
        "                print(response)\n",
        "\n",
        "\n",
        "########################################################################\n",
        "# Main Execution\n",
        "########################################################################\n",
        "class AIClient:\n",
        "    def __init__(self):\n",
        "        self.deepseek = DeepSeekClient()\n",
        "        self.qwen = QwenClient()\n",
        "\n",
        "async def main():\n",
        "    start_index = await load_checkpoint()\n",
        "    shop_files = []\n",
        "    for state in os.listdir(BASE_INPUT_DIR):\n",
        "        if state.lower() not in [s.lower() for s in STATES]:\n",
        "            continue\n",
        "        state_path = Path(BASE_INPUT_DIR) / state\n",
        "        for city in os.listdir(state_path):\n",
        "            city_path = state_path / city\n",
        "            if city_path.is_dir():\n",
        "                shop_files.extend([\n",
        "                    city_path / shop_file\n",
        "                    for shop_file in os.listdir(city_path)\n",
        "                    if shop_file.endswith('.json')\n",
        "                ])\n",
        "\n",
        "    total_shops = len(shop_files)\n",
        "    print(f\"Found {total_shops} shops to process\")\n",
        "\n",
        "    ai_client = AIClient()\n",
        "    print(\"starting\",start_index)\n",
        "\n",
        "    for i, shop_path in enumerate(shop_files[start_index:]):\n",
        "        current_index =  start_index + i\n",
        "        await process_shop(shop_path, ai_client, current_index, total_shops)\n",
        "        if SHOPDD:\n",
        "          break\n",
        "        await save_checkpoint(current_index + 1)\n",
        "\n",
        "    print(\"Processing completed successfully\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    await main()"
      ],
      "metadata": {
        "id": "k2CXyHXotpn5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}