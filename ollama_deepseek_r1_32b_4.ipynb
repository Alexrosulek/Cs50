{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexrosulek/Cs50/blob/main/ollama_deepseek_r1_32b_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzG-3nviVQTM",
        "outputId": "0c954c87-be40-417d-8c88-78f2e600b72b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13269    0 13269    0     0  61155      0 --:--:-- --:--:-- --:--:-- 61430\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Adding ollama user to render group...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 384 kB in 2s (160 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cuda-drivers is already the newest version (570.86.15-0ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!curl https://ollama.ai/install.sh | sh\n",
        "\n",
        "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "!sudo apt-get update && sudo apt-get install -y cuda-drivers\n",
        "\n",
        "import os\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &\n",
        "!ollama pull deepseek-r1:14b\n",
        "\n",
        "!ollama pull deepseek-r1:7b\n",
        "!ollama pull qwen2.5:3b\n",
        "!pip install ollama\n",
        "!unzip -q /content/processed_shops.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5toc_VkVffm",
        "outputId": "20d08470-8b18-44fd-ed36-5ef324a0c3a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 6e9f90f02bb3... 100% ▕▏ 9.0 GB                         \n",
            "pulling 369ca498f347... 100% ▕▏  387 B                         \n",
            "pulling 6e4c38e1172f... 100% ▕▏ 1.1 KB                         \n",
            "pulling f4d24e9138dd... 100% ▕▏  148 B                         \n",
            "pulling 3c24b0c80794... 100% ▕▏  488 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\n",
            "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 96c415656d37... 100% ▕▏ 4.7 GB                         \n",
            "pulling 369ca498f347... 100% ▕▏  387 B                         \n",
            "pulling 6e4c38e1172f... 100% ▕▏ 1.1 KB                         \n",
            "pulling f4d24e9138dd... 100% ▕▏  148 B                         \n",
            "pulling 40fb844194b2... 100% ▕▏  487 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\n",
            "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling 5ee4f07cdb9b... 100% ▕▏ 1.9 GB                         \n",
            "pulling 66b9ea09bd5b... 100% ▕▏   68 B                         \n",
            "pulling eb4402837c78... 100% ▕▏ 1.5 KB                         \n",
            "pulling b5c0e5cf74cf... 100% ▕▏ 7.4 KB                         \n",
            "pulling 161ddde4c9cd... 100% ▕▏  487 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\n",
            "Requirement already satisfied: ollama in /usr/local/lib/python3.11/dist-packages (0.4.7)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from ollama) (2.10.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import asyncio\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import hashlib\n",
        "from urllib.parse import urlparse\n",
        "from pathlib import Path\n",
        "import ollama\n",
        "import httpx\n",
        "import requests\n",
        "import asyncio\n",
        "########################################################################\n",
        "# Configuration\n",
        "########################################################################\n",
        "MODEL_SEMAPHORE = asyncio.Semaphore(1)  # Ensures only one model runs at a time\n",
        "CHECKPOINT_FILE = \"progress.checkpoint2\"\n",
        "BASE_INPUT_DIR = \"processed_shops\"      # Root directory containing input shop data\n",
        "BASE_OUTPUT_DIR = \"generated_articles\"  # Root directory for generated articles\n",
        "STATES = [\"Florida\"]                    # Target states to process\n",
        "MIN_CONTENT_LENGTH = 600                # Minimum combined content length to process\n",
        "CHUNK_SIZE = 10000                       # Maximum text chunk size for summarization\n",
        "\n",
        "########################################################################\n",
        "# AI Client Classes\n",
        "########################################################################\n",
        "CHUNK_SIZE = 10000  # unchanged\n",
        "\n",
        "class QwenClient:\n",
        "    async def fast_summarize(self, text):\n",
        "        \"\"\"Use Qwen (0.5b) for business/SEO-focused summarization.\"\"\"\n",
        "        async with MODEL_SEMAPHORE:\n",
        "            prompt = (\n",
        "                \"Your task is to extract key service details from this text\\n\"\n",
        "\n",
        "\n",
        "                \"STRICTLY IGNORE:\\n\"\n",
        "                \"- Business name/address (already known do not worry about)\\n\"\n",
        "\n",
        "                \"- Marketing fluff ('best in town')\\n\"\n",
        "                \"- Customer reviews\\n\"\n",
        "                \"- Repeated information\\n\\n\"\n",
        "                \"FORMAT REQUIREMENTS:\\n\"\n",
        "                \"- Bullet points only\\n\"\n",
        "                \"No extra spaceing\\n\"\n",
        "                \"- No complete sentences\\n\"\n",
        "                \"- Preserve exact numbers/special & unique info\\n\\n\"\n",
        "                \"Keep all concrete facts.\\n\"\n",
        "                \"No one is talking to you, you are extracting ALL info.\\n\"\n",
        "\n",
        "                \"If error or empty or unsure don't respond. \"\n",
        "                f\"RAW INPUT TEXT:\\n{text}\"\n",
        "            )\n",
        "\n",
        "            try:\n",
        "\n",
        "                response = ollama.chat(\n",
        "                        model='qwen2.5:3b',\n",
        "                        messages=[{'role': 'user', 'content': prompt}]\n",
        "                    )\n",
        "\n",
        "                response = response['message']['content'].strip()\n",
        "\n",
        "\n",
        "                # Debug line for you—feel free to remove if you want fewer logs:\n",
        "                print(f\"[AI] QwenClient.fast_summarize response: {response}\")\n",
        "\n",
        "                # Fallback if response is empty\n",
        "                if not response:\n",
        "                    print(\"Empty summary response, returning partial original text.\")\n",
        "                    return text[: CHUNK_SIZE // 4]  # integer slice\n",
        "\n",
        "                return response\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Qwen error: {e}\")\n",
        "                # Fallback return if something goes wrong\n",
        "                return text[: CHUNK_SIZE // 4]  # integer slice\n",
        "\n",
        "\n",
        "async def chunked_summarize(qwen_client, text, chunk_size=CHUNK_SIZE):\n",
        "    \"\"\"\n",
        "    Splits 'text' into chunk_size blocks, summarizes each,\n",
        "    then combines those summaries into one final summary.\n",
        "    This produces a single, concise summary without repeated partials.\n",
        "    \"\"\"\n",
        "\n",
        "    # If it's already small enough, just return it.\n",
        "    if len(text) <= chunk_size:\n",
        "        print(f\"Text fits in one chunk (length={len(text)}). No further chunking needed.\")\n",
        "        return text\n",
        "\n",
        "    split_size = chunk_size // 2\n",
        "    chunks = [text[i : i + split_size] for i in range(0, len(text), split_size)]\n",
        "    print(f\"Split into {len(chunks)} chunks ~{split_size} chars each.\")\n",
        "\n",
        "    # Step B: Summarize each chunk individually\n",
        "    summarized_chunks = []\n",
        "    for idx, chunk in enumerate(chunks, start=1):\n",
        "        print(\"starting\", idx)\n",
        "        summary = await qwen_client.fast_summarize(chunk)\n",
        "        if not summary:\n",
        "            print(\"nosum\")\n",
        "            # Fallback if the model returns empty\n",
        "            summary = chunk[: split_size // 4]\n",
        "        summarized_chunks.append(summary)\n",
        "\n",
        "    # Step C: Join all chunk summaries\n",
        "    combined_summary = \"\\n\".join(summarized_chunks)\n",
        "\n",
        "\n",
        "    return combined_summary\n",
        "\n",
        "class DeepSeekClient:\n",
        "    async def seo_analysis(self, text):\n",
        "        \"\"\"Use DeepSeek 1.5b for SEO analysis.\"\"\"\n",
        "        async with MODEL_SEMAPHORE:\n",
        "            prompt = f\"Analyze this text for SEO for keywords, not for the webpage but for us to make an article about this webpage for findthatshop.com; Keep aligned with the shop info and ensure the writer know that he must write an article using this info about the shop specifically for findthatshop.com, and not to mention seo. most likely to be typed in google search. no emojis. You are a writer for findthatshop.com writing what these shops have. shortly suggest seo for us to rank for their keywords ect:\\n{text}\"\n",
        "\n",
        "            try:\n",
        "\n",
        "                response = ollama.chat(\n",
        "                        model='deepseek-r1:7b',\n",
        "                        messages=[{'role': 'user', 'content': prompt}]\n",
        "                    )\n",
        "\n",
        "                response = response['message']['content'].strip()\n",
        "\n",
        "                clean_response = self._clean_output(response)\n",
        "                print(f\"[AI] DeepSeekClient.seo_analysis response: {clean_response}\")\n",
        "                return clean_response\n",
        "            except Exception as e:\n",
        "                print(f\"DeepSeek SEO error: {e}\")\n",
        "                return \"\"\n",
        "\n",
        "    async def generate_article(self, prompt):\n",
        "        \"\"\"Use DeepSeek 7b for article generation.\"\"\"\n",
        "        async with MODEL_SEMAPHORE:\n",
        "            try:\n",
        "                response = ollama.chat(\n",
        "                        model='deepseek-r1:14b',\n",
        "                        messages=[{'role': 'user', 'content': prompt}]\n",
        "                    )\n",
        "\n",
        "                response = response['message']['content'].strip()\n",
        "\n",
        "                clean_response = self._clean_output(response)\n",
        "                print(f\"[AI] DeepSeekClient.generate_article response: {clean_response}\")\n",
        "                return clean_response\n",
        "            except Exception as e:\n",
        "                print(f\"DeepSeek Article error: {e}\")\n",
        "                return \"\"\n",
        "\n",
        "\n",
        "\n",
        "    async def validate_article(self, article):\n",
        "        \"\"\"Validation logic remains same\"\"\"\n",
        "        async with MODEL_SEMAPHORE:\n",
        "            prompt = (\n",
        "                \"You are an article checker. Please check the following article for these requirements. The article must be a 1-3 paragraphs, \"\n",
        "\n",
        "                \"MAKE SURE IT IS FRIENDY CONSUMER LANGUAGE READABLE ARTICLE ABOUT THE SPECIFIC SHOP SERVICES\"\n",
        "                \"IF ITS ABOUT SOME HOLIDAY OR DEAL OR ISNT WHAT A READER WOULD EXPECT ON A REPORT ABOUT A SHOPS SERVICES SAY 'BAD'\"\n",
        "                \"Article must be a article about a shop not anything else or somthing random.\"\n",
        "                  \"ENSURE THAT IT IS A PROPER ARTICLE BASED ON A SHOP\"\n",
        "                \"must be aligned with writing from findthatshop.com about this shops details\"\n",
        "                \"without any markdown formatting, no *, extra commentary, or additional text beyond the core article content. NO EXTRA COMMENTS BEFORE OR AFTER THE ARTICLE, JUST THE ARTICLE. \"\n",
        "                \"article must not mention seo tactics or anything, must be consumer readable article.\"\n",
        "                \"must be based on a shop\"\n",
        "                \"ONLY If the article meets these criteria, you MUST say 'VALID', IF GOOD SAY exactly 'VALID' ELSE SAY 'BAD'.  \"\n",
        "                \"\\n\\n\"\n",
        "                f\"Article:\\n{article}\\n\"\n",
        "            )\n",
        "            try:\n",
        "                response = ollama.chat(\n",
        "                    model='deepseek-r1:14b',\n",
        "                    messages=[{'role': 'user', 'content': prompt}]\n",
        "                )\n",
        "                clean_response = self._clean_output(response['message']['content'].strip())\n",
        "                return \"valid\" in clean_response.lower(), clean_response\n",
        "            except Exception as e:\n",
        "                print(f\"Validation error: {e}\")\n",
        "                return False, \"validation error\"\n",
        "\n",
        "    async def fixer(self, article, validation_response):\n",
        "        \"\"\"Targeted article repair based on validation feedback\"\"\"\n",
        "        async with MODEL_SEMAPHORE:\n",
        "            prompt = (\n",
        "                \"ARTICLE REPAIR PROTOCOL\\n\\n\"\n",
        "\n",
        "                \"MAKE SURE IT IS FRIENDY CONSUMER LANGUAGE READABLE ARTICLE ABOUT THE SPECIFIC SHOP SERVICES\"\n",
        "                \"FIX THESE SPECIFIC ISSUES:\\n\"\n",
        "                f\"{validation_response}\\n\\n\"\n",
        "                \"REPAIR RULES:\\n\"\n",
        "                \"MAKE SURE IT IS CONSUMER LANGUAGE READABLE ARTICLE\"\n",
        "                \"1. MOST IMPORTANT! Remove ALL text before/after article content, RETURN NOTHING BUT THE ARTICLE\\n\"\n",
        "                \"ENSURE THAT IT IS A PROPER ARTICLE BASED ON A SHOP\"\n",
        "                \"2. Ensure 3 paragraph structure:\\n\"\n",
        "                \"   - Services & Location\\n\"\n",
        "                \"   - Pricing & Operations\\n\"\n",
        "                \"   - Unique Features\\n\"\n",
        "                \"3. Preserve all business details\\n\"\n",
        "                \"4. Use natural consumer language\\n\\n\"\n",
        "                \"BAD EXAMPLE TO AVOID:\\n\"\n",
        "                \"'Here's your article: ...'\\n\\n\"\n",
        "                \"GOOD EXAMPLE FORMAT:\\n\"\n",
        "                \"'ABC Shop at 123 Main St offers... Their services include...'\\n\\n\"\n",
        "                \"DEFECTIVE ARTICLE:\\n\"\n",
        "                f\"{article}\\n\\n\"\n",
        "                \"RETURN ONLY THE FIXED ARTICLE, NO EXTRA REMARKS OR COMMENTS, DO NOT SAY 'HERE IS YOUR ARTICLE', GIVE ONLY THE ARTICLE!\"\n",
        "            )\n",
        "            try:\n",
        "                response = ollama.chat(\n",
        "                    model='deepseek-r1:14b',\n",
        "                    messages=[{'role': 'user', 'content': prompt}]\n",
        "                )\n",
        "                return self._clean_output(response['message']['content'].strip())\n",
        "            except Exception as e:\n",
        "                print(f\"Fixer error: {e}\")\n",
        "                return article\n",
        "\n",
        "\n",
        "\n",
        "    def _clean_output(self, text):\n",
        "        \"\"\"Clean model output.\"\"\"\n",
        "        return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()\n",
        "\n",
        "########################################################################\n",
        "# Helper Functions\n",
        "########################################################################\n",
        "def clean_filename(name):\n",
        "    \"\"\"Clean a string to be a valid filename.\"\"\"\n",
        "    return re.sub(r'[\\\\/*?:\"<>|]', \"\", name).strip()\n",
        "\n",
        "async def save_checkpoint(index):\n",
        "    \"\"\"Save current shop index to a checkpoint file.\"\"\"\n",
        "    with open(CHECKPOINT_FILE, \"w\") as f:\n",
        "        f.write(str(index))\n",
        "\n",
        "async def load_checkpoint():\n",
        "    \"\"\"Load the current shop index from a checkpoint file.\"\"\"\n",
        "    try:\n",
        "        with open(CHECKPOINT_FILE, \"r\") as f:\n",
        "            return int(f.read().strip())\n",
        "    except FileNotFoundError:\n",
        "        return 0\n",
        "\n",
        "def validate_shop_data(shop_data):\n",
        "    \"\"\"Validate required fields in shop data and content length.\"\"\"\n",
        "    required_metadata = ['name', 'city', 'state', 'website']\n",
        "    meta = shop_data.get('metadata', {})\n",
        "\n",
        "    missing = [field for field in required_metadata if not meta.get(field)]\n",
        "    if missing:\n",
        "        return False, f\"Missing metadata fields: {missing}\"\n",
        "\n",
        "    content = shop_data.get('content', '')\n",
        "    visual_text = ' '.join([v.get('extracted_text', '')\n",
        "                          for v in shop_data.get('visual_analysis', [])])\n",
        "    combined = f\"{meta.get('name', '')} {meta.get('city', '')} {meta.get('state', '')} {content} {visual_text}\"\n",
        "\n",
        "    if len(combined) < MIN_CONTENT_LENGTH:\n",
        "        return False, f\"Content too short ({len(combined)} chars)\"\n",
        "\n",
        "    return True, \"Valid shop data\"\n",
        "\n",
        "########################################################################\n",
        "# Processing Functions\n",
        "########################################################################\n",
        "import math\n",
        "\n",
        "def split_text_evenly_dynamic(text, max_chunk_size):\n",
        "    \"\"\"\n",
        "    Splits the text into an equal number of chunks, where each chunk is\n",
        "    as close as possible to the same length and no chunk exceeds max_chunk_size.\n",
        "    \"\"\"\n",
        "    total_length = len(text)\n",
        "    # Determine the number of chunks needed such that each chunk is at most max_chunk_size.\n",
        "    n_chunks = math.ceil(total_length / max_chunk_size)\n",
        "    # Determine the equal chunk size.\n",
        "    chunk_size = math.ceil(total_length / n_chunks)\n",
        "\n",
        "    chunks = []\n",
        "    for i in range(n_chunks):\n",
        "        start = i * chunk_size\n",
        "        end = start + chunk_size\n",
        "        chunks.append(text[start:end])\n",
        "    return chunks\n",
        "\n",
        "async def process_shop(shop_path, ai_client, index, total):\n",
        "    \"\"\"Process a single shop JSON file.\"\"\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        with open(shop_path, 'r', encoding='utf-8', errors='replace') as f:\n",
        "            shop_data = json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {shop_path}: {e}\")\n",
        "        return\n",
        "\n",
        "    is_valid, validation_msg = validate_shop_data(shop_data)\n",
        "    if not is_valid:\n",
        "        print(f\"Skipping {shop_path}: {validation_msg}\")\n",
        "        return\n",
        "\n",
        "    meta = shop_data['metadata']\n",
        "    slug  = meta['slug']\n",
        "    print(meta)\n",
        "    content = shop_data['content']\n",
        "    visual_text = ' '.join([v.get('extracted_text', '')\n",
        "                          for v in shop_data.get('visual_analysis', [])])\n",
        "    output_path = Path(BASE_OUTPUT_DIR) / meta['state'].lower() / meta['city'].lower()\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "    filename = output_path / f\"{clean_filename(meta['name'])}.txt\"\n",
        "    if filename.exists():\n",
        "        print(f\"Skipping {shop_path}: Generated article already exists at {filename}\")\n",
        "        return\n",
        "\n",
        "    combined_text = (\n",
        "        f\"Business Name: {meta['name']}\\n\"\n",
        "        f\"Location: {meta['address']}\\n\"\n",
        "        f\"Services: {content}\\n\"\n",
        "        f\"Additional Details: {visual_text}\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        summary = await chunked_summarize(ai_client.qwen, combined_text)\n",
        "        print(f\"[AI] Final summary: {summary}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Summarization failed: {e}\")\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    article_prompt = (\n",
        "    \"ROLE: Professional writer for findthatshop.com  writing about shop services\\n\"\n",
        "    \"TASK: Create article about this shop using ONLY the provided data\\n\\n\"\n",
        "    \"KEEP SEO IN MIND FOR MOST LIKELY SEARCHED TERMS FOR ARTICLE FOR FINDTHATSHOP.COM\\n\"\n",
        "    \"STRICT RULES:\\n\"\n",
        "\n",
        "                \"MAKE SURE IT IS FRIENDY CONSUMER LANGUAGE READABLE ARTICLE ABOUT THE SPECIFIC SHOP SERVICES\"\n",
        "    \"1. BEGIN IMMEDIATELY WITH ARTICLE CONTENT. \"\n",
        "\n",
        "     \"You must only provide the article wanted and no other extra text, no *. NO EXTRA COMMENTS BEFORE OR AFTER THE ARTICLE, JUST RESPOND WITH ONLY THE ARTICLE.\"\n",
        "    \"2. NO text before/after the article\\n\"\n",
        "    \"3. NO markdown, asterisks, or special formatting\\n\"\n",
        "    \"4. ONLY include base URLs and make sure they seem legit\\n\\n\"\n",
        "    \"You write about the shop not the webpage.\"\n",
        "    \"REPORT ON THE SHOP GENERALLY as a findthatshop.com writer\"\n",
        "    \"CONTENT STRUCTURE:\\n\"\n",
        "      \"ENSURE THAT IT IS A PROPER ARTICLE BASED ON A SHOP\"\n",
        "    \"1-3 PARAGRAPHS ON SERVICES IF NOT SERVICES LOCATION ECT, NOT SPECIFIC DEALS OR HOLIDAY STUFF\"\n",
        "\n",
        "    \"FORBIDDEN PHRASES:\\n\"\n",
        "    \"- 'Here is the article'\\n\"\n",
        "    \"- 'As a business'\\n\"\n",
        "    \"- 'In conclusion'\\n\"\n",
        "    \"- 'We recommend'\\n\\n\"\n",
        "\n",
        "    \"BAD EXAMPLE (REJECT THIS):\\n\"\n",
        "    \"'Here's your article about Example Shop: They provide...'\\n\\n\"\n",
        "    \"'Here's your article about...' [WRONG START, ONLY PROVIDE THE ARTICLE]\\n\"\n",
        "\n",
        "\n",
        "    \"GOOD EXAMPLE (COPY THIS FORMAT):\\n\"\n",
        "    \"'ABC Auto Repair in downtown Miami offers... Their team specializes in...'\\n\"\n",
        "    \"'123 Main Street offers... Their services include...'\\n\\n\"\n",
        "\n",
        "    \"BUSINESS DATA:\\n\"\n",
        "    f\"Name: {meta['name']}\\n\"\n",
        "    f\"Address: {meta['address']}\\n\"\n",
        "    f\"Services: {summary}\\n\"\n",
        "\n",
        "\n",
        "    \"- Natural language with location keywords\\n\"\n",
        "    \"- Address reader as 'you'\\n\\n\"\n",
        "    \"- No markdown, bullets, or special characters\\n\"\n",
        "    \"IMPERATIVE: RESPOND ONLY WITH THE RAW ARTICLE TEXT - NO OTHER CONTENT!\"\n",
        ")\n",
        "    try:\n",
        "        article = await ai_client.deepseek.generate_article(article_prompt)\n",
        "        print(f\"[AI] Generated article: {article}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Article generation failed: {e}\")\n",
        "        return\n",
        "\n",
        "    is_valid, validation_msg = await ai_client.deepseek.validate_article(article)\n",
        "    if not is_valid:\n",
        "        print(\"Article failed validation. Attempting fix...\")\n",
        "\n",
        "        article = await ai_client.deepseek.fixer(article, validation_msg)\n",
        "\n",
        "\n",
        "        if not await ai_client.deepseek.validate_article(article):\n",
        "            print(\"final validation failed\")\n",
        "            return\n",
        "\n",
        "    if article:\n",
        "        print(\"ARTICLE VALID\")\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(article)\n",
        "        if slug:\n",
        "            print(\"HERE WITH SLUG\")\n",
        "            payload = {\"slug\": slug, \"description\": article}\n",
        "            endpoint = \"https://www.findthatshop.com/e/s/s/update/\"\n",
        "\n",
        "            async with httpx.AsyncClient() as client:\n",
        "\n",
        "                response = await client.post(endpoint, json=payload, timeout = 50)\n",
        "                print(response)\n",
        "    # Delete processed file after successful generation and upload\n",
        "    try:\n",
        "        shop_path.unlink()  # Pathlib's delete method\n",
        "        print(f\"Deleted processed file: {shop_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting {shop_path}: {e}\")\n",
        "\n",
        "########################################################################\n",
        "# Main Execution\n",
        "########################################################################\n",
        "class AIClient:\n",
        "    def __init__(self):\n",
        "        self.deepseek = DeepSeekClient()\n",
        "        self.qwen = QwenClient()\n",
        "\n",
        "async def main():\n",
        "\n",
        "    shop_files = []\n",
        "    for state in os.listdir(BASE_INPUT_DIR):\n",
        "        if state.lower() not in [s.lower() for s in STATES]:\n",
        "            continue\n",
        "        state_path = Path(BASE_INPUT_DIR) / state\n",
        "        for city in os.listdir(state_path):\n",
        "            city_path = state_path / city\n",
        "            if city_path.is_dir():\n",
        "                shop_files.extend([\n",
        "                    city_path / shop_file\n",
        "                    for shop_file in os.listdir(city_path)\n",
        "                    if shop_file.endswith('.json')\n",
        "                ])\n",
        "\n",
        "    total_shops = len(shop_files)\n",
        "    print(f\"Found {total_shops} shops to process\")\n",
        "\n",
        "    ai_client = AIClient()\n",
        "\n",
        "    for i, shop_path in enumerate(shop_files[:]):\n",
        "        current_index =  i\n",
        "        await process_shop(shop_path, ai_client, current_index, total_shops)\n",
        "        await save_checkpoint(current_index + 1)\n",
        "\n",
        "    print(\"Processing completed successfully\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    await main()"
      ],
      "metadata": {
        "id": "k2CXyHXotpn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62dbbec8-cc76-4117-f224-db65eec1c41a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 shops to process\n",
            "Processing completed successfully\n"
          ]
        }
      ]
    }
  ]
}