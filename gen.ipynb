{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexrosulek/Cs50/blob/main/gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://ollama.ai/install.sh | sh\n",
        "\n",
        "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "!sudo apt-get update && sudo apt-get install -y cuda-drivers\n",
        "\n",
        "import os\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "!nohup ollama serve &\n",
        "!pip install pyOpenSSL==24.2.1\n",
        "\n",
        "\n",
        "# Pull Ollama Models\n",
        "\n",
        "!ollama pull qwen3:0.6b\n",
        "\n",
        "!ollama pull gemma3:1b\n",
        "!ollama pull gemma3:4b\n",
        "# Install Packages\n",
        "!pip install -q ollama crawl4ai aiohttp pillow beautifulsoup4 wikipedia googlesearch-python playwright nest_asyncio\n",
        "!playwright install chromium\n",
        "!nohup ollama serve &\n",
        "\n"
      ],
      "metadata": {
        "id": "-b9oppEb4cm2",
        "outputId": "0d476e55-3820-4c19-857b-fa1800820599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 13281    0 13281    0     0  62797      0 --:--:-- --:--:-- --:--:-- 62943\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to render group...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 129 kB in 1s (128 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cuda-drivers is already the newest version (575.51.03-0ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 88 not upgraded.\n",
            "nohup: appending output to 'nohup.out'\n",
            "Collecting pyOpenSSL==24.2.1\n",
            "  Downloading pyOpenSSL-24.2.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: cryptography<44,>=41.0.5 in /usr/local/lib/python3.11/dist-packages (from pyOpenSSL==24.2.1) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<44,>=41.0.5->pyOpenSSL==24.2.1) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<44,>=41.0.5->pyOpenSSL==24.2.1) (2.22)\n",
            "Downloading pyOpenSSL-24.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyOpenSSL\n",
            "  Attempting uninstall: pyOpenSSL\n",
            "    Found existing installation: pyOpenSSL 25.1.0\n",
            "    Uninstalling pyOpenSSL-25.1.0:\n",
            "      Successfully uninstalled pyOpenSSL-25.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "crawl4ai 0.6.3 requires pyOpenSSL>=24.3.0, but you have pyopenssl 24.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyOpenSSL-24.2.1\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mnohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &\n",
        "import random\n",
        "import subprocess\n",
        "import wikipedia\n",
        "import requests\n",
        "from crawl4ai.content_filter_strategy import PruningContentFilter\n",
        "\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "from googlesearch import search\n",
        "from crawl4ai import LLMConfig, LLMExtractionStrategy, CrawlerRunConfig,CacheMode\n",
        "from crawl4ai.deep_crawling.filters import FilterChain, ContentRelevanceFilter\n",
        "from crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n",
        "import json\n",
        "\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "from crawl4ai.extraction_strategy import CosineStrategy\n",
        "\n",
        "from crawl4ai.async_configs import BrowserConfig\n",
        "\n",
        "from crawl4ai import AsyncWebCrawler,GeolocationConfig\n",
        "\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "import asyncio\n",
        "import time\n",
        "import requests\n",
        "import nest_asyncio\n",
        "import httpx\n",
        "\n",
        "API_BASE = \"https://www.nearestdoor.com\"  # Replace with actual server URL\n",
        "CLIENT_ID = \"client001\"\n",
        "HEARTBEAT_INTERVAL = 60  # seconds\n",
        "SHOP_FLOW_STATIC = [\n",
        "    \"search\", \"aggregate\", \"createplan\", \"create\",\n",
        "    \"find_available_fields\", \"extract_fields_from_aggregate\", \"fillintheshop\"\n",
        "]\n",
        "# --------------------------------------------------------------------------- #\n",
        "# üß†  LIGHT‚ÄëWEIGHT LOCAL LLM EXECUTION                                       #\n",
        "# --------------------------------------------------------------------------- #\n",
        "class OllamaRunner:\n",
        "    \"\"\"\n",
        "    `ollama run ‚Ä¶`\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, default_model: str = \"gemma3:1b\", default_timeout: int = 600):\n",
        "        self.default_model = default_model\n",
        "        self.default_timeout = default_timeout\n",
        "\n",
        "    def run(self, prompt: str, model: str | None = None, timeout: int | None = None) -> str:\n",
        "        model = model or self.default_model\n",
        "        timeout = timeout or self.default_timeout\n",
        "        print(f\"üß† Running Ollama: {model}\")\n",
        "\n",
        "        try:\n",
        "            proc = subprocess.run(\n",
        "                [\"ollama\", \"run\", model],\n",
        "                input=prompt.encode(\"utf-8\"),\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                timeout=timeout,\n",
        "            )\n",
        "\n",
        "            raw_output = proc.stdout.decode(\"utf-8\").strip()\n",
        "            return re.sub(r\"<think>.*?</think>\", \"\", raw_output, flags=re.DOTALL).strip()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Ollama execution failed: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# üåê  LOOK‚ÄëUP ENGINE                                                          #\n",
        "# --------------------------------------------------------------------------- #\n",
        "class LookupEngine:\n",
        "    \"\"\"\n",
        "    ‚Äì Validates every URL first\n",
        "    ‚Äì Google results exclude Yelp & Reddit and are content‚Äëchecked\n",
        "    ‚Äì Yelp & Reddit results are *also* content‚Äëchecked before ‚Äòbattling‚Äô\n",
        "    ‚Äì At most one Yelp URL & one Reddit URL are returned\n",
        "    ‚Äì Wikipedia returns at most one page (auto_suggest)\n",
        "    \"\"\"\n",
        "    def __init__(self,  ollama_runner: OllamaRunner | None = None):\n",
        "\n",
        "        self.llm_config = LLMConfig(provider=\"ollama/gemma3:1b\")\n",
        "        self.ollama = ollama_runner or OllamaRunner()\n",
        "        self.crawler_manager = self.CrawlerManager()\n",
        "    async def initialize(self):\n",
        "        await self.crawler_manager.start()\n",
        "\n",
        "\n",
        "    # ---------------------  LOW‚ÄëLEVEL HELPERS  ----------------------------- #\n",
        "    class CrawlerManager:\n",
        "        def __init__(self):\n",
        "            self.crawler = None\n",
        "\n",
        "\n",
        "\n",
        "        async def crawl(self, url: str,browserconfig, config: CrawlerRunConfig):\n",
        "\n",
        "\n",
        "            try:\n",
        "                async with AsyncWebCrawler(config=browserconfig) as crawler:\n",
        "                    result = await crawler.arun(url=url, config=config)\n",
        "                    return result\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå crawl error for {url}: {e}\")\n",
        "                return None\n",
        "\n",
        "\n",
        "        async def _normalize_and_validate_url(self, url: str) -> str | None:\n",
        "          try:\n",
        "              url = url.lower()\n",
        "              if not url.startswith((\"http://\", \"https://\")):\n",
        "                  url = \"https://\" + url\n",
        "\n",
        "              parsed = urlparse(url)\n",
        "              if (\n",
        "                  parsed.scheme not in [\"http\", \"https\"]\n",
        "                  or not parsed.netloc\n",
        "                  or \".\" not in parsed.netloc\n",
        "                  or \" \" in parsed.netloc\n",
        "                  or \"/http\" in parsed.netloc\n",
        "              ):\n",
        "                  return None  # Invalid URL\n",
        "\n",
        "              # ‚úÖ Check if the URL is reachable using httpx (this was incorrectly indented before)\n",
        "              async with httpx.AsyncClient(timeout=5) as client:  # Timeout should be in seconds, not 800ms\n",
        "                  try:\n",
        "                      response = await client.head(url, follow_redirects=True)\n",
        "                      if response.status_code < 400:\n",
        "                          return url\n",
        "                      # Some servers reject HEAD, fallback to GET\n",
        "                      response = await client.get(url, follow_redirects=True)\n",
        "                      if response.status_code < 400:\n",
        "                          return url\n",
        "                      return False  # URL unreachable\n",
        "                  except httpx.RequestError as e:\n",
        "                      print(f\"‚ùå HTTP check failed: {url} ({e})\")\n",
        "                      return None\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"‚ùå URL normalization failed: {e}\")\n",
        "              return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    async def _extract_snippet(self, url, max_chars, min_chars):\n",
        "        try:\n",
        "            async with async_playwright() as p:\n",
        "                browser = await p.chromium.launch(headless=True)\n",
        "                context = await browser.new_context(\n",
        "                    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n",
        "                    locale=\"en-US\",\n",
        "                    java_script_enabled=False,\n",
        "                    permissions=[\"geolocation\"],\n",
        "\n",
        "    viewport={\"width\": 1280, \"height\": 720}\n",
        "                )\n",
        "                page = await context.new_page()\n",
        "\n",
        "                print(f\"Fetching: {url}\")\n",
        "                await page.goto(url, wait_until=\"domcontentloaded\", timeout=20000)\n",
        "\n",
        "                await page.wait_for_timeout(random.randint(1000, 3000))  # Randomized delay\n",
        "\n",
        "\n",
        "                title = await page.title()\n",
        "                desc = None\n",
        "\n",
        "                # Try standard meta description first\n",
        "                try:\n",
        "                    desc = await page.locator('meta[name=\"description\"]').get_attribute('content')\n",
        "                except Exception:\n",
        "                    # Try OpenGraph and Twitter metadata as fallback\n",
        "                    for meta_tag in [\"meta[property='og:description']\", \"meta[name='twitter:description']\"]:\n",
        "                        try:\n",
        "                            desc = await page.locator(meta_tag).get_attribute('content')\n",
        "                            if desc:\n",
        "                                break\n",
        "                        except Exception:\n",
        "                            continue\n",
        "                    if not desc:\n",
        "                        print(f\"No meta description found for {url}\")\n",
        "\n",
        "\n",
        "                parts = []\n",
        "                if title:\n",
        "                  parts.append(\"Title: \" + title.strip())\n",
        "                if desc:\n",
        "                  parts.append(\"Desc: \" + desc.strip())\n",
        "\n",
        "                await browser.close()\n",
        "\n",
        "                result = \"\\n\".join(parts)\n",
        "\n",
        "\n",
        "                print(\"ddj\")\n",
        "                if len(str(result)) < min_chars:\n",
        "                    print(f\"‚ùå Extracted content too short: {len(result)} chars\")\n",
        "                    return None\n",
        "                result = result[:max_chars]\n",
        "                return f\"Snippet From {url}:\\n{result[:max_chars]}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Playwright extraction failed for {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    async def _basic_url_checker(self, snippet,url: str, shop_name: str, shop_type: str, state: str, city:str) -> bool:\n",
        "        try:\n",
        "\n",
        "            \"\"\"\n",
        "            Improved relevance check for a URL:\n",
        "            1. Uses Crawl4AI's ContentRelevanceFilter for semantic similarity.\n",
        "            2. Falls back to LLM prompt decision if semantic check is inconclusive.\n",
        "            \"\"\"\n",
        "\n",
        "            if not url:\n",
        "                return False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Step 2: Fallback ‚Äî Quick Snippet and LLM Yes/No Decision\n",
        "\n",
        "            if not snippet:\n",
        "                print(\"no snippet\")\n",
        "                return False\n",
        "\n",
        "            prompt = (\n",
        "                f\"Is the website {url} related at all to '{shop_name}' or in any way related to the place its located in, {state}, {city}? \"\n",
        "                f\"Be super lient and allow all websites related to {shop_type} to pass. If the name {shop_name} or {state}, {city} is present or any related info like the town, the category, ect then let it pass. Be super lient. Answer only `true` or `false` or `none`.\\n\\n{snippet}\"\n",
        "            )\n",
        "\n",
        "            decision = self.ollama.run(prompt, 'qwen3:0.6b')\n",
        "            print(decision)\n",
        "            print(prompt)\n",
        "            if \"true\" in decision.lower():\n",
        "                print(f\"‚úÖ LLM confirmed relevance for: {url}\")\n",
        "                return True\n",
        "\n",
        "            print(f\"‚ö†Ô∏è URL deemed irrelevant: {url}\")\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"error in basicc checker\", {url}, {shop_name}, {shop_type}, e)\n",
        "            return False\n",
        "    @staticmethod\n",
        "    def get_semantic_query(shop_type, shop_name):\n",
        "        queries = {\n",
        "            \"church\": f\"{shop_type}, {shop_name}, history, review, hours, muslim, phone, church, christian, church events, holiday schedules, mass times, sermons, church history, community programs, accessibility options, FAQs, donation methods, parking, contact information\",\n",
        "            \"plasma_center\": f\"{shop_type}, {shop_name}, history, review, stocked brands, review, hours, phone, plasma, plasma donation requirements, compensation rates, donor reward, donor eligibility, contact details, operating hours, health guidelines, FAQ, appointment scheduling, safety procedures\",\n",
        "            \"thrift_store\": f\"{shop_type}, {shop_name}, history, review, stocked brands, second hand,  review, hours, phone, thrift, store hours, donation guidelines, accepted items, discounts, sales events, store history, accessibility, contact info, volunteer programs, reviews\",\n",
        "            \"dog_park\": f\"{shop_type}, {shop_name}, history, review, water, shade, agility equipment, park, review, hours, phone, dog, dog park hours, leash rules, pet-friendly areas, dog-friendly facilities, park amenities, accessibility options, entry fees, safety tips, events, pet policies, reviews\",\n",
        "        }\n",
        "        return queries.get(shop_type.lower(), \"business information, contact details, operating hours, reviews, FAQs, history\")\n",
        "\n",
        "    async def _get_site_content(\n",
        "\n",
        "            self,\n",
        "\n",
        "            url: str,\n",
        "            shop_name: str,\n",
        "            shop_type: str,\n",
        "state:str, city:str\n",
        "            ) -> str | None:\n",
        "        try:\n",
        "            print(\"scraping\")\n",
        "        # Step 1: Semantic Filter Based on Shop Type\n",
        "            #semantic_query = self.get_semantic_query(shop_type, shop_name)\n",
        "\n",
        "\n",
        "            prune_filter = PruningContentFilter(\n",
        "                        threshold=0.05,\n",
        "                        threshold_type=\"dynamic\",  # or \"dynamic\"\n",
        "                        min_word_threshold=5\n",
        "                    )\n",
        "\n",
        "            md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)\n",
        "\n",
        "\n",
        "            crawl_config = CrawlerRunConfig(\n",
        "\n",
        "                  markdown_generator=md_generator,\n",
        "\n",
        "\n",
        "                  excluded_tags=[\"style\", \"script\", \"footer\"],\n",
        "  cache_mode=CacheMode.BYPASS,\n",
        "    page_timeout=60000,\n",
        "\n",
        "\n",
        "            )\n",
        "            bconfig = BrowserConfig(\n",
        "            headless=True,\n",
        "            viewport_width=1280,\n",
        "                    viewport_height=720,\n",
        "                   user_agent_mode=\"random\",\n",
        "                    text_mode=True\n",
        "            )\n",
        "            result = await self.crawler_manager.crawl(url, bconfig,config=crawl_config)\n",
        "\n",
        "\n",
        "            if result.success:\n",
        "\n",
        "                  print(result.markdown.fit_markdown)\n",
        "                  return result.markdown.fit_markdown\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to parse site content: {e}\")\n",
        "            return None\n",
        "    # ---------------------  LOOK‚ÄëUP ROUTINES  ------------------------------ #\n",
        "    async def wikipedia_lookup(self, name: str, city: str, shop_type: str) -> str | None:\n",
        "        try:\n",
        "            query = f\"{name} {city} {shop_type}\".strip()\n",
        "            print(f\"üìö Wikipedia lookup ‚Üí {query}\")\n",
        "            page = wikipedia.page(query, auto_suggest=True)\n",
        "            content = page.content\n",
        "\n",
        "            if len(content) <= 2000:\n",
        "                return content\n",
        "            chunks = [content[i:i + 500] for i in range(0, len(content), 500)]\n",
        "            # Fallback-safe middle extraction\n",
        "            if len(chunks) > 6:\n",
        "                middle = chunks[2:-2]  # Remove first and last 2 chunks\n",
        "                if not middle:\n",
        "                    middle = chunks  # If middle is empty, fallback to all chunks\n",
        "            else:\n",
        "                middle = chunks\n",
        "\n",
        "            # Intelligent selection\n",
        "            if len(middle) > 6:\n",
        "                selected = random.sample(middle, 6)  # Randomly select 6 if too many\n",
        "            else:\n",
        "                selected = middle  # Take all available if 6 or fewer\n",
        "            formatted_chunks = [f\"\\nWIKI CHUNK {idx + 1}:\\n{chunk}\" for idx, chunk in enumerate(selected)]\n",
        "\n",
        "            return f\"ALL EXTRACTED WIKIPEDIA SEARCH INFO FOR {name}:\\n\" + \"\\n\".join(formatted_chunks)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Wikipedia fetch failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    async def search_lookup(self,  name: str,  shop_type: str, query: str, placename: str, amount: int,state:str, city:str, isyelp: bool) -> str | None:\n",
        "        try:\n",
        "            print(f\"üîé {placename} search ‚Üí {query}\")\n",
        "            print(\"dd\")\n",
        "            raw = list(search(query, amount))\n",
        "            if not isyelp:\n",
        "                print(\"ddf\")\n",
        "                candidate_urls = [\n",
        "                      u for u in raw\n",
        "                      if all(excl not in str(u).lower() for excl in [\"yelp\", \"reddit\", \"wiki\", \"nearestdoor\", \"facebook\", \"twitter\"])\n",
        "                  ]\n",
        "                print(\"bfd\")\n",
        "            else:\n",
        "                print(\"rgrg\")\n",
        "                candidate_urls = [\n",
        "                      u for u in raw\n",
        "                      if (\"yelp\" in str(u).lower() or \"reddit\" in str(u).lower())\n",
        "                      and all(excl not in str(u).lower() for excl in [\"wiki\", \"nearestdoor\", \"facebook\", \"twitter\"])\n",
        "                  ]\n",
        "                print(\"frg\")\n",
        "\n",
        "            good_content = []\n",
        "\n",
        "            print(\"ddd\")\n",
        "            for i, url in enumerate(candidate_urls):\n",
        "                print(url, \"url canidate\")\n",
        "                urld = await self.crawler_manager._normalize_and_validate_url(url)\n",
        "                if not urld:\n",
        "                    continue\n",
        "                snippet = await self._extract_snippet(url, 750, 40)\n",
        "                if not await self._basic_url_checker(snippet, urld, name, shop_type, state, city):\n",
        "                    continue\n",
        "\n",
        "                content = await self._get_site_content(urld, name, shop_type,state, city)\n",
        "                if content:\n",
        "                    good_content.append(f\"\\n‚Üê¬†{placename} SEARCH¬†DATA¬†SITE {i} FROM: {url}\\n {content}\")\n",
        "\n",
        "            return f\"ALL EXTRACTED {placename} SEARCH DATA FOR {name}, {city}\\n\".join(good_content) if good_content else None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to parse {placename} search lookup results: {e}\")\n",
        "            return None\n",
        "    # ---------------------  PUBLIC ENTRY POINT  ---------------------------- #\n",
        "    async def combined_search(self, name: str, city: str, state: str, shop_type: str, website_url: str) -> tuple[bool, str | None, None]:\n",
        "        print(\"üåê Starting combined search‚Ä¶\")\n",
        "\n",
        "        Google_query = f\"{name} {city} {state} {shop_type} \"\n",
        "\n",
        "        if website_url:\n",
        "            Official_query = f\"{name} site: {website_url} \"\n",
        "\n",
        "        g_res = await self.search_lookup(name, shop_type, Google_query, \"Google\", 10,state, city,  False)\n",
        "        w_res = await self.wikipedia_lookup(name, city, shop_type)\n",
        "        o_res = None\n",
        "        if website_url:\n",
        "            o_res = await self.search_lookup(name, shop_type, Official_query, f\"Official Website of {name}\", 5,state, city, True)\n",
        "        main = \"\"\n",
        "\n",
        "\n",
        "        if g_res:\n",
        "            main += g_res\n",
        "        if w_res:\n",
        "            main += w_res\n",
        "        if o_res:\n",
        "            main += o_res\n",
        "        print('sss')\n",
        "        if len(main) < 500:\n",
        "            print(\"‚ùå Not enough content gathered.\")\n",
        "            return False, None, None\n",
        "\n",
        "        print(\"‚úÖ Combined search complete.\")\n",
        "        return True, main, None\n",
        "# --------------------------------------------------------------------------- #\n",
        "# üì¶  HIGH‚ÄëLEVEL CONTENT¬†SUMMARIZER                                          #\n",
        "# --------------------------------------------------------------------------- #\n",
        "class ContentSummarizer:\n",
        "    \"\"\"\n",
        "    Reduce a large blob of text about a specific business down to ‚â§‚ÄØmax_final_chars\n",
        "    while preserving high‚Äëvalue facts. Uses multi-stage LLM summarisation with\n",
        "    chunk filtering and escalation if necessary.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        ollama_runner: OllamaRunner,\n",
        "        shop_name: str,\n",
        "        shop_type: str,\n",
        "        city: str | None = None,\n",
        "        state: str | None = None,\n",
        "        max_final_chars: int = 6000,\n",
        "        min_final_chars: int = 500,\n",
        "    ):\n",
        "        self.ollama = ollama_runner\n",
        "        self.shop_name = shop_name\n",
        "        self.shop_type = shop_type\n",
        "        self.city = city or \"\"\n",
        "        self.state = state or \"\"\n",
        "        self.max_final_chars = max_final_chars\n",
        "        self.min_final_chars = min_final_chars\n",
        "\n",
        "    # ----------------- Internal Helpers ----------------- #\n",
        "    def _clean_raw_content(self, content: str) -> str:\n",
        "        lines = content.splitlines()\n",
        "        cleaned, seen = [], set()\n",
        "        NOISE = [\n",
        "            \"cookie policy\", \"all rights reserved\", \"subscribe\", \"advertisement\",\n",
        "            \"accept cookies\", \"privacy policy\", \"terms of service\", \"sign in\", \"cookie\"\n",
        "        ]\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            lo = line.lower()\n",
        "            if len(line) < 30 or lo in seen:\n",
        "                continue\n",
        "            if any(noise in lo for noise in NOISE):\n",
        "                continue\n",
        "            seen.add(lo)\n",
        "            cleaned.append(line)\n",
        "        return \"\\n\".join(cleaned)\n",
        "\n",
        "    def _chunk_text(self, text: str, chunk_size: int) -> list[str]:\n",
        "        return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "    def _filter_chunks(self, chunks: list[str], model: str = \"gemma3:1b\") -> list[str]:\n",
        "        try:\n",
        "            good = []\n",
        "            print(f\"Filtering {len(chunks)} chunks...\")\n",
        "            for chunk in chunks:\n",
        "                prompt = (\n",
        "                    f\"Is the following content related at all to {self.shop_type}, {self.city} {self.state}\"\n",
        "                    f\"'{self.shop_name}'. Be super lient. Only reply 'true' or 'false'.\\n\\n{chunk}\"\n",
        "                )\n",
        "                print(\"prompt\", prompt)\n",
        "                decision = self.ollama.run(prompt, model=model).strip().lower()\n",
        "                print(\"decision\", decision)\n",
        "                if \"true\" in decision:\n",
        "                    good.append(chunk)\n",
        "\n",
        "            return good\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to filter chunk: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _build_prompt(self, text_chunk: str) -> str:\n",
        "        return (\n",
        "            f\"You are creating a SHORT unformatted summary for \"\n",
        "            f\"{self.shop_type} **{self.shop_name}** \"\n",
        "            f\"{'in ' + self.city if self.city else ''}{', ' + self.state if self.state else ''}.\\n\\n\"\n",
        "            f\"KEEP USEFUL DATA\"\n",
        "            \"DO NOT USE ASTERISKS OR * OR **\"\n",
        "            \"KEEP SERVICES.\\n\"\n",
        "            f\"KEEP URLS ONLY WHEN IT IS THE LITERAL {self.shop_name}'s WEBSITE.\\n\"\n",
        "            f\"KEEP ALL GOOD INFO AND HISTORY, FACTS, INFO, Extract all usefull info,\\n\"\n",
        "\n",
        "            f\" remove weird info.\\n\"\n",
        "\n",
        "            f\"--- SOURCE TEXT START ---\\n{text_chunk}\\n--- SOURCE TEXT END ---\"\n",
        "\n",
        "        )\n",
        "\n",
        "    def _summarize_with_ollama(self, text: str, model: str) -> str:\n",
        "        try:\n",
        "            prompt = self._build_prompt(text)\n",
        "            print(\"prompt summarize\", prompt)\n",
        "            return self.ollama.run(prompt, model=model)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to summarize w ollama: {e}\")\n",
        "            return None\n",
        "    def summarize_chunks(\n",
        "        self,\n",
        "        content: str,\n",
        "\n",
        "        initial_model: str = \"gemma3:1b\",\n",
        "\n",
        "    ) -> str:\n",
        "        try:\n",
        "            if len(content) < self.min_final_chars:\n",
        "                print(\"filtered less than min\")\n",
        "                return None\n",
        "            if len(content) > self.max_final_chars:\n",
        "\n",
        "                chunks = self._chunk_text(content, 1000)\n",
        "\n",
        "                content = self._filter_chunks(chunks)\n",
        "                if not content:\n",
        "                    content = chunks\n",
        "                content = ''.join(content)\n",
        "\n",
        "            if len(content) < self.min_final_chars:\n",
        "                print(\"filtered less than min2\")\n",
        "                return None\n",
        "            chunks = self._chunk_text(content, 3000)\n",
        "\n",
        "            summarized_chunks = []\n",
        "\n",
        "            for idx, chunk in enumerate(chunks, start=1):\n",
        "                print(f\"üìö Summarizing chunk {idx}/{len(chunks)}...\")\n",
        "                summary = self._summarize_with_ollama(chunk, model=initial_model)\n",
        "                print(\"summary\", summary)\n",
        "                if not summary or len(summary) < 50:\n",
        "                    print(f\"‚ö†Ô∏è Failed to summarize chunk {idx}, keeping raw content.\")\n",
        "                    summary = chunk  # Fallback to raw content if summary failed\n",
        "\n",
        "                summarized_chunks.append(f\"### CHUNK {idx} SUMMARY:\\n{summary}\")\n",
        "\n",
        "                # Early exit: check if adding all remaining raw chunks without summarizing fits within limit\n",
        "                combined_so_far = \"\\n\\n\".join(summarized_chunks)\n",
        "                remaining_raw = \"\".join(chunks[idx:])  # Remaining chunks after current one\n",
        "\n",
        "                if len(combined_so_far) + len(remaining_raw) <= self.max_final_chars:\n",
        "                    print(f\"‚úÖ Early exit: current summary + remaining raw fits within limit. Skipping further summarization.\")\n",
        "                    for r_idx, remaining_chunk in enumerate(chunks[idx:], start=idx + 1):\n",
        "                        summarized_chunks.append(f\"### CHUNK {r_idx} (Raw):\\n{remaining_chunk}\")\n",
        "                    break\n",
        "\n",
        "            combined_summary = \"\\n\\n\".join(summarized_chunks)\n",
        "            if len(combined_summary) < self.min_final_chars:\n",
        "                print(\"smmariez less than min\")\n",
        "                return None\n",
        "            # Final trim if absolutely necessary\n",
        "            if len(combined_summary) > self.max_final_chars:\n",
        "                print(\"‚ö†Ô∏è Final combined summary exceeds character limit. Trimming result.\")\n",
        "                return combined_summary[:self.max_final_chars]\n",
        "\n",
        "            return combined_summary\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to sumarrize chunks: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    def summarize_content(self, raw_content: str) -> str:\n",
        "        print(\"üßπ Cleaning raw content...\")\n",
        "        cleaned = self._clean_raw_content(raw_content)\n",
        "        print(cleaned)\n",
        "\n",
        "        print(\"summarizing\")\n",
        "        final_summary = self.summarize_chunks(\n",
        "        content=cleaned,           # The large raw text content you want to reduce\n",
        "               # Size of each chunk before summarizing\n",
        "        initial_model=\"gemma3:1b\",      # Start with the lightweight model\n",
        "\n",
        "    )\n",
        "        print(final_summary)\n",
        "        if final_summary is None:\n",
        "            print(\"‚ùå Summarization failed. Returning none instead.\")\n",
        "            return '', False\n",
        "\n",
        "        print(\"üéâ Final summarization complete.\")\n",
        "        return final_summary, True\n",
        "\n",
        "import re\n",
        "import json\n",
        "\n",
        "class Smartypants:\n",
        "    def __init__(self, ollama_runner: OllamaRunner):\n",
        "        self.ollama = ollama_runner\n",
        "\n",
        "    def _run4(self, prompt: str) -> str:\n",
        "        return self.ollama.run(prompt, model=\"gemma3:4b\",)\n",
        "    def _run1(self, prompt: str) -> str:\n",
        "        return self.ollama.run(prompt, model=\"gemma3:1b\")\n",
        "\n",
        "    # ------------ PLAN ------------ #\n",
        "    def create_plan(self, aggregate: str,shop_name: str, shop_type: str,  city: str, state: str) -> tuple[bool, list[str]]:\n",
        "        prompt = (\n",
        "            \"You have an aggregated summary about a \"\n",
        "\n",
        "            f\"place called {shop_name} {shop_type} in {city}, {state}\\n\\n\"\n",
        "            f\"{aggregate}\\n\\n\"\n",
        "            \"Which content sections can confidently be generated based on this?\\n\"\n",
        "            \"Options: article, faq, history.\\n\"\n",
        "            \"Reply with a correct python comma-separated list of available sections to write about, nothing else, Options: article, faq, history.\"\n",
        "        )\n",
        "        try:\n",
        "            count = 0\n",
        "            resp = self._run4(prompt).lower()\n",
        "            valid = {\"article\", \"faq\", \"history\"}\n",
        "            print(resp)\n",
        "            for s in valid:\n",
        "                if s not in resp:\n",
        "                    count +=1\n",
        "            if count == 3:\n",
        "                return False, []\n",
        "\n",
        "            return True, [s for s in valid if s in resp]\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå create_plan error: {e}\")\n",
        "            return False, []\n",
        "\n",
        "    def check_aggregate_quality(self, shop_name: str, aggregate: str,shop_type: str,  city: str, state: str) -> bool:\n",
        "        prompt = (\n",
        "            f\"Check if this content contains usefull info about {shop_name} {shop_type} in {city}, {state}.\\n\\n{aggregate}\\n\\n\"\n",
        "            \"Reply only `true` or `false`.\"\n",
        "        )\n",
        "        try:\n",
        "            return \"true\" in self._run1(prompt).lower()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå quality check error: {e}\")\n",
        "            return False\n",
        "\n",
        "    # ------------ SECTION VALIDATION / FIX ------------ #\n",
        "    def validate_section_html(self, shop_name: str, section: str, text: str) -> bool:\n",
        "        prompt = (\n",
        "\n",
        "            f\"Validate the following text for section '{section}'. It is supposed to be about '{shop_name}'.\\n\\n{text}\\n\\n\"\n",
        "            \"Rules: DO NOT ALLOW ASTERISKS, DO NOT ALLOW * OR **.\\n- No HTML.\\n- No irrelevant info. Is it useful and no format or weird characters? Nothing Else, Nothing before or after our content\\n\"\n",
        "            \"- Only factual, structured, and clear content.\\n- Reply `true` or `false` only.\"\n",
        "        )\n",
        "        print(\"validate html\", prompt)\n",
        "        return \"true\" in self._run1(prompt).lower()\n",
        "\n",
        "    def fix_section_html(self, shop_name: str, section: str, text: str) -> str | None:\n",
        "        prompt = (\n",
        "            f\"Clean and fix this section '{section}'s format. It is about '{shop_name}'.\\n\\n{text}\\n\\n\"\n",
        "            \"Rules:DO NOT USE ASTERISKS, DO NOT USE * OR **. \\n- No HTML, Is it useful and and no format or weird characters? asterisks, or irrelevant info.\\n\"\n",
        "            \"Return only the final cleaned text for consumers on nearestdoor.com to read, no junk, no explanations, nothing else, nothing before or after our content. Only return the corrected text.\"\n",
        "        )\n",
        "        print(\"fixing html\",prompt)\n",
        "        return self._run4(prompt).strip()\n",
        "\n",
        "    # ------------ JSON & FIELD EXTRACTION ------------ #\n",
        "\n",
        "    def extract_clean_json_structure(self,text: str, field_name: str = None) -> dict | list | None:\n",
        "        try:\n",
        "            # ‚úÖ Extract content inside ```json ... ```\n",
        "            json_block = re.search(r\"```json\\s*(.*?)\\s*```\", text, re.IGNORECASE | re.DOTALL)\n",
        "            if json_block:\n",
        "                text = json_block.group(1).strip()\n",
        "            else:\n",
        "                text = text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "            # ‚úÖ Extract JSON object or array\n",
        "            match = re.search(r\"(\\{.*?\\}|\\[.*?\\])\", text, re.DOTALL)\n",
        "            if not match:\n",
        "                return None\n",
        "\n",
        "            json_ready = match.group(0)\n",
        "            match_text_lower = json_ready.lower()\n",
        "\n",
        "            # ‚úÖ Exclude meaningless content\n",
        "            exclusion_keywords = ['n/a', 'n-a', 'none', 'false', 'na', 'cant', 'not', 'found', 'unable', '{{', '()', 'unavailable']\n",
        "            if any(bad in match_text_lower for bad in exclusion_keywords):\n",
        "                return None\n",
        "\n",
        "            # ‚úÖ Clean JSON formatting issues\n",
        "            json_ready = json_ready.replace(\"'\", '\"')\n",
        "            json_ready = re.sub(r\",\\s*([\\]}])\", r\"\\1\", json_ready)\n",
        "\n",
        "            parsed = json.loads(json_ready)\n",
        "\n",
        "            # ‚úÖ Handle List: Deduplicate and Title Case\n",
        "            if isinstance(parsed, list):\n",
        "                seen = set()\n",
        "                cleaned_list = []\n",
        "                for item in parsed:\n",
        "                    if isinstance(item, str):\n",
        "                        cleaned_item = item.strip().title()\n",
        "                        if cleaned_item and cleaned_item.lower() not in exclusion_keywords and cleaned_item not in seen:\n",
        "                            seen.add(cleaned_item)\n",
        "                            cleaned_list.append(cleaned_item)\n",
        "                result = cleaned_list if cleaned_list else None\n",
        "\n",
        "            # ‚úÖ Handle Dict: Clean keys and values\n",
        "            elif isinstance(parsed, dict):\n",
        "                cleaned_dict = {}\n",
        "                for k, v in parsed.items():\n",
        "                    if isinstance(v, str):\n",
        "                        v_clean = v.strip().lower()\n",
        "                        if v_clean in exclusion_keywords:\n",
        "                            continue\n",
        "                        cleaned_dict[k.title()] = v.title()\n",
        "                    else:\n",
        "                        cleaned_dict[k.title()] = v\n",
        "                result = cleaned_dict if cleaned_dict else None\n",
        "\n",
        "            else:\n",
        "                result = None\n",
        "\n",
        "            # ‚úÖ Final Validation Using FIELD_VALIDATORS\n",
        "            if field_name and field_name in FIELD_VALIDATORS:\n",
        "                validator = FIELD_VALIDATORS[field_name]\n",
        "                if not validator(result):\n",
        "                    print(f\"‚ùå Validation failed for field: {field_name} with value: {result}\")\n",
        "                    return None\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå JSON extraction failed: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "    def extract_available_fields(self, aggregate: str,shop_name: str, shop_type: str,  city: str, state: str) -> tuple[bool, list[str]]:\n",
        "        try:\n",
        "            field_list = list(FIELD_EXTRACTORS.keys())\n",
        "            field_str = ', '.join(field_list)\n",
        "            prompt = (\n",
        "                f\"Analyze the content:\\n\\n{aggregate}\\n\\n\"\n",
        "                f\"Whos data we want {shop_type}, {shop_name}. \"\n",
        "\n",
        "                f\"Which of these fields can be confidently extracted from the content?\\n{field_str}\\n\"\n",
        "                \"Reply ONLY with the field keys that are present in the text, as ONLY the correct format requested. no junk. Nothing else. \"\n",
        "                \"If none, reply exactly 'none'.\"\n",
        "            )\n",
        "            response = self._run4(prompt).strip().lower()\n",
        "            if \"none\" in response:\n",
        "                return True, []\n",
        "            detected = [field for field in field_list if field.lower() in response]\n",
        "            return True, detected\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå extract_available_fields error: {e}\")\n",
        "            return False, []\n",
        "    def validate_extracted_field_value(self, field: str,fieldextractprompt, value) -> bool:\n",
        "        \"\"\"\n",
        "        Validate the extracted field value using LLM and manual schema checks.\n",
        "\n",
        "        - If it's a JSON list, remove invalid entries.\n",
        "        - If it's invalid after cleaning, return False.\n",
        "        \"\"\"\n",
        "        # LLM-Based Validation Prompt\n",
        "        prompt = (\n",
        "            f\"Validate this extracted value for field '{field}':\\n\\n{value}\\n\\n, {fieldextractprompt} \"\n",
        "            \"Is this a valid and correct value and format for the specified field requested? Is it weird for the field or contain none values? Reply ONLY `true` or `false`.\"\n",
        "        )\n",
        "        print(\"validate field\", prompt)\n",
        "        llm_decision = \"true\" in self._run1(prompt).lower()\n",
        "        print(\"field valid decision\", llm_decision)\n",
        "        # If LLM says it's invalid, fail immediately\n",
        "        if not llm_decision:\n",
        "            print(f\"‚ùå LLM validation failed for field '{field}'.\")\n",
        "            return False\n",
        "\n",
        "\n",
        "\n",
        "        # Final check for singular values\n",
        "        return True\n",
        "\n",
        "    def extract_fields(\n",
        "        self, aggregate: str, available_fields: list[str],\n",
        "        shop_name: str, shop_type: str, city: str, state: str\n",
        "    ) -> tuple[bool, dict]:\n",
        "        extracted = {}\n",
        "        try:\n",
        "            for field in available_fields:\n",
        "                try:\n",
        "                    print(field)\n",
        "                    prompt = (\n",
        "                        f\"{FIELD_EXTRACTORS[field]}\\n\\nContent:\\n{aggregate}\\n\\n\"\n",
        "                        f\"Return ONLY the valid structure requested. Respond with nothing but the correct format requested. \"\n",
        "                        f\"If none, say 'none'. Nothing else, nothing before or after our content. Data about {shop_type}, {shop_name}.\"\n",
        "                    )\n",
        "                    print(\"aggregate\",aggregate)\n",
        "                    print(\"extracting fields\",prompt )\n",
        "                    raw_value = self._run4(prompt).strip()\n",
        "                    print(\"raw field\", raw_value)\n",
        "                    final_value = self.extract_clean_json_structure(raw_value, field)\n",
        "                    if final_value is None:\n",
        "                        print(\"final value none\")\n",
        "                        continue\n",
        "                    if self.validate_extracted_field_value(field,{FIELD_EXTRACTORS[field]}, final_value):\n",
        "                        print(\"final value\", final_value)\n",
        "                        extracted[field] = final_value\n",
        "                    else:\n",
        "                        print(\"final value failed validate\")\n",
        "                        continue\n",
        "                except Exception as inner_e:\n",
        "                    print(f\"‚ö†Ô∏è Field extraction failed for '{field}': {inner_e}\")\n",
        "                    continue\n",
        "\n",
        "            return True, extracted if extracted else {}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è extract_fields failed: {e}\")\n",
        "            return False, {}\n",
        "\n",
        "\n",
        "    # ------------ SECTION GENERATION ------------ #\n",
        "    def create_sections(\n",
        "        self, shop_name: str, shop_type: str, aggregate: str,\n",
        "        approved_sections: list[str], city: str | None = None, state: str | None = None\n",
        "    ) -> tuple[bool, dict]:\n",
        "        def _generate(section: str, prompt: str) -> str | None:\n",
        "            raw = self._run4(prompt).strip()\n",
        "            if self.validate_section_html(shop_name, section, raw):\n",
        "                print(\"validated\")\n",
        "                return raw\n",
        "            fixed = self.fix_section_html(shop_name, section, raw)\n",
        "            print(\"fixed html\", fixed)\n",
        "\n",
        "            return fixed if fixed and self.validate_section_html(shop_name, section, fixed) else None\n",
        "\n",
        "        location = f\"in {city}, {state}\" if city or state else \"\"\n",
        "        base_instr = (\n",
        "            f\"You are writing for nearestdoor.com about our listing, about the {shop_type} '{shop_name}' {location}. \"\n",
        "            f\"You will get a summary of this place and write useful information according to your assignment which consumers will read as you write it on the nearestdoor.com website listing page for {shop_name}, no bad info or bad formatting. Urls will be https://example.com formatted.\"\n",
        "            \"Be factual, SEO-friendly, help the users learn use this place and learn about it. no unrelated info, no HTML and no asterisks. NO ASTERISKS, NO **, nothing else, nothing before or after our content. DO NOT USE * \"\n",
        "        )\n",
        "\n",
        "        sections = {}\n",
        "        try:\n",
        "            print(\"approved sections\", approved_sections)\n",
        "            if \"article\" in approved_sections:\n",
        "                prompt = f\"{base_instr}\\n\\nContent:\\n{aggregate}\\n\\nAssignment: Write a detailed article. DO NOT USE ASTERISKS, DO NOT USE * OR **. Write an article about {shop_name} for nearestdoor.com.\"\n",
        "                result = _generate(\"article\", prompt)\n",
        "\n",
        "                if result:\n",
        "                    print(\"article\", result)\n",
        "                    sections[\"article\"] = result\n",
        "\n",
        "            if \"faq\" in approved_sections:\n",
        "                prompt = f\"{base_instr}\\n\\nContent:\\n{aggregate}\\n\\nAssignment: Write a detailed FAQ. DO NOT USE ASTERISKS, DO NOT USE * OR **. Write an FAQ about {shop_name} for nearestdoor.com.\"\n",
        "                result = _generate(\"faq\", prompt)\n",
        "                if result:\n",
        "                    print(\"faq\", result)\n",
        "                    sections[\"faq\"] = result\n",
        "\n",
        "            if \"history\" in approved_sections:\n",
        "                prompt = f\"{base_instr}\\n\\nContent:\\n{aggregate}\\n\\nAssignment: DO NOT USE ASTERISKS. DO NOT USE * OR **. Write the history section about {shop_name} for nearestdoor.com.\"\n",
        "                result = _generate(\"history\", prompt)\n",
        "                if result:\n",
        "                    print(\"history\", result)\n",
        "                    sections[\"history\"] = result\n",
        "\n",
        "            return True, sections\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå create_sections error: {e}\")\n",
        "            return False, {}\n",
        "\n",
        "    # ------------ FULL WORKFLOW ------------ #\n",
        "    def process(\n",
        "        self, shop_name: str, shop_type: str, aggregate: str,\n",
        "        city: str | None = None, state: str | None = None\n",
        "    ) -> dict:\n",
        "        result = {\"plan\": [], \"sections\": {}, \"fields\": {}}\n",
        "        print(\"got aggregate checking quality\", aggregate)\n",
        "        if not self.check_aggregate_quality(shop_name, aggregate, shop_type, city, state):\n",
        "            print(\"‚ùå Aggregate failed quality check.\")\n",
        "            return None\n",
        "        print(\"making plan\")\n",
        "        ok, plan = self.create_plan(aggregate, shop_name, shop_type, city, state)\n",
        "        if not ok or not plan:\n",
        "            print(\"‚ùå No sections can be created.\")\n",
        "            return None\n",
        "        print(\"plan\", plan)\n",
        "        result[\"plan\"] = plan\n",
        "        print(\"making sections\")\n",
        "        ok, sections = self.create_sections(shop_name, shop_type, aggregate, city, state)\n",
        "        if ok:\n",
        "            result[\"sections\"] = sections\n",
        "        print('sections', sections)\n",
        "        print(\"making fields\")\n",
        "        ok, available = self.extract_available_fields(aggregate, shop_name, shop_type, city, state)\n",
        "        print(\"avaliabale\", available)\n",
        "        if ok and available:\n",
        "            print(\"making fields\")\n",
        "            ok, fields = self.extract_fields(aggregate, available,shop_name, shop_type,city, state)\n",
        "\n",
        "            if ok:\n",
        "                print(\"fields\", fields)\n",
        "                result[\"fields\"] = fields\n",
        "\n",
        "        return result\n",
        "\n",
        "import re\n",
        "import json\n",
        "\n",
        "\n",
        "def is_non_empty_string(value) -> bool:\n",
        "    return isinstance(value, str) and len(value.strip()) > 0\n",
        "\n",
        "def is_valid_json(value) -> bool:\n",
        "    if isinstance(value, (dict, list)):\n",
        "        return True\n",
        "    try:\n",
        "        json.loads(value)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def is_valid_phone(value) -> bool:\n",
        "    if not isinstance(value, str):\n",
        "        return False\n",
        "    return bool(re.fullmatch(r\"\\d{3}-\\d{3}-\\d{4}\", value.strip()))\n",
        "\n",
        "def is_valid_email(value) -> bool:\n",
        "    if not isinstance(value, str):\n",
        "        return False\n",
        "    return bool(re.fullmatch(r\"[^@\\s]+@[^@\\s]+\\.[a-zA-Z0-9]+\", value.strip()))\n",
        "\n",
        "def is_valid_url(value) -> bool:\n",
        "    return isinstance(value, str) and value.strip().lower().startswith(\"http\")\n",
        "\n",
        "def is_valid_dict(value) -> bool:\n",
        "    if isinstance(value, dict):\n",
        "        return True\n",
        "    try:\n",
        "        return isinstance(json.loads(value), dict)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def is_valid_list(value) -> bool:\n",
        "    if isinstance(value, list):\n",
        "        return True\n",
        "    try:\n",
        "        return isinstance(json.loads(value), list)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def is_positive_integer_or_string(value) -> bool:\n",
        "    try:\n",
        "        return int(str(value).strip()) > 0\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "FIELD_VALIDATORS = {\n",
        "    # Contact Info\n",
        "    \"extract_phone\": is_valid_phone,\n",
        "    \"extract_email\": is_valid_email,\n",
        "    \"extract_website\": is_valid_url,\n",
        "\n",
        "    # Structured Fields\n",
        "    \"extract_categories\": is_valid_list,\n",
        "    \"extract_operating_hours\": is_valid_dict,\n",
        "    \"extract_holiday_hours\": is_valid_dict,\n",
        "    \"extract_delivery_services\": is_valid_list,\n",
        "    \"extract_social_media\": is_valid_dict,\n",
        "    \"extract_stocked_brands\": is_valid_list,\n",
        "    \"extract_inventory_categories\": is_valid_dict,\n",
        "    \"extract_customer_reviews\": is_valid_list,\n",
        "\n",
        "    # Event / Misc\n",
        "    \"extract_admission\": is_non_empty_string,\n",
        "    \"extract_date_available\": is_non_empty_string,\n",
        "    \"extract_attendance_amount\": is_positive_integer_or_string,\n",
        "    \"extract_exhibitor_amount\": is_positive_integer_or_string,\n",
        "}\n",
        "FIELD_EXTRACTORS = {\n",
        "    # Contact Information\n",
        "    \"extract_phone\": (\n",
        "        \"Extract ONLY the phone number in this format: 727-237-2132. \"\n",
        "        \"Return ONLY the number, no quotes, no text, no comments, no markup.\"\n",
        "    ),\n",
        "    \"extract_email\": (\n",
        "        \"Extract ONLY the email address. Example: example@mail.com. \"\n",
        "        \"Return ONLY the email address, no quotes, no text, no extras.\"\n",
        "    ),\n",
        "    \"extract_website\": (\n",
        "        \"Extract ONLY the official website URL. Must be the offical url for this business/place or return none. Example: https://website.com. \"\n",
        "        \"Return ONLY the URL, no quotes, no text, no markup.\"\n",
        "    ),\n",
        "\n",
        "    # JSON / Structured Fields\n",
        "    \"extract_categories\": (\n",
        "        \"Extract ONLY the SHORT product/service categories in JSON list format. \"\n",
        "        \"Example: ['Thrift Store', 'Charity']. Return ONLY the JSON array.\"\n",
        "    ),\n",
        "    \"extract_operating_hours\": (\n",
        "        \"Extract ONLY weekly operating hours in JSON dictionary format. \"\n",
        "        \"Example: {'monday': '9:00 AM - 5:00 PM', 'sunday': 'Closed'}. \"\n",
        "        \"Return ONLY the JSON object.\"\n",
        "    ),\n",
        "    \"extract_holiday_hours\": (\n",
        "        \"Extract ONLY holiday-specific hours in JSON dictionary format. \"\n",
        "        \"Example: {'2024-12-25': 'Closed', '2024-12-31': '10:00 AM - 4:00 PM'}. \"\n",
        "        \"Return ONLY the JSON object.\"\n",
        "    ),\n",
        "    \"extract_delivery_services\": (\n",
        "        \"Extract ONLY available delivery services in JSON list format. \"\n",
        "        \"Example: ['Uber Eats', 'Self Delivery']. Return ONLY the JSON array.\"\n",
        "    ),\n",
        "    \"extract_social_media\": (\n",
        "        \"Extract ONLY social media links in JSON dictionary format. \"\n",
        "        \"Example: {'facebook': 'https://facebook.com/example', 'instagram': 'https://instagram.com/example'}. \"\n",
        "        \"Return ONLY the JSON object.\"\n",
        "    ),\n",
        "    \"extract_stocked_brands\": (\n",
        "        \"Extract ONLY stocked brands in JSON list format. \"\n",
        "        \"Example: ['Nike', 'Adidas']. Return ONLY the JSON array.\"\n",
        "    ),\n",
        "    \"extract_inventory_categories\": (\n",
        "        \"Extract ONLY inventory categories in JSON dictionary format. \"\n",
        "        \"Example: {'Apparel': ['Shirts', 'Hoodies']}. Return ONLY the JSON object.\"\n",
        "    ),\n",
        "    \"extract_customer_reviews\": (\n",
        "        \"Extract ONLY customer reviews in JSON list format. \"\n",
        "        \"Example: [{'user': 'John', 'comment': 'Great store!', 'rating': 5}]. \"\n",
        "        \"Return ONLY the JSON array.\"\n",
        "    ),\n",
        "\n",
        "    # Event / Scheduling\n",
        "    \"extract_admission\": (\n",
        "        \"Extract ONLY the admission cost or entry fee. Return ONLY the plain text, no prefixes or suffixes.\"\n",
        "    ),\n",
        "    \"extract_date_available\": (\n",
        "        \"Extract ONLY the available date range or date description. \"\n",
        "        \"Example: 'Available from May 1st to June 30th'. Return ONLY the plain text.\"\n",
        "    ),\n",
        "    \"extract_attendance_amount\": (\n",
        "        \"Extract ONLY the expected attendance as a number. Example: 500. Return ONLY the number or numeric string.\"\n",
        "    ),\n",
        "    \"extract_exhibitor_amount\": (\n",
        "        \"Extract ONLY the expected number of exhibitors. Example: 12. Return ONLY the number or numeric string.\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "class NearestDoorClient:\n",
        "    def __init__(self, smartypants, lookup_engine, ollama,  client_id=CLIENT_ID, api_base=API_BASE):\n",
        "        self.client_id = client_id\n",
        "        self.api_base = api_base\n",
        "        self.ollama = ollama\n",
        "        self.lookup_engine = lookup_engine\n",
        "\n",
        "        self.last_heartbeat = 0\n",
        "\n",
        "\n",
        "        self.smartypants = smartypants\n",
        "\n",
        "    def _api_get(self, endpoint, params=None):\n",
        "        try:\n",
        "            print(f\"üì° GET ‚Üí {endpoint}\")\n",
        "            response = requests.get(f\"{self.api_base}{endpoint}\", params=params or {}, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            return response\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"‚ùå GET failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _api_post(self, endpoint, data):\n",
        "        try:\n",
        "            print(f\"üì° POST ‚Üí {endpoint}\")\n",
        "            response = requests.post(f\"{self.api_base}{endpoint}\", json=data, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            return response\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"‚ùå POST failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_task(self):\n",
        "        res = self._api_get(\"/next-task\", params={\"client_id\": self.client_id})\n",
        "        if res and res.status_code == 200:\n",
        "            task = res.json()\n",
        "            if isinstance(task, dict) and \"task_id\" in task:\n",
        "                return task\n",
        "            print(f\"‚ö†Ô∏è Invalid task structure received: {task}\")\n",
        "        return None\n",
        "\n",
        "    def send_heartbeat(self, current_task_id=None):\n",
        "        data = {\"client_id\": self.client_id}\n",
        "        if current_task_id:\n",
        "            data[\"task_id\"] = current_task_id\n",
        "        self._api_post(\"/heartbeat\", data)\n",
        "        print(\"ü´Ä Heartbeat sent.\")\n",
        "\n",
        "    async def handle_task(self, task):\n",
        "        task_id =task.get(\"task_id\")\n",
        "        task_type =task.get(\"task_type\")\n",
        "        if not task_id or not task_type:\n",
        "            print(\"‚ùå Invalid task format.\")\n",
        "            return\n",
        "\n",
        "        print(f\"‚ñ∂Ô∏è Handling task {task_type} (ID: {task_id})\")\n",
        "\n",
        "        result, summary, mainstring, images = False, None, None, None\n",
        "        aggregateplan, createdinfo, extractedfields, foundfields = None, None, None, None\n",
        "        print(task)\n",
        "        name = task['target'].get(\"name\")\n",
        "\n",
        "        slug = task['target'].get(\"slug\")\n",
        "        city = task['target'].get(\"city\")\n",
        "        state = task['target'].get(\"state\")\n",
        "\n",
        "        print(\"SHOP SLUG\",{city}, {state}, {slug})\n",
        "        website_url = task['target'].get(\"website\", None)\n",
        "\n",
        "        shop_type = task['target'].get(\"shop_type\")\n",
        "        aggregate = task['target'].get(\"aggregate\", \"\")\n",
        "        plan = task['target'].get(\"plan\", [])\n",
        "        fields = task['target'].get(\"fields\", [])\n",
        "\n",
        "        match task_type:\n",
        "            case \"search\":\n",
        "                result, mainstring, images = await self.lookup_engine.combined_search(name, city, state, shop_type, website_url)\n",
        "                result = str(result)\n",
        "            case \"aggregate\":\n",
        "\n",
        "                summarizer = ContentSummarizer(self.ollama, name, shop_type, city, state)\n",
        "                summary, result = summarizer.summarize_content(aggregate)\n",
        "\n",
        "\n",
        "            case \"createplan\":\n",
        "                print(\"creating plan\", aggregate)\n",
        "                result, aggregateplan = self.smartypants.create_plan(aggregate, name, shop_type, city, state)\n",
        "                print(\"plan\", aggregateplan)\n",
        "            case \"create\":\n",
        "                print(\"creating\", aggregate)\n",
        "                result, createdinfo = self.smartypants.create_sections(name, shop_type, aggregate, plan, city, state)\n",
        "                print(\"created\", createdinfo)\n",
        "            case \"find_available_fields\":\n",
        "                print(\"finding fields\", aggregate)\n",
        "                if aggregate != '':\n",
        "                  result, foundfields = self.smartypants.extract_available_fields(aggregate, name, shop_type, city, state)\n",
        "                else:\n",
        "\n",
        "                  print(\"NONE AGGREGATE FINDfIELDS\")\n",
        "                  result = False\n",
        "                  foundfields = []\n",
        "                print(\"fields\", foundfields)\n",
        "            case \"extract_fields_from_aggregate\":\n",
        "                print(\"extracting fields\", aggregate)\n",
        "                if aggregate != '':\n",
        "\n",
        "                  result, extractedfields = self.smartypants.extract_fields(aggregate, fields, name, shop_type, city, state)\n",
        "                else:\n",
        "                  print(\"NONE AGGREGATE EXTRACTFIELDS\")\n",
        "                  result = False\n",
        "                  extractedfields = {}\n",
        "                print(\"extracted\", extractedfields)\n",
        "            case _:\n",
        "                print(f\"‚ùå Unknown task type: {task_type}\")\n",
        "                return\n",
        "        if result:\n",
        "                print(f\"üì§ Submitting result for {task_type} ({task_id})\")\n",
        "                try:\n",
        "                    if task_type == 'search':\n",
        "                        res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"mainstring\": mainstring, \"client_id\": CLIENT_ID})\n",
        "\n",
        "                    if task_type == 'aggregate':\n",
        "                        if summary:\n",
        "                            res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"summary\": summary, \"client_id\": CLIENT_ID})\n",
        "                        else:\n",
        "                            print(\"nosummary\")\n",
        "                            res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"fail\", \"client_id\": CLIENT_ID})\n",
        "\n",
        "\n",
        "                    if task_type == 'createplan':\n",
        "                        res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"aggregateplan\": aggregateplan, \"client_id\": CLIENT_ID})\n",
        "                    if task_type == 'create':\n",
        "                        res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"createdinfo\":createdinfo, \"client_id\": CLIENT_ID})\n",
        "                    if task_type == 'find_available_fields':\n",
        "\n",
        "                        res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"foundfields\": foundfields, \"client_id\": CLIENT_ID})\n",
        "                    if task_type == 'extract_fields_from_aggregate':\n",
        "                        res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"extractedfields\": extractedfields, \"client_id\": CLIENT_ID})\n",
        "\n",
        "                    print(f\"Server responded: {res.status_code} - {res.text}\")\n",
        "                    if res:\n",
        "                      if res.status_code == 200:\n",
        "                          print(f\"‚úÖ Submitted: {task_type}\")\n",
        "                      else:\n",
        "                          print(f\"‚ùå Submit failed: {task_type} - {res.status_code}\")\n",
        "                    else:\n",
        "                      print(f\"‚ùå Submit failedd: {task_type} - {res.status_code}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Submit exception: {e}\")\n",
        "        else:\n",
        "\n",
        "            print(f\"Submit Failure {task_type}\")\n",
        "            res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"fail\", \"client_id\": CLIENT_ID})\n",
        "\n",
        "            if res:\n",
        "                if res.status_code == 200:\n",
        "                          print(f\"failed: {task_type}\")\n",
        "                else:\n",
        "                          print(f\"‚ùå couldnt failed: {task_type} - {res.status_code}\")\n",
        "            else:\n",
        "                print(\"NO RES\")\n",
        "\n",
        "    async def run(self):\n",
        "\n",
        "\n",
        "        try:\n",
        "            while True:\n",
        "                task = self.get_task()\n",
        "                if task:\n",
        "                    now = time.time()\n",
        "                    if now - self.last_heartbeat > HEARTBEAT_INTERVAL:\n",
        "                        self.send_heartbeat(task.get(\"task_id\"))\n",
        "                        self.last_heartbeat = now\n",
        "                    await self.handle_task(task)\n",
        "                else:\n",
        "                    print(\"‚è≥ No task available, sleeping...\")\n",
        "                    await asyncio.sleep(10)\n",
        "        finally:\n",
        "            print(\"main error\")\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    import asyncio\n",
        "    import nest_asyncio\n",
        "\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "    ollama = OllamaRunner()\n",
        "    smartypants = Smartypants(ollama)\n",
        "    lookup_engine = LookupEngine( ollama)  # Proper initialization\n",
        "\n",
        "    async def main():\n",
        "\n",
        "        client = NearestDoorClient(smartypants, lookup_engine, ollama)\n",
        "        await client.run()\n",
        "\n",
        "    try:\n",
        "\n",
        "        asyncio.run(main())\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nüõë Shutting down gracefully...\")\n",
        "        sys.exit(0)"
      ],
      "metadata": {
        "id": "g-2rqy_rMlo7",
        "outputId": "e31833e1-16b6-4c89-b94a-1c2b33333635",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "üì° GET ‚Üí /next-task\n",
            "üì° POST ‚Üí /heartbeat\n",
            "ü´Ä Heartbeat sent.\n",
            "‚ñ∂Ô∏è Handling task create (ID: 31066)\n",
            "{'task_id': 31066, 'task_type': 'create', 'object_type': 'shop', 'data': {}, 'target': {'id': 30539, 'name': 'Fort frederica national monument', 'city': 'St. Simmons', 'state': 'Georgia', 'website': None, 'slug': 'fort-frederica-national-monument', 'shop_type': 'Dog Park', 'aggregate': '', 'plan': ['history', 'article']}}\n",
            "SHOP SLUG {'St. Simmons'} {'Georgia'} {'fort-frederica-national-monument'}\n",
            "creating \n",
            "approved sections ['history', 'article']\n",
            "üß† Running Ollama: gemma3:4b\n"
          ]
        }
      ]
    }
  ]
}