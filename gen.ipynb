{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexrosulek/Cs50/blob/main/gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://ollama.ai/install.sh | sh\n",
        "\n",
        "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "!sudo apt-get update && sudo apt-get install -y cuda-drivers\n",
        "\n",
        "import os\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "!nohup ollama serve &\n",
        "!pip install pyOpenSSL==24.2.1\n",
        "\n",
        "\n",
        "# Pull Ollama Models\n",
        "!ollama pull gemma3:1b\n",
        "!ollama pull gemma3:4b\n",
        "# Install Packages\n",
        "!pip install -q ollama crawl4ai aiohttp pillow beautifulsoup4 wikipedia googlesearch-python playwright nest_asyncio\n",
        "!playwright install chromium\n",
        "!nohup ollama serve &\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-b9oppEb4cm2",
        "outputId": "1b98c4e2-f56c-4d60-afd9-ec56a6b4c283",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 13281    0 13281    0     0  41269      0 --:--:-- --:--:-- --:--:-- 41245\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to render group...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cuda-drivers is already the newest version (575.51.03-0ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 88 not upgraded.\n",
            "nohup: appending output to 'nohup.out'\n",
            "Requirement already satisfied: pyOpenSSL==24.2.1 in /usr/local/lib/python3.11/dist-packages (24.2.1)\n",
            "Requirement already satisfied: cryptography<44,>=41.0.5 in /usr/local/lib/python3.11/dist-packages (from pyOpenSSL==24.2.1) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<44,>=41.0.5->pyOpenSSL==24.2.1) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<44,>=41.0.5->pyOpenSSL==24.2.1) (2.22)\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mnohup: appending output to 'nohup.out'\n",
            "‚úÖ Environment Fully Ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "import random\n",
        "import time\n",
        "import aiohttp\n",
        "import shutil\n",
        "import os\n",
        "import re\n",
        "import base64\n",
        "import io\n",
        "import subprocess\n",
        "import requests\n",
        "import textwrap\n",
        "from PIL import Image\n",
        "from bs4 import BeautifulSoup\n",
        "import wikipedia\n",
        "\n",
        "from urllib.parse import urlparse\n",
        "from googlesearch import search\n",
        "\n",
        "# Crawl4AI Imports (Valid Ones)\n",
        "from crawl4ai import LLMConfig, LLMExtractionStrategy, CrawlerRunConfig\n",
        "from crawl4ai.deep_crawling.filters import FilterChain, ContentRelevanceFilter\n",
        "\n",
        "from crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n",
        "import json\n",
        "from crawl4ai.extraction_strategy import CosineStrategy\n",
        "\n",
        "from crawl4ai.async_configs import BrowserConfig\n",
        "\n",
        "from crawl4ai import AsyncWebCrawler\n",
        "from crawl4ai.async_configs import BrowserConfig\n",
        "\n",
        "\n",
        "print(\"‚úÖ Environment Fully Ready!\")\n",
        "\n",
        "API_BASE = \"https://www.nearestdoor.com\"  # Replace with actual server URL\n",
        "CLIENT_ID = \"client001\"\n",
        "HEARTBEAT_INTERVAL = 60  # seconds\n",
        "SHOP_FLOW_STATIC = [\n",
        "    \"search\", \"aggregate\", \"createplan\", \"create\",\n",
        "    \"find_available_fields\", \"extract_fields_from_aggregate\", \"fillintheshop\"\n",
        "]\n",
        "# --------------------------------------------------------------------------- #\n",
        "# üß†  LIGHT‚ÄëWEIGHT LOCAL LLM EXECUTION                                       #\n",
        "# --------------------------------------------------------------------------- #\n",
        "class OllamaRunner:\n",
        "    \"\"\"\n",
        "    `ollama run ‚Ä¶`\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, default_model: str = \"gemma3:1b\", default_timeout: int = 300):\n",
        "        self.default_model = default_model\n",
        "        self.default_timeout = default_timeout\n",
        "\n",
        "    def run(self, prompt: str, model: str | None = None, timeout: int | None = None) -> str:\n",
        "        model = model or self.default_model\n",
        "        timeout = timeout or self.default_timeout\n",
        "        print(f\"üß† Running Ollama: {model}\")\n",
        "\n",
        "        try:\n",
        "            proc = subprocess.run(\n",
        "                [\"ollama\", \"run\", model],\n",
        "                input=prompt.encode(\"utf-8\"),\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                timeout=timeout,\n",
        "            )\n",
        "            return proc.stdout.decode(\"utf-8\").strip()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Ollama execution failed: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# üåê  LOOK‚ÄëUP ENGINE                                                          #\n",
        "# --------------------------------------------------------------------------- #\n",
        "class LookupEngine:\n",
        "    \"\"\"\n",
        "    ‚Äì Validates every URL first\n",
        "    ‚Äì Google results exclude Yelp & Reddit and are content‚Äëchecked\n",
        "    ‚Äì Yelp & Reddit results are *also* content‚Äëchecked before ‚Äòbattling‚Äô\n",
        "    ‚Äì At most one Yelp URL & one Reddit URL are returned\n",
        "    ‚Äì Wikipedia returns at most one page (auto_suggest)\n",
        "    \"\"\"\n",
        "    def __init__(self, crawler, ollama_runner: OllamaRunner | None = None):\n",
        "        self.crawler = crawler\n",
        "        self.llm_config = LLMConfig(provider=\"ollama/gemma3:1b\")\n",
        "        self.ollama = ollama_runner or OllamaRunner()\n",
        "        self.crawler_manager = self.CrawlerManager()\n",
        "    async def initialize(self):\n",
        "        await self.crawler_manager.start()\n",
        "\n",
        "\n",
        "    # ---------------------  LOW‚ÄëLEVEL HELPERS  ----------------------------- #\n",
        "    class CrawlerManager:\n",
        "        def __init__(self):\n",
        "            self.crawler = None\n",
        "        async def start(self):\n",
        "            if self.crawler is None:\n",
        "                self.crawler = AsyncWebCrawler(config=BrowserConfig())\n",
        "                await self.crawler.__aenter__()\n",
        "\n",
        "        async def stop(self):\n",
        "            if self.crawler:\n",
        "                await self.crawler.__aexit__(None, None, None)\n",
        "                self.crawler = None\n",
        "\n",
        "        async def crawl(self, url: str, config: CrawlerRunConfig | None = None):\n",
        "            if not self.crawler:\n",
        "                raise RuntimeError(\"Crawler not started\")\n",
        "\n",
        "            url = self._normalize_and_validate_url(url)\n",
        "            if not url:\n",
        "                return None\n",
        "\n",
        "            try:\n",
        "                result = await self.crawler.arun(url=url, config=config or CrawlerRunConfig())\n",
        "                return result\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå crawl error for {url}: {e}\")\n",
        "                return None\n",
        "\n",
        "        def _normalize_and_validate_url(self, url: str) -> str | None:\n",
        "            try:\n",
        "                url = url.lower()\n",
        "                if not url.startswith((\"http://\", \"https://\")):\n",
        "\n",
        "                    url = \"https://\" + url\n",
        "\n",
        "                parsed = urlparse(url)\n",
        "                if parsed.scheme not in [\"http\", \"https\"] or not parsed.netloc or \".\" not in parsed.netloc or \" \" in parsed.netloc or \"/http\" in parsed.netloc:\n",
        "                    return None\n",
        "\n",
        "                return url\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå URL normalization failed: {e}\")\n",
        "                return None\n",
        "\n",
        "\n",
        "    async def _extract_snippet(self, url: str, max_chars: int, min_chars: int) -> str | None:\n",
        "        try:\n",
        "            resp = requests.get(\n",
        "                url, timeout=10,\n",
        "                headers={\"User-Agent\": \"Mozilla/5.0 (compatible; LookupEngine/1.0)\"}\n",
        "            )\n",
        "            if resp.status_code != 200:\n",
        "                return None\n",
        "\n",
        "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "            parts: list[str] = []\n",
        "\n",
        "            if soup.title:\n",
        "                parts.append(\"Meta Title: \"+ soup.title.string.strip())\n",
        "\n",
        "            desc = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
        "            if desc and desc.get(\"content\"):\n",
        "                parts.append(\"Meta Desc: \"+ desc[\"content\"].strip())\n",
        "\n",
        "            heads = [h.get_text(strip=True) for h in soup.find_all([\"h1\", \"h2\", \"h3\"])][:3]\n",
        "            parts.extend([\"Headers: \" + h for h in heads])\n",
        "\n",
        "\n",
        "            para = next((p.get_text(strip=True) for p in soup.find_all(\"p\") if len(p.get_text(strip=True)) > 50), \"\")\n",
        "            if para:\n",
        "                parts.append(\"Paragraph\" + para)\n",
        "            if len(\"\".join(parts)) < min_chars:\n",
        "                return None\n",
        "            return f\"Snippet From {url}:\\n\" + \"\\n\".join(parts)[:max_chars]\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Snippet extraction failed for {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    async def _basic_url_checker(self, url: str, shop_name: str, shop_type: str) -> bool:\n",
        "        try:\n",
        "\n",
        "            \"\"\"\n",
        "            Improved relevance check for a URL:\n",
        "            1. Uses Crawl4AI's ContentRelevanceFilter for semantic similarity.\n",
        "            2. Falls back to LLM prompt decision if semantic check is inconclusive.\n",
        "            \"\"\"\n",
        "\n",
        "            if not url:\n",
        "                return False\n",
        "\n",
        "            # Step 1: Semantic Query Based on Shop Type\n",
        "            semantic_query = self.get_semantic_query(shop_type, shop_name)\n",
        "            relevance_filter = ContentRelevanceFilter(\n",
        "            query=semantic_query,\n",
        "            threshold=0.4\n",
        "        )\n",
        "\n",
        "            crawl_config = CrawlerRunConfig(\n",
        "                deep_crawl_strategy=BFSDeepCrawlStrategy(\n",
        "                    max_depth=1,\n",
        "                    filter_chain=FilterChain([relevance_filter])\n",
        "                ),\n",
        "                word_count_threshold=20,\n",
        "                excluded_tags=[\"script\", \"style\", \"footer\", \"nav\"],\n",
        "                exclude_social_media_links=True,\n",
        "                exclude_external_links=True\n",
        "            )\n",
        "\n",
        "\n",
        "            url = self.crawler_manager._normalize_and_validate_url(url)\n",
        "\n",
        "\n",
        "\n",
        "            if not url:\n",
        "                return False\n",
        "            result = await self.crawler_manager.crawl(url, config=crawl_config)\n",
        "\n",
        "\n",
        "            if result.success and result.extracted_content:\n",
        "                    # Passed semantic filtering ‚Äî consider it a valid URL\n",
        "                print(f\"‚úÖ Semantic check passed for: {url}\")\n",
        "                return True\n",
        "\n",
        "\n",
        "            # Step 2: Fallback ‚Äî Quick Snippet and LLM Yes/No Decision\n",
        "            snippet = await self._extract_snippet(url, 500, 100)\n",
        "            if not snippet:\n",
        "                return False\n",
        "\n",
        "            prompt = (\n",
        "                f\"Is the following content about the {shop_type} named '{shop_name}'? \"\n",
        "                f\"Answer only `true` or `false` or `none`.\\n\\n{snippet}\"\n",
        "            )\n",
        "\n",
        "            decision = self.ollama.run(prompt)\n",
        "            if \"true\" in decision.lower():\n",
        "                print(f\"‚úÖ LLM confirmed relevance for: {url}\")\n",
        "                return True\n",
        "\n",
        "            print(f\"‚ö†Ô∏è URL deemed irrelevant: {url}\")\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"error in basicc checker\", {url}, {shop_name}, {shop_type}, e)\n",
        "            return False\n",
        "    @staticmethod\n",
        "    def get_semantic_query(shop_type, shop_name):\n",
        "        queries = {\n",
        "            \"church\": f\"{shop_name}, history, review, hours, muslim, phone, church, christian, church events, holiday schedules, mass times, sermons, church history, community programs, accessibility options, FAQs, donation methods, parking, contact information\",\n",
        "            \"plasma_center\": f\"{shop_name}, history, review, stocked brands, review, hours, phone, plasma, plasma donation requirements, compensation rates, donor reward, donor eligibility, contact details, operating hours, health guidelines, FAQ, appointment scheduling, safety procedures\",\n",
        "            \"thrift_store\": f\"{shop_name}, history, review, stocked brands, second hand,  review, hours, phone, thrift, store hours, donation guidelines, accepted items, discounts, sales events, store history, accessibility, contact info, volunteer programs, reviews\",\n",
        "            \"dog_park\": f\"{shop_name}, history, review, water, shade, agility equipment, park, review, hours, phone, dog, dog park hours, leash rules, pet-friendly areas, dog-friendly facilities, park amenities, accessibility options, entry fees, safety tips, events, pet policies, reviews\",\n",
        "        }\n",
        "        return queries.get(shop_type.lower(), \"business information, contact details, operating hours, reviews, FAQs, history\")\n",
        "\n",
        "    async def _get_site_content(\n",
        "\n",
        "            self,\n",
        "            url: str,\n",
        "            shop_name: str,\n",
        "            shop_type: str,\n",
        "\n",
        "            ) -> str | None:\n",
        "        try:\n",
        "        # Step 1: Semantic Filter Based on Shop Type\n",
        "            semantic_query = self.get_semantic_query(shop_type, shop_name)\n",
        "\n",
        "            cosine_strategy = CosineStrategy(\n",
        "                semantic_filter=semantic_query,\n",
        "                word_count_threshold=20,  # Filter short content blocks early\n",
        "                sim_threshold=0.35,       # Loose enough to capture diverse relevant content\n",
        "                top_k=8,                  # Top 5 most relevant content clusters\n",
        "                model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "                verbose=True\n",
        "            )\n",
        "\n",
        "            crawl_config = CrawlerRunConfig(\n",
        "                extraction_strategy=cosine_strategy,\n",
        "                excluded_tags=[\"script\", \"style\", \"header\", \"footer\", \"nav\"],\n",
        "                exclude_external_links=False,\n",
        "                exclude_social_media_links=False,\n",
        "                word_count_threshold=20,\n",
        "                process_iframes=True,\n",
        "                remove_overlay_elements=True\n",
        "            )\n",
        "\n",
        "            # ‚úÖ Use CrawlerManager properly\n",
        "            url = self.crawler_manager._normalize_and_validate_url(url)\n",
        "            if not url:\n",
        "                return None\n",
        "\n",
        "            result = await self.crawler_manager.crawl(url, config=crawl_config)\n",
        "\n",
        "            if not result or not result.success or not result.extracted_content:\n",
        "                print(f\"‚ùå Semantic content extraction failed for {url}\")\n",
        "                return None\n",
        "\n",
        "            # ‚úÖ Extract content safelytry:\n",
        "            try:\n",
        "                extracted_data = json.loads(result.extracted_content)\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"‚ùå Invalid JSON from {url}\")\n",
        "                return None\n",
        "\n",
        "            if not extracted_data:\n",
        "                print(f\"‚ö†Ô∏è No content extracted using semantic clustering for {url}\")\n",
        "                return None\n",
        "\n",
        "            content_chunks = [item[\"content\"] for item in extracted_data]\n",
        "            combined_content = \"\\n\\n\".join(content_chunks)\n",
        "\n",
        "            return combined_content\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to parse site content: {e}\")\n",
        "            return None\n",
        "    # ---------------------  LOOK‚ÄëUP ROUTINES  ------------------------------ #\n",
        "    async def wikipedia_lookup(self, name: str, city: str, shop_type: str) -> str | None:\n",
        "        try:\n",
        "            query = f\"{name} {city} {shop_type}\".strip()\n",
        "            print(f\"üìö Wikipedia lookup ‚Üí {query}\")\n",
        "            page = wikipedia.page(query, auto_suggest=True)\n",
        "            content = page.content\n",
        "\n",
        "            if len(content) <= 2000:\n",
        "                return content\n",
        "            chunks = [content[i:i + 500] for i in range(0, len(content), 500)]\n",
        "            # Fallback-safe middle extraction\n",
        "            if len(chunks) > 6:\n",
        "                middle = chunks[2:-2]  # Remove first and last 2 chunks\n",
        "                if not middle:\n",
        "                    middle = chunks  # If middle is empty, fallback to all chunks\n",
        "            else:\n",
        "                middle = chunks\n",
        "\n",
        "            # Intelligent selection\n",
        "            if len(middle) > 6:\n",
        "                selected = random.sample(middle, 6)  # Randomly select 6 if too many\n",
        "            else:\n",
        "                selected = middle  # Take all available if 6 or fewer\n",
        "            formatted_chunks = [f\"\\nWIKI CHUNK {idx + 1}:\\n{chunk}\" for idx, chunk in enumerate(selected)]\n",
        "\n",
        "            return f\"ALL EXTRACTED WIKIPEDIA SEARCH INFO FOR {name}:\\n\" + \"\\n\".join(formatted_chunks)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Wikipedia fetch failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    async def search_lookup(self,  name: str,  shop_type: str, query: str, placename: str, amount: int, isyelp: bool=False) -> str | None:\n",
        "        try:\n",
        "            print(f\"üîé Google search ‚Üí {query}\")\n",
        "\n",
        "            raw = list(search(query, amount))\n",
        "            if not isyelp:\n",
        "                candidate_urls = [u for u in raw if \"yelp\" not in u.lower() and \"reddit\" not in u.lower()]\n",
        "            else:\n",
        "\n",
        "                candidate_urls = [u for u in raw if \"yelp\" in u.lower() or \"reddit\" in u.lower()]\n",
        "            good_content = []\n",
        "            for i, url in enumerate(candidate_urls):\n",
        "\n",
        "\n",
        "                if not url:\n",
        "                    continue\n",
        "                if not await self._basic_url_checker(url, name, shop_type):\n",
        "                    continue\n",
        "\n",
        "                content = await self._get_site_content(url, name, shop_type)\n",
        "                if content:\n",
        "                    good_content.append(f\"\\n‚Üê¬†{placename} SEARCH¬†DATA¬†SITE {i} FROM: {url}\\n {content}\")\n",
        "\n",
        "            return f\"ALL EXTRACTED {placename} SEARCH DATA FOR {name}\\n\".join(good_content) if good_content else None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to parse search lookup results: {e}\")\n",
        "            return None\n",
        "    # ---------------------  PUBLIC ENTRY POINT  ---------------------------- #\n",
        "    async def combined_search(self, name: str, city: str, state: str, shop_type: str, website_url: str) -> tuple[bool, str | None, None]:\n",
        "        print(\"üåê Starting combined search‚Ä¶\")\n",
        "\n",
        "        Google_query = f\"{name} {city} {state} {shop_type} \"\n",
        "\n",
        "        Yelp_query = f\"{name} {city} {state} {shop_type} site: yelp.com \"\n",
        "\n",
        "        Reddit_query = f\"{name} {city} {state} {shop_type} site: reddit.com \"\n",
        "        if website_url:\n",
        "            Official_query = f\"{name} site: {website_url} \"\n",
        "\n",
        "        g_res = await self.search_lookup(name, shop_type, Google_query, \"Google\", 10, False)\n",
        "\n",
        "        y_res = await self.search_lookup(name, shop_type, Yelp_query, \"Yelp\", 5, True)\n",
        "\n",
        "        r_res = await self.search_lookup(name, shop_type, Reddit_query, \"Reddit\", 5, True)\n",
        "        w_res = await self.wikipedia_lookup(name, city, shop_type)\n",
        "        o_res = None\n",
        "        if website_url:\n",
        "            o_res = await self.search_lookup(name, shop_type, Official_query, f\"Official Website of {name}\", 5, True)\n",
        "        main = \"\"\n",
        "        if y_res:\n",
        "            main += y_res\n",
        "        if r_res:\n",
        "            main += r_res\n",
        "        if g_res:\n",
        "            main += g_res\n",
        "        if w_res:\n",
        "            main += w_res\n",
        "        if o_res:\n",
        "            main += o_res\n",
        "        if len(main) < 500:\n",
        "            print(\"‚ùå Not enough content gathered.\")\n",
        "            return False, None, None\n",
        "\n",
        "        print(\"‚úÖ Combined search complete.\")\n",
        "        return True, main, None\n",
        "# --------------------------------------------------------------------------- #\n",
        "# üì¶  HIGH‚ÄëLEVEL CONTENT¬†SUMMARIZER                                          #\n",
        "# --------------------------------------------------------------------------- #\n",
        "class ContentSummarizer:\n",
        "    \"\"\"\n",
        "    Reduce a large blob of text about a specific business down to ‚â§‚ÄØmax_final_chars\n",
        "    while preserving high‚Äëvalue facts. Uses multi-stage LLM summarisation with\n",
        "    chunk filtering and escalation if necessary.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        ollama_runner: OllamaRunner,\n",
        "        shop_name: str,\n",
        "        shop_type: str,\n",
        "        city: str | None = None,\n",
        "        state: str | None = None,\n",
        "        max_final_chars: int = 6000,\n",
        "        min_final_chars: int = 500,\n",
        "    ):\n",
        "        self.ollama = ollama_runner\n",
        "        self.shop_name = shop_name\n",
        "        self.shop_type = shop_type\n",
        "        self.city = city or \"\"\n",
        "        self.state = state or \"\"\n",
        "        self.max_final_chars = max_final_chars\n",
        "        self.min_final_chars = min_final_chars\n",
        "\n",
        "    # ----------------- Internal Helpers ----------------- #\n",
        "    def _clean_raw_content(self, content: str) -> str:\n",
        "        lines = content.splitlines()\n",
        "        cleaned, seen = [], set()\n",
        "        NOISE = [\n",
        "            \"cookie policy\", \"all rights reserved\", \"subscribe\", \"advertisement\",\n",
        "            \"accept cookies\", \"privacy policy\", \"terms of service\", \"sign in\", \"cookie\"\n",
        "        ]\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            lo = line.lower()\n",
        "            if len(line) < 30 or lo in seen:\n",
        "                continue\n",
        "            if any(noise in lo for noise in NOISE):\n",
        "                continue\n",
        "            seen.add(lo)\n",
        "            cleaned.append(line)\n",
        "        return \"\\n\".join(cleaned)\n",
        "\n",
        "    def _chunk_text(self, text: str, chunk_size: int) -> list[str]:\n",
        "        return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "    def _filter_chunks(self, chunks: list[str], model: str = \"gemma3:1b\") -> list[str]:\n",
        "        try:\n",
        "            good = []\n",
        "            for chunk in chunks:\n",
        "                prompt = (\n",
        "                    f\"Is the following content useful for creating a profile for the {self.shop_type} \"\n",
        "                    f\"'{self.shop_name}'? Only reply 'true' or 'false'.\\n\\n{chunk}\"\n",
        "                )\n",
        "                decision = self.ollama.run(prompt, model=model).strip().lower()\n",
        "                if \"true\" in decision:\n",
        "                    good.append(chunk)\n",
        "\n",
        "            return good\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to filter chunk: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _build_prompt(self, text_chunk: str) -> str:\n",
        "        return (\n",
        "            f\"You are creating a SHORT high-quality summary for \"\n",
        "            f\"{self.shop_type} **{self.shop_name}** \"\n",
        "            f\"{'in ' + self.city if self.city else ''}{', ' + self.state if self.state else ''}.\\n\\n\"\n",
        "            f\"Keep ALL USEFUL DATA\"\n",
        "            \"KEEP SERVICES.\\n\"\n",
        "            f\"KEEP URLS ONLY WHEN IT IS THE LITERAL {self.shop_name}'s WEBSITE.\\n\"\n",
        "            f\"KEEP ALL GOOD INFO AND HISTORY, FACTS, INFO, Extract all usefull info, Example: \\n\"\n",
        "            f\"‚Ä¢ Keep Contact info (phone, email)\\n\"\n",
        "            f\"‚Ä¢ Keep Official website URLs only if directly related to {self.shop_name}\\n\"\n",
        "            f\"‚Ä¢ Keep Operating hours, holiday hours, KEEP ALL USEFUL ANY DATA\\n\"\n",
        "            f\"‚Ä¢ Keep Brands Price range, services, FAQs, key facts, reviews, history\\n\"\n",
        "            f\"‚Ä¢ Keep Reviews sentiment, pros/cons, unique selling points\\n\"\n",
        "            f\"‚Ä¢ Keep Awards, certifications, reviews, press articles\\n\\n\"\n",
        "            f\" remove unrelated info.\\n\"\n",
        "\n",
        "            f\"--- SOURCE TEXT START ---\\n{text_chunk}\\n--- SOURCE TEXT END ---\"\n",
        "\n",
        "        )\n",
        "\n",
        "    def _summarize_with_ollama(self, text: str, model: str) -> str:\n",
        "        try:\n",
        "\n",
        "            prompt = self._build_prompt(text)\n",
        "            return self.ollama.run(prompt, model=model)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to summarize w ollama: {e}\")\n",
        "            return None\n",
        "    def summarize_chunks(\n",
        "        self,\n",
        "        content: str,\n",
        "        chunk_size: int = 2000,\n",
        "        initial_model: str = \"gemma3:1b\",\n",
        "\n",
        "    ) -> str:\n",
        "        try:\n",
        "            chunks = self._chunk_text(content, chunk_size)\n",
        "            content = self._filter_chunks(chunks)\n",
        "            if not content:\n",
        "                content = chunks\n",
        "            filtered_content = ''.join(content)\n",
        "            if len(filtered_content) < self.min_final_chars:\n",
        "                return None\n",
        "            if len(filtered_content) < self.max_final_chars:\n",
        "                return filtered_content\n",
        "\n",
        "            summarized_chunks = []\n",
        "\n",
        "            for idx, chunk in enumerate(chunks, start=1):\n",
        "                print(f\"üìö Summarizing chunk {idx}/{len(chunks)}...\")\n",
        "                summary = self._summarize_with_ollama(chunk, model=initial_model)\n",
        "\n",
        "                if not summary or len(summary) < 50:\n",
        "                    print(f\"‚ö†Ô∏è Failed to summarize chunk {idx}, keeping raw content.\")\n",
        "                    summary = chunk  # Fallback to raw content if summary failed\n",
        "\n",
        "                summarized_chunks.append(f\"### CHUNK {idx} SUMMARY:\\n{summary}\")\n",
        "\n",
        "                # Early exit: check if adding all remaining raw chunks without summarizing fits within limit\n",
        "                combined_so_far = \"\\n\\n\".join(summarized_chunks)\n",
        "                remaining_raw = \"\".join(chunks[idx:])  # Remaining chunks after current one\n",
        "\n",
        "                if len(combined_so_far) + len(remaining_raw) <= self.max_final_chars:\n",
        "                    print(f\"‚úÖ Early exit: current summary + remaining raw fits within limit. Skipping further summarization.\")\n",
        "                    for r_idx, remaining_chunk in enumerate(chunks[idx:], start=idx + 1):\n",
        "                        summarized_chunks.append(f\"### CHUNK {r_idx} (Raw):\\n{remaining_chunk}\")\n",
        "                    break\n",
        "\n",
        "            combined_summary = \"\\n\\n\".join(summarized_chunks)\n",
        "            if len(combined_summary) < self.min_final_chars:\n",
        "                return None\n",
        "            # Final trim if absolutely necessary\n",
        "            if len(combined_summary) > self.max_final_chars:\n",
        "                print(\"‚ö†Ô∏è Final combined summary exceeds character limit. Trimming result.\")\n",
        "                return combined_summary[:self.max_final_chars]\n",
        "\n",
        "            return combined_summary\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to sumarrize chunks: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    def summarize_content(self, raw_content: str) -> str:\n",
        "        print(\"üßπ Cleaning raw content...\")\n",
        "        cleaned = self._clean_raw_content(raw_content)\n",
        "        if len(cleaned) <= self.max_final_chars:\n",
        "            print(\"‚úÖ Cleaned content fits within final character limit.\")\n",
        "            return cleaned\n",
        "\n",
        "        final_summary = self.summarize_chunks(\n",
        "        content=cleaned,           # The large raw text content you want to reduce\n",
        "        chunk_size=1500,                # Size of each chunk before summarizing\n",
        "        initial_model=\"gemma3:1b\",      # Start with the lightweight model\n",
        "\n",
        "    )\n",
        "        if final_summary is None:\n",
        "            print(\"‚ùå Summarization failed. Returning cleaned content instead.\")\n",
        "            return cleaned[:self.max_final_chars]\n",
        "\n",
        "        print(\"üéâ Final summarization complete.\")\n",
        "        return final_summary\n",
        "\n",
        "import re\n",
        "import json\n",
        "\n",
        "class Smartypants:\n",
        "    def __init__(self, ollama_runner: OllamaRunner):\n",
        "        self.ollama = ollama_runner\n",
        "\n",
        "    def _run4(self, prompt: str) -> str:\n",
        "        return self.ollama.run(prompt, model=\"gemma3:4b\")\n",
        "    def _run1(self, prompt: str) -> str:\n",
        "        return self.ollama.run(prompt, model=\"gemma3:1b\")\n",
        "\n",
        "    # ------------ PLAN ------------ #\n",
        "    def create_plan(self, aggregate: str,shop_name: str, shop_type: str,  city: str, state: str) -> tuple[bool, list[str]]:\n",
        "        prompt = (\n",
        "            \"You have an aggregated summary about a \"\n",
        "\n",
        "            f\"place called {shop_name} {shop_type} in {city}, {state}\\n\\n\"\n",
        "            f\"{aggregate}\\n\\n\"\n",
        "            \"Which content sections can confidently be generated based on this?\\n\"\n",
        "            \"Options: article, faq, history.\\n\"\n",
        "            \"Reply with a correct python comma-separated list of available sections to write about, nothing else, Options: article, faq, history.\"\n",
        "        )\n",
        "        try:\n",
        "            count = 0\n",
        "            resp = self._run4(prompt).lower()\n",
        "            valid = {\"article\", \"faq\", \"history\"}\n",
        "            for s in valid:\n",
        "                if s not in resp:\n",
        "                    count +=1\n",
        "            if count == 3:\n",
        "                return False\n",
        "\n",
        "            return True, [s for s in valid if s in resp]\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå create_plan error: {e}\")\n",
        "            return False, []\n",
        "\n",
        "    def check_aggregate_quality(self, shop_name: str, aggregate: str,shop_type: str,  city: str, state: str) -> bool:\n",
        "        prompt = (\n",
        "            f\"Check if this content contains usefull info about {shop_name} {shop_type} in {city}, {state}.\\n\\n{aggregate}\\n\\n\"\n",
        "            \"Reply only `true` or `false`.\"\n",
        "        )\n",
        "        try:\n",
        "            return \"true\" in self._run1(prompt).lower()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå quality check error: {e}\")\n",
        "            return False\n",
        "\n",
        "    # ------------ SECTION VALIDATION / FIX ------------ #\n",
        "    def validate_section_html(self, shop_name: str, section: str, text: str) -> bool:\n",
        "        prompt = (\n",
        "            f\"Validate the following text for section '{section}'. It is supposed to be about '{shop_name}'.\\n\\n{text}\\n\\n\"\n",
        "            \"Rules:\\n- No HTML.\\n- No irrelevant info. Is it useful and no format or weird characters? Nothing Else, Nothing before or after our content\\n\"\n",
        "            \"- Only factual, structured, and clear content.\\n- Reply `true` or `false` only.\"\n",
        "        )\n",
        "        return \"true\" in self._run1(prompt).lower()\n",
        "\n",
        "    def fix_section_html(self, shop_name: str, section: str, text: str) -> str | None:\n",
        "        prompt = (\n",
        "            f\"Clean and fix this section '{section}'s format. It is about '{shop_name}'.\\n\\n{text}\\n\\n\"\n",
        "            \"Rules:\\n- No HTML, Is it useful and and no format or weird characters? asterisks, or irrelevant info.\\n\"\n",
        "            \"Return only the final cleaned text for consumers on nearestdoor.com to read, no junk, no explanations, nothing else, nothing before or after our content. Only return the corrected text.\"\n",
        "        )\n",
        "        return self._run4(prompt).strip()\n",
        "\n",
        "    # ------------ JSON & FIELD EXTRACTION ------------ #\n",
        "    def extract_clean_json_structure(self, text: str) -> dict | None:\n",
        "        try:\n",
        "            match = re.search(r\"\\{.*?\\}\", text.strip(), re.DOTALL)\n",
        "            if not match:\n",
        "                return None\n",
        "\n",
        "            match_text = match.group(0).lower()\n",
        "            exclusion_keywords = ['n/a', 'n-a', 'none', 'false', 'na', 'cant', 'not', 'found', 'unable', '{{', '()', 'unavailable']\n",
        "\n",
        "            if any(bad in match_text for bad in exclusion_keywords):\n",
        "                return None\n",
        "\n",
        "            json_ready = match.group(0).replace(\"'\", '\"')\n",
        "            return json.loads(json_ready)\n",
        "\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "\n",
        "    def extract_available_fields(self, aggregate: str,shop_name: str, shop_type: str,  city: str, state: str) -> tuple[bool, list[str]]:\n",
        "        try:\n",
        "            field_list = list(FIELD_EXTRACTORS.keys())\n",
        "            field_str = ', '.join(field_list)\n",
        "            prompt = (\n",
        "                f\"Analyze the content:\\n\\n{aggregate}\\n\\n\"\n",
        "                f\"Whos data we want {shop_type}, {shop_name}. \"\n",
        "                f\"Which of these fields can be confidently extracted?\\n{field_str}\\n\"\n",
        "                \"Reply ONLY with the field keys that are present in the text, as ONLY the correct format requested. no junk. Nothing else. \"\n",
        "                \"If none, reply exactly 'FALSE'.\"\n",
        "            )\n",
        "            response = self._run4(prompt).strip().lower()\n",
        "            if \"false\" in response:\n",
        "                return True, []\n",
        "            detected = [field for field in field_list if field.lower() in response]\n",
        "            return True, detected\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå extract_available_fields error: {e}\")\n",
        "            return False, []\n",
        "    def validate_extracted_field_value(self, field: str, value) -> bool:\n",
        "        \"\"\"\n",
        "        Validate the extracted field value using LLM and manual schema checks.\n",
        "\n",
        "        - If it's a JSON list, remove invalid entries.\n",
        "        - If it's invalid after cleaning, return False.\n",
        "        \"\"\"\n",
        "        # LLM-Based Validation Prompt\n",
        "        prompt = (\n",
        "            f\"Validate this extracted value for field '{field}':\\n\\n{value}\\n\\n\"\n",
        "            \"Is this a valid and correct value and format for the specified field requested? Is it weird for the field or contain none values? Reply ONLY `true` or `false`.\"\n",
        "        )\n",
        "        llm_decision = \"true\" in self._run1(prompt).lower()\n",
        "\n",
        "        # If LLM says it's invalid, fail immediately\n",
        "        if not llm_decision:\n",
        "            print(f\"‚ùå LLM validation failed for field '{field}'.\")\n",
        "            return False\n",
        "\n",
        "\n",
        "\n",
        "        # Final check for singular values\n",
        "        return True\n",
        "\n",
        "    def extract_fields(\n",
        "        self, aggregate: str, available_fields: list[str],\n",
        "        shop_name: str, shop_type: str, city: str, state: str\n",
        "    ) -> tuple[bool, dict]:\n",
        "        extracted = {}\n",
        "        try:\n",
        "            for field in available_fields:\n",
        "                try:\n",
        "                    prompt = (\n",
        "                        f\"{FIELD_EXTRACTORS[field]}\\n\\nContent:\\n{aggregate}\\n\\n\"\n",
        "                        f\"Return ONLY the valid structure requested. Respond with nothing but the correct format requested. \"\n",
        "                        f\"If none, say 'none'. Nothing else, nothing before or after our content. Data about {shop_type}, {shop_name}.\"\n",
        "                    )\n",
        "                    raw_value = self._run4(prompt).strip()\n",
        "                    final_value = self.extract_clean_json_structure(raw_value) or raw_value\n",
        "\n",
        "                    if self.validate_extracted_field_value(field, final_value):\n",
        "\n",
        "                        extracted[field] = final_value\n",
        "\n",
        "                except Exception as inner_e:\n",
        "                    print(f\"‚ö†Ô∏è Field extraction failed for '{field}': {inner_e}\")\n",
        "                    continue\n",
        "\n",
        "            return True, extracted if extracted else {}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è extract_fields failed: {e}\")\n",
        "            return False, {}\n",
        "\n",
        "\n",
        "    # ------------ SECTION GENERATION ------------ #\n",
        "    def create_sections(\n",
        "        self, shop_name: str, shop_type: str, aggregate: str,\n",
        "        approved_sections: list[str], city: str | None = None, state: str | None = None\n",
        "    ) -> tuple[bool, dict]:\n",
        "        def _generate(section: str, prompt: str) -> str | None:\n",
        "            raw = self._run4(prompt).strip()\n",
        "            if self.validate_section_html(shop_name, section, raw):\n",
        "                return raw\n",
        "            fixed = self.fix_section_html(shop_name, section, raw)\n",
        "            return fixed if fixed and self.validate_section_html(shop_name, section, fixed) else None\n",
        "\n",
        "        location = f\"in {city}, {state}\" if city or state else \"\"\n",
        "        base_instr = (\n",
        "            f\"You are writing for nearestdoor.com about our listings, about the {shop_type} '{shop_name}' {location}. \"\n",
        "            \"You will get a summary and write useful information according to your assignment which consumers will read on the nearestdoor.com website about this place, no bad info or bad formatting\"\n",
        "            \"Be factual, SEO-friendly, help the users learn use this place and learn about it. no unrelated info, no HTML or asterisks. nothing else, nothing before or after our content. DO NOT USE * \"\n",
        "        )\n",
        "\n",
        "        sections = {}\n",
        "        try:\n",
        "            if \"article\" in approved_sections:\n",
        "                prompt = f\"{base_instr}\\n\\nContent:\\n{aggregate}\\n\\nAssignment: Write a detailed article. Write an article about {shop_name} for nearestdoor.com.\"\n",
        "                result = _generate(\"article\", prompt)\n",
        "                if result:\n",
        "                    sections[\"article\"] = result\n",
        "\n",
        "            if \"faq\" in approved_sections:\n",
        "                prompt = f\"{base_instr}\\n\\nContent:\\n{aggregate}\\n\\nAssignment: Write a detailed FAQ. Write an FAQ about {shop_name} for nearestdoor.com.\"\n",
        "                result = _generate(\"faq\", prompt)\n",
        "                if result:\n",
        "                    sections[\"faq\"] = result\n",
        "\n",
        "            if \"history\" in approved_sections:\n",
        "                prompt = f\"{base_instr}\\n\\nContent:\\n{aggregate}\\n\\nAssignment: Write the history section about {shop_name} for nearestdoor.com.\"\n",
        "                result = _generate(\"history\", prompt)\n",
        "                if result:\n",
        "                    sections[\"history\"] = result\n",
        "\n",
        "            return True, sections\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå create_sections error: {e}\")\n",
        "            return False, {}\n",
        "\n",
        "    # ------------ FULL WORKFLOW ------------ #\n",
        "    def process(\n",
        "        self, shop_name: str, shop_type: str, aggregate: str,\n",
        "        city: str | None = None, state: str | None = None\n",
        "    ) -> dict:\n",
        "        result = {\"plan\": [], \"sections\": {}, \"fields\": {}}\n",
        "\n",
        "        if not self.check_aggregate_quality(shop_name, aggregate, shop_type, city, state):\n",
        "            print(\"‚ùå Aggregate failed quality check.\")\n",
        "            return None\n",
        "\n",
        "        ok, plan = self.create_plan(aggregate, shop_name, shop_type, city, state)\n",
        "        if not ok or not plan:\n",
        "            print(\"‚ùå No sections can be created.\")\n",
        "            return None\n",
        "        result[\"plan\"] = plan\n",
        "\n",
        "        ok, sections = self.create_sections(shop_name, shop_type, aggregate, city, state)\n",
        "        if ok:\n",
        "            result[\"sections\"] = sections\n",
        "\n",
        "        ok, available = self.extract_available_fields(aggregate, shop_name, shop_type, city, state)\n",
        "        if ok and available:\n",
        "            ok, fields = self.extract_fields(aggregate, available,shop_name, shop_type,city, state)\n",
        "            if ok:\n",
        "                result[\"fields\"] = fields\n",
        "\n",
        "        return result\n",
        "\n",
        "import re\n",
        "import json\n",
        "\n",
        "def is_non_empty_string(value) -> bool:\n",
        "    return isinstance(value, str) and len(value.strip()) > 0\n",
        "\n",
        "def is_valid_json(value) -> bool:\n",
        "    try:\n",
        "        json.loads(value)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def is_valid_phone(value) -> bool:\n",
        "    return bool(re.fullmatch(r\"\\d{3}-\\d{3}-\\d{4}\", value.strip()))\n",
        "\n",
        "def is_valid_email(value) -> bool:\n",
        "    return bool(re.fullmatch(r\"[^@\\s]+@[^@\\s]+\\.[a-zA-Z0-9]+\", value.strip()))\n",
        "\n",
        "def is_valid_url(value) -> bool:\n",
        "    return isinstance(value, str) and value.strip().startswith(\"http\")\n",
        "\n",
        "def is_valid_dict(value) -> bool:\n",
        "    try:\n",
        "        return isinstance(json.loads(value), dict)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def is_valid_list(value) -> bool:\n",
        "    try:\n",
        "        return isinstance(json.loads(value), list)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def is_positive_integer_or_string(value) -> bool:\n",
        "    try:\n",
        "        return int(str(value).strip()) > 0\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "FIELD_VALIDATORS = {\n",
        "    # Contact Info\n",
        "    \"extract_phone\": is_valid_phone,\n",
        "    \"extract_email\": is_valid_email,\n",
        "    \"extract_website\": is_valid_url,\n",
        "\n",
        "    # Structured Fields\n",
        "    \"extract_categories\": is_valid_list,\n",
        "    \"extract_operating_hours\": is_valid_dict,\n",
        "    \"extract_holiday_hours\": is_valid_dict,\n",
        "    \"extract_delivery_services\": is_valid_list,\n",
        "    \"extract_social_media\": is_valid_dict,\n",
        "    \"extract_stocked_brands\": is_valid_list,\n",
        "    \"extract_inventory_categories\": is_valid_dict,\n",
        "    \"extract_customer_reviews\": is_valid_list,\n",
        "\n",
        "    # Event / Misc\n",
        "    \"extract_admission\": is_non_empty_string,\n",
        "    \"extract_date_available\": is_non_empty_string,\n",
        "    \"extract_attendance_amount\": is_positive_integer_or_string,\n",
        "    \"extract_exhibitor_amount\": is_positive_integer_or_string,\n",
        "}\n",
        "FIELD_EXTRACTORS = {\n",
        "    # Contact Information\n",
        "    \"extract_phone\": (\n",
        "        \"Extract the phone number in this format: 727-237-2132. \"\n",
        "        \"Return ONLY the number, no quotes, no text, no comments, no markup.\"\n",
        "    ),\n",
        "    \"extract_email\": (\n",
        "        \"Extract the email address. Example: example@mail.com. \"\n",
        "        \"Return ONLY the email address, no quotes, no text, no extras.\"\n",
        "    ),\n",
        "    \"extract_website\": (\n",
        "        \"Extract the official website URL. Example: https://website.com. \"\n",
        "        \"Return ONLY the URL, no quotes, no text, no markup.\"\n",
        "    ),\n",
        "\n",
        "    # JSON / Structured Fields\n",
        "    \"extract_categories\": (\n",
        "        \"Extract the product/service categories in JSON list format. \"\n",
        "        \"Example: ['Thrift Store', 'Charity']. Return ONLY the JSON array.\"\n",
        "    ),\n",
        "    \"extract_operating_hours\": (\n",
        "        \"Extract weekly operating hours in JSON dictionary format. \"\n",
        "        \"Example: {'monday': '9:00 AM - 5:00 PM', 'sunday': 'Closed'}. \"\n",
        "        \"Return ONLY the JSON object.\"\n",
        "    ),\n",
        "    \"extract_holiday_hours\": (\n",
        "        \"Extract holiday-specific hours in JSON dictionary format. \"\n",
        "        \"Example: {'2024-12-25': 'Closed', '2024-12-31': '10:00 AM - 4:00 PM'}. \"\n",
        "        \"Return ONLY the JSON object.\"\n",
        "    ),\n",
        "    \"extract_delivery_services\": (\n",
        "        \"Extract available delivery services in JSON list format. \"\n",
        "        \"Example: ['Uber Eats', 'Self Delivery']. Return ONLY the JSON array.\"\n",
        "    ),\n",
        "    \"extract_social_media\": (\n",
        "        \"Extract social media links in JSON dictionary format. \"\n",
        "        \"Example: {'facebook': 'https://facebook.com/example', 'instagram': 'https://instagram.com/example'}. \"\n",
        "        \"Return ONLY the JSON object.\"\n",
        "    ),\n",
        "    \"extract_stocked_brands\": (\n",
        "        \"Extract stocked brands in JSON list format. \"\n",
        "        \"Example: ['Nike', 'Adidas']. Return ONLY the JSON array.\"\n",
        "    ),\n",
        "    \"extract_inventory_categories\": (\n",
        "        \"Extract inventory categories in JSON dictionary format. \"\n",
        "        \"Example: {'Apparel': ['Shirts', 'Hoodies']}. Return ONLY the JSON object.\"\n",
        "    ),\n",
        "    \"extract_customer_reviews\": (\n",
        "        \"Extract customer reviews in JSON list format. \"\n",
        "        \"Example: [{'user': 'John', 'comment': 'Great store!', 'rating': 5}]. \"\n",
        "        \"Return ONLY the JSON array.\"\n",
        "    ),\n",
        "\n",
        "    # Event / Scheduling\n",
        "    \"extract_admission\": (\n",
        "        \"Extract the admission cost or entry fee. Return ONLY the plain text, no prefixes or suffixes.\"\n",
        "    ),\n",
        "    \"extract_date_available\": (\n",
        "        \"Extract the available date range or date description. \"\n",
        "        \"Example: 'Available from May 1st to June 30th'. Return ONLY the plain text.\"\n",
        "    ),\n",
        "    \"extract_attendance_amount\": (\n",
        "        \"Extract expected attendance as a number. Example: 500. Return ONLY the number or numeric string.\"\n",
        "    ),\n",
        "    \"extract_exhibitor_amount\": (\n",
        "        \"Extract expected number of exhibitors. Example: 12. Return ONLY the number or numeric string.\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "class NearestDoorClient:\n",
        "    def __init__(self, crawler_manager, client_id=CLIENT_ID, api_base=API_BASE):\n",
        "        self.client_id = client_id\n",
        "        self.api_base = api_base\n",
        "        self.crawler_mgr = crawler_manager\n",
        "\n",
        "        self.ollama = OllamaRunner()\n",
        "        self.lookup_engine = LookupEngine(self.crawler_mgr, self.ollama)\n",
        "\n",
        "        self.last_heartbeat = 0\n",
        "\n",
        "        # Inject dependencies\n",
        "\n",
        "        self.smartypants = Smartypants(self.ollama)\n",
        "\n",
        "    def _api_get(self, endpoint, params=None):\n",
        "        try:\n",
        "            print(f\"üì° GET ‚Üí {endpoint}\")\n",
        "            response = requests.get(f\"{self.api_base}{endpoint}\", params=params or {}, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            return response\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"‚ùå GET failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _api_post(self, endpoint, data):\n",
        "        try:\n",
        "            print(f\"üì° POST ‚Üí {endpoint}\")\n",
        "            response = requests.post(f\"{self.api_base}{endpoint}\", json=data, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            return response\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"‚ùå POST failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_task(self):\n",
        "        res = self._api_get(\"/next-task\", params={\"client_id\": self.client_id})\n",
        "        if res and res.status_code == 200:\n",
        "            task = res.json()\n",
        "            if isinstance(task, dict) and \"task_id\" in task:\n",
        "                return task\n",
        "            print(f\"‚ö†Ô∏è Invalid task structure received: {task}\")\n",
        "        return None\n",
        "\n",
        "    def send_heartbeat(self, current_task_id=None):\n",
        "        data = {\"client_id\": self.client_id}\n",
        "        if current_task_id:\n",
        "            data[\"task_id\"] = current_task_id\n",
        "        self._api_post(\"/heartbeat\", data)\n",
        "        print(\"ü´Ä Heartbeat sent.\")\n",
        "\n",
        "    async def handle_task(self, task):\n",
        "        task_id =task.get(\"task_id\")\n",
        "        task_type =task.get(\"task_type\")\n",
        "        if not task_id or not task_type:\n",
        "            print(\"‚ùå Invalid task format.\")\n",
        "            return\n",
        "\n",
        "        print(f\"‚ñ∂Ô∏è Handling task {task_type} (ID: {task_id})\")\n",
        "\n",
        "        result, summary, mainstring, images = False, None, None, None\n",
        "        aggregateplan, createdinfo, extractedfields, foundfields = None, None, None, None\n",
        "        print(task)\n",
        "        name = task['target'].get(\"name\")\n",
        "        city = task['target'].get(\"city\")\n",
        "        state = task['target'].get(\"state\")\n",
        "        website_url = task['target'].get(\"website\", None)\n",
        "\n",
        "        shop_type = task['target'].get(\"shop_type\")\n",
        "        aggregate = task['target'].get(\"aggregate\", \"\")\n",
        "        plan = task['target'].get(\"plan\", [])\n",
        "        fields = task['target'].get(\"fields\", [])\n",
        "\n",
        "        match task_type:\n",
        "            case \"search\":\n",
        "                result, mainstring, images = await self.lookup_engine.combined_search(name, city, state, shop_type, website_url)\n",
        "                result = str(result)\n",
        "            case \"aggregate\":\n",
        "                summarizer = ContentSummarizer(self.ollama, name, shop_type, city, state)\n",
        "                summary = summarizer.summarize_content(aggregate)\n",
        "                result = bool(summary)\n",
        "\n",
        "            case \"createplan\":\n",
        "                result, aggregateplan = self.smartypants.create_plan(aggregate, name, shop_type, city, state)\n",
        "\n",
        "            case \"create\":\n",
        "                result, createdinfo = self.smartypants.create_sections(name, shop_type, aggregate, plan, city, state)\n",
        "\n",
        "            case \"find_available_fields\":\n",
        "                result, foundfields = self.smartypants.extract_available_fields(aggregate, name, shop_type, city, state)\n",
        "\n",
        "            case \"extract_fields_from_aggregate\":\n",
        "                result, extractedfields = self.smartypants.extract_fields(aggregate, fields, name, shop_type, city, state)\n",
        "\n",
        "            case _:\n",
        "                print(f\"‚ùå Unknown task type: {task_type}\")\n",
        "                return\n",
        "        if result:\n",
        "                print(f\"üì§ Submitting result for {task_type} ({task_id})\")\n",
        "                try:\n",
        "                    if task_type == 'search':\n",
        "                        res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"mainstring\": mainstring, \"client_id\": CLIENT_ID})\n",
        "\n",
        "                    if task_type == 'aggregate':\n",
        "                        if summary:\n",
        "                            res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"summary\": summary, \"client_id\": CLIENT_ID})\n",
        "                        else:\n",
        "                            print(\"nosummary\")\n",
        "                            res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"fail\", \"client_id\": CLIENT_ID})\n",
        "\n",
        "\n",
        "                    if task_type == 'createplan':\n",
        "                        res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"aggregateplan\": aggregateplan, \"client_id\": CLIENT_ID})\n",
        "                    if task_type == 'create':\n",
        "                        res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"createdinfo\":createdinfo, \"client_id\": CLIENT_ID})\n",
        "                    if task_type == 'find_available_fields':\n",
        "                        res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"foundfields\": foundfields, \"client_id\": CLIENT_ID})\n",
        "                    if task_type == 'extract_fields_from_aggregate':\n",
        "                        res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"extractedfields\": extractedfields, \"client_id\": CLIENT_ID})\n",
        "\n",
        "                    print(f\"Server responded: {res.status_code} - {res.text}\")\n",
        "                    if res.status_code == 200:\n",
        "                        print(f\"‚úÖ Submitted: {task_type}\")\n",
        "                    else:\n",
        "                        print(f\"‚ùå Submit failed: {task_type} - {res.status_code}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Submit exception: {e}\")\n",
        "        else:\n",
        "\n",
        "            print(f\"Submit Failure {task_type}\")\n",
        "            res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"fail\", \"client_id\": CLIENT_ID})\n",
        "\n",
        "\n",
        "\n",
        "    async def run(self):\n",
        "\n",
        "\n",
        "        try:\n",
        "            while True:\n",
        "                task = self.get_task()\n",
        "                if task:\n",
        "                    now = time.time()\n",
        "                    if now - self.last_heartbeat > HEARTBEAT_INTERVAL:\n",
        "                        self.send_heartbeat(task.get(\"task_id\"))\n",
        "                        self.last_heartbeat = now\n",
        "                    await self.handle_task(task)\n",
        "                else:\n",
        "                    print(\"‚è≥ No task available, sleeping...\")\n",
        "                    await asyncio.sleep(10)\n",
        "        finally:\n",
        "            await self.crawler_mgr.stop()\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "\n",
        "    # Ensure compatibility with environments like Jupyter or IPython\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "    # Create the crawler manager\n",
        "    crawler_manager = LookupEngine.CrawlerManager()\n",
        "    await crawler_manager.start()\n",
        "    # Create the client\n",
        "    client = NearestDoorClient(crawler_manager)\n",
        "\n",
        "    # Run the client loop\n",
        "    try:\n",
        "        asyncio.run(client.run())\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nüõë Shutting down gracefully...\")\n",
        "        sys.exit(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uB3mPA8_LmxP",
        "outputId": "dc870b46-2ebf-4e09-b9d6-8ac529daf73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Environment Fully Ready!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;36m[\u001b[0m\u001b[36mINIT\u001b[0m\u001b[1;36m]\u001b[0m\u001b[36m...\u001b[0m\u001b[36m. ‚Üí Crawl4AI \u001b[0m\u001b[1;36m0.6\u001b[0m\u001b[36m.\u001b[0m\u001b[1;36m3\u001b[0m\u001b[36m \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080\">INIT</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">]</span><span style=\"color: #008080; text-decoration-color: #008080\">.... ‚Üí Crawl4AI </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.6</span><span style=\"color: #008080; text-decoration-color: #008080\">.</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #008080; text-decoration-color: #008080\"> </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì° GET ‚Üí /next-task\n",
            "üì° POST ‚Üí /heartbeat\n",
            "ü´Ä Heartbeat sent.\n",
            "‚ñ∂Ô∏è Handling task search (ID: 30658)\n",
            "{'task_id': 30658, 'task_type': 'search', 'object_type': 'shop', 'data': {}, 'target': {'id': 31337, 'name': 'Eldridge dog park', 'city': 'Elmira', 'state': 'New York', 'website': None, 'slug': 'eldridge-dog-park', 'shop_type': 'dog-park'}}\n",
            "üåê Starting combined search‚Ä¶\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park \n",
            "error in basicc checker {'https://www.bringfido.com/attraction/13506'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/208/off-lease-area-for-dogs'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.mapquest.com/us/new-york/dog-park-at-eldrige-park-381009049'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/facilities/facility/details/eldridgepark-8'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://eldridgepark.org/new/eldridge-park-attractions/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.bringfido.com/attraction/33238'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.facebook.com/100068349571160'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://eldridgepark.org/new/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park site: yelp.com \n",
            "error in basicc checker {'https://www.yelp.com/biz/dog-park-at-eldrige-park-elmira'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.yelp.com/search?cflt=dog_parks&find_loc=elmira%2c+ny'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.mapquest.com/us/new-york/dog-park-at-eldrige-park-381009049'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/208/off-lease-area-for-dogs'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park site: reddit.com \n",
            "error in basicc checker {'https://eldridgepark.org/new/eldridge-park-attractions/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.bringfido.com/attraction/13506'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/208/off-lease-area-for-dogs'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.tripadvisor.com.au/attraction_review-g47682-d3242680-reviews-eldridge_park-elmira_finger_lakes_new_york.html'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üìö Wikipedia lookup ‚Üí Eldridge dog park Elmira dog-park\n",
            "‚úÖ Combined search complete.\n",
            "üì§ Submitting result for search (30658)\n",
            "üì° POST ‚Üí /submit/30658\n",
            "Server responded: 200 - {\"status\": \"success\", \"slug\": \"eldridge-dog-park\"}\n",
            "‚úÖ Submitted: search\n",
            "üì° GET ‚Üí /next-task\n",
            "‚ñ∂Ô∏è Handling task search (ID: 30659)\n",
            "{'task_id': 30659, 'task_type': 'search', 'object_type': 'shop', 'data': {}, 'target': {'id': 31337, 'name': 'Eldridge dog park', 'city': 'Elmira', 'state': 'New York', 'website': None, 'slug': 'eldridge-dog-park', 'shop_type': 'dog-park'}}\n",
            "üåê Starting combined search‚Ä¶\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park \n",
            "error in basicc checker {'https://www.bringfido.com/attraction/13506'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/208/off-lease-area-for-dogs'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.mapquest.com/us/new-york/dog-park-at-eldrige-park-381009049'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/facilities/facility/details/eldridgepark-8'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://eldridgepark.org/new/eldridge-park-attractions/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.bringfido.com/attraction/33238'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.facebook.com/100068349571160'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://eldridgepark.org/new/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park site: yelp.com \n",
            "error in basicc checker {'https://www.yelp.com/biz/dog-park-at-eldrige-park-elmira'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.yelp.com/search?cflt=dog_parks&find_loc=elmira%2c+ny'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.facebook.com/eldridgeparkelmira/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park site: reddit.com \n",
            "error in basicc checker {'https://eldridgepark.org/new/eldridge-park-attractions/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.bringfido.com/attraction/13506'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/208/off-lease-area-for-dogs'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.tripadvisor.com.au/attraction_review-g47682-d3242680-reviews-eldridge_park-elmira_finger_lakes_new_york.html'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üìö Wikipedia lookup ‚Üí Eldridge dog park Elmira dog-park\n",
            "‚úÖ Combined search complete.\n",
            "üì§ Submitting result for search (30659)\n",
            "üì° POST ‚Üí /submit/30659\n",
            "Server responded: 200 - {\"status\": \"success\", \"slug\": \"eldridge-dog-park\"}\n",
            "‚úÖ Submitted: search\n",
            "üì° GET ‚Üí /next-task\n",
            "‚ñ∂Ô∏è Handling task search (ID: 30660)\n",
            "{'task_id': 30660, 'task_type': 'search', 'object_type': 'shop', 'data': {}, 'target': {'id': 31337, 'name': 'Eldridge dog park', 'city': 'Elmira', 'state': 'New York', 'website': None, 'slug': 'eldridge-dog-park', 'shop_type': 'dog-park'}}\n",
            "üåê Starting combined search‚Ä¶\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park \n",
            "error in basicc checker {'https://www.bringfido.com/attraction/13506'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/208/off-lease-area-for-dogs'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.mapquest.com/us/new-york/dog-park-at-eldrige-park-381009049'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/facilities/facility/details/eldridgepark-8'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://eldridgepark.org/new/eldridge-park-attractions/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.bringfido.com/attraction/33238'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.facebook.com/100068349571160'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://eldridgepark.org/new/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park site: yelp.com \n",
            "error in basicc checker {'https://www.yelp.com/biz/dog-park-at-eldrige-park-elmira'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.yelp.com/search?cflt=dog_parks&find_loc=elmira%2c+ny'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.facebook.com/eldridgeparkelmira/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park site: reddit.com \n",
            "error in basicc checker {'https://www.bringfido.com/attraction/13506'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/208/off-lease-area-for-dogs'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.sniffspot.com/listings/elmira-ny'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://eldridgepark.org/new/eldridge-park-attractions/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üìö Wikipedia lookup ‚Üí Eldridge dog park Elmira dog-park\n",
            "‚úÖ Combined search complete.\n",
            "üì§ Submitting result for search (30660)\n",
            "üì° POST ‚Üí /submit/30660\n",
            "Server responded: 200 - {\"status\": \"success\", \"slug\": \"eldridge-dog-park\"}\n",
            "‚úÖ Submitted: search\n",
            "üì° GET ‚Üí /next-task\n",
            "‚ñ∂Ô∏è Handling task search (ID: 30661)\n",
            "{'task_id': 30661, 'task_type': 'search', 'object_type': 'shop', 'data': {}, 'target': {'id': 31337, 'name': 'Eldridge dog park', 'city': 'Elmira', 'state': 'New York', 'website': None, 'slug': 'eldridge-dog-park', 'shop_type': 'dog-park'}}\n",
            "üåê Starting combined search‚Ä¶\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park \n",
            "error in basicc checker {'https://www.bringfido.com/attraction/13506'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/208/off-lease-area-for-dogs'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.mapquest.com/us/new-york/dog-park-at-eldrige-park-381009049'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/facilities/facility/details/eldridgepark-8'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://eldridgepark.org/new/eldridge-park-attractions/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.bringfido.com/attraction/33238'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.facebook.com/100068349571160'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://eldridgepark.org/new/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park site: yelp.com \n",
            "error in basicc checker {'https://www.yelp.com/biz/dog-park-at-eldrige-park-elmira'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.yelp.com/search?cflt=dog_parks&find_loc=elmira%2c+ny'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.mapquest.com/us/new-york/dog-park-at-eldrige-park-381009049'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park site: reddit.com \n",
            "error in basicc checker {'https://www.bringfido.com/attraction/13506'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/208/off-lease-area-for-dogs'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.sniffspot.com/listings/elmira-ny'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://eldridgepark.org/new/eldridge-park-attractions/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üìö Wikipedia lookup ‚Üí Eldridge dog park Elmira dog-park\n",
            "‚úÖ Combined search complete.\n",
            "üì§ Submitting result for search (30661)\n",
            "üì° POST ‚Üí /submit/30661\n",
            "Server responded: 200 - {\"status\": \"success\", \"slug\": \"eldridge-dog-park\"}\n",
            "‚úÖ Submitted: search\n",
            "üì° GET ‚Üí /next-task\n",
            "‚ñ∂Ô∏è Handling task search (ID: 30662)\n",
            "{'task_id': 30662, 'task_type': 'search', 'object_type': 'shop', 'data': {}, 'target': {'id': 31337, 'name': 'Eldridge dog park', 'city': 'Elmira', 'state': 'New York', 'website': None, 'slug': 'eldridge-dog-park', 'shop_type': 'dog-park'}}\n",
            "üåê Starting combined search‚Ä¶\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park \n",
            "error in basicc checker {'https://www.bringfido.com/attraction/13506'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/208/off-lease-area-for-dogs'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.mapquest.com/us/new-york/dog-park-at-eldrige-park-381009049'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/facilities/facility/details/eldridgepark-8'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://eldridgepark.org/new/eldridge-park-attractions/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.bringfido.com/attraction/parks/city/elmira_ny_us/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.facebook.com/100068349571160'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://eldridgepark.org/new/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park site: yelp.com \n",
            "error in basicc checker {'https://www.yelp.com/biz/dog-park-at-eldrige-park-elmira'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.yelp.com/search?cflt=dog_parks&find_loc=elmira%2c+ny'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.facebook.com/eldridgeparkelmira/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park site: reddit.com \n",
            "error in basicc checker {'https://eldridgepark.org/new/eldridge-park-attractions/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.bringfido.com/attraction/13506'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/208/off-lease-area-for-dogs'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.tripadvisor.com.au/attraction_review-g47682-d3242680-reviews-eldridge_park-elmira_finger_lakes_new_york.html'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üìö Wikipedia lookup ‚Üí Eldridge dog park Elmira dog-park\n",
            "‚úÖ Combined search complete.\n",
            "üì§ Submitting result for search (30662)\n",
            "üì° POST ‚Üí /submit/30662\n",
            "Server responded: 200 - {\"status\": \"success\", \"slug\": \"eldridge-dog-park\"}\n",
            "‚úÖ Submitted: search\n",
            "üì° GET ‚Üí /next-task\n",
            "‚ñ∂Ô∏è Handling task search (ID: 30663)\n",
            "{'task_id': 30663, 'task_type': 'search', 'object_type': 'shop', 'data': {}, 'target': {'id': 31337, 'name': 'Eldridge dog park', 'city': 'Elmira', 'state': 'New York', 'website': None, 'slug': 'eldridge-dog-park', 'shop_type': 'dog-park'}}\n",
            "üåê Starting combined search‚Ä¶\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park \n",
            "error in basicc checker {'https://www.bringfido.com/attraction/13506'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/208/off-lease-area-for-dogs'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.mapquest.com/us/new-york/dog-park-at-eldrige-park-381009049'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/facilities/facility/details/eldridgepark-8'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://eldridgepark.org/new/eldridge-park-attractions/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.bringfido.com/attraction/parks/city/elmira_ny_us/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.facebook.com/100068349571160'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://eldridgepark.org/new/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park site: yelp.com \n",
            "error in basicc checker {'https://www.yelp.com/biz/dog-park-at-eldrige-park-elmira'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.yelp.com/search?cflt=dog_parks&find_loc=elmira%2c+ny'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.facebook.com/eldridgeparkelmira/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park site: reddit.com \n",
            "error in basicc checker {'https://eldridgepark.org/new/eldridge-park-attractions/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.bringfido.com/attraction/13506'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/208/off-lease-area-for-dogs'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.tripadvisor.com.au/attraction_review-g47682-d3242680-reviews-eldridge_park-elmira_finger_lakes_new_york.html'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üìö Wikipedia lookup ‚Üí Eldridge dog park Elmira dog-park\n",
            "‚úÖ Combined search complete.\n",
            "üì§ Submitting result for search (30663)\n",
            "üì° POST ‚Üí /submit/30663\n",
            "Server responded: 200 - {\"status\": \"success\", \"slug\": \"eldridge-dog-park\"}\n",
            "‚úÖ Submitted: search\n",
            "üì° GET ‚Üí /next-task\n",
            "‚ñ∂Ô∏è Handling task search (ID: 30664)\n",
            "{'task_id': 30664, 'task_type': 'search', 'object_type': 'shop', 'data': {}, 'target': {'id': 31337, 'name': 'Eldridge dog park', 'city': 'Elmira', 'state': 'New York', 'website': None, 'slug': 'eldridge-dog-park', 'shop_type': 'dog-park'}}\n",
            "üåê Starting combined search‚Ä¶\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park \n",
            "error in basicc checker {'https://www.bringfido.com/attraction/13506'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/208/off-lease-area-for-dogs'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.mapquest.com/us/new-york/dog-park-at-eldrige-park-381009049'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/facilities/facility/details/eldridgepark-8'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://eldridgepark.org/new/eldridge-park-attractions/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.bringfido.com/attraction/parks/city/elmira_ny_us/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.facebook.com/100068349571160'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://eldridgepark.org/new/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park site: yelp.com \n",
            "error in basicc checker {'https://www.yelp.com/biz/dog-park-at-eldrige-park-elmira'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.yelp.com/search?cflt=dog_parks&find_loc=elmira%2c+ny'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.facebook.com/eldridgeparkelmira/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üîé Google search ‚Üí Eldridge dog park Elmira New York dog-park site: reddit.com \n",
            "error in basicc checker {'https://eldridgepark.org/new/eldridge-park-attractions/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.bringfido.com/attraction/13506'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.cityofelmira.net/208/off-lease-area-for-dogs'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://www.tripadvisor.com.au/attraction_review-g47682-d3242680-reviews-eldridge_park-elmira_finger_lakes_new_york.html'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "error in basicc checker {'https://gravityextremezone.com/trampoline-park/eldridge-park-dog-park/'} {'Eldridge dog park'} {'dog-park'} Crawler not started\n",
            "üìö Wikipedia lookup ‚Üí Eldridge dog park Elmira dog-park\n"
          ]
        }
      ]
    }
  ]
}