{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexrosulek/Cs50/blob/main/gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://ollama.ai/install.sh | sh\n",
        "\n",
        "!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n",
        "!sudo apt-get update && sudo apt-get install -y cuda-drivers\n",
        "\n",
        "import os\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "!nohup ollama serve &\n",
        "!pip install pyOpenSSL==24.2.1\n",
        "\n",
        "\n",
        "# Pull Ollama Models\n",
        "\n",
        "!ollama pull qwen3:0.6b\n",
        "\n",
        "!ollama pull gemma3:1b\n",
        "!ollama pull gemma3:4b\n",
        "# Install Packages\n",
        "!pip install -q ollama crawl4ai aiohttp pillow beautifulsoup4 wikipedia googlesearch-python playwright nest_asyncio\n",
        "!playwright install chromium\n",
        "!nohup ollama serve &\n",
        "\n"
      ],
      "metadata": {
        "id": "-b9oppEb4cm2",
        "outputId": "0d476e55-3820-4c19-857b-fa1800820599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 13281    0 13281    0     0  62797      0 --:--:-- --:--:-- --:--:-- 62943\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to render group...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 129 kB in 1s (128 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cuda-drivers is already the newest version (575.51.03-0ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 88 not upgraded.\n",
            "nohup: appending output to 'nohup.out'\n",
            "Collecting pyOpenSSL==24.2.1\n",
            "  Downloading pyOpenSSL-24.2.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: cryptography<44,>=41.0.5 in /usr/local/lib/python3.11/dist-packages (from pyOpenSSL==24.2.1) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<44,>=41.0.5->pyOpenSSL==24.2.1) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<44,>=41.0.5->pyOpenSSL==24.2.1) (2.22)\n",
            "Downloading pyOpenSSL-24.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyOpenSSL\n",
            "  Attempting uninstall: pyOpenSSL\n",
            "    Found existing installation: pyOpenSSL 25.1.0\n",
            "    Uninstalling pyOpenSSL-25.1.0:\n",
            "      Successfully uninstalled pyOpenSSL-25.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "crawl4ai 0.6.3 requires pyOpenSSL>=24.3.0, but you have pyopenssl 24.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyOpenSSL-24.2.1\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mnohup: appending output to 'nohup.out'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve &\n",
        "import random\n",
        "import subprocess\n",
        "import wikipedia\n",
        "import requests\n",
        "from crawl4ai.content_filter_strategy import PruningContentFilter\n",
        "\n",
        "from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "from googlesearch import search\n",
        "from crawl4ai import LLMConfig, LLMExtractionStrategy, CrawlerRunConfig,CacheMode\n",
        "from crawl4ai.deep_crawling.filters import FilterChain, ContentRelevanceFilter\n",
        "from crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n",
        "import json\n",
        "\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "from crawl4ai.extraction_strategy import CosineStrategy\n",
        "\n",
        "from crawl4ai.async_configs import BrowserConfig\n",
        "\n",
        "from crawl4ai import AsyncWebCrawler,GeolocationConfig\n",
        "\n",
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "import asyncio\n",
        "import time\n",
        "import requests\n",
        "import nest_asyncio\n",
        "import httpx\n",
        "\n",
        "API_BASE = \"https://www.nearestdoor.com\"  # Replace with actual server URL\n",
        "CLIENT_ID = \"client001\"\n",
        "HEARTBEAT_INTERVAL = 60  # seconds\n",
        "SHOP_FLOW_STATIC = [\n",
        "    \"search\", \"aggregate\", \"createplan\", \"create\",\n",
        "    \"find_available_fields\", \"extract_fields_from_aggregate\", \"fillintheshop\"\n",
        "]\n",
        "# --------------------------------------------------------------------------- #\n",
        "# üß†  LIGHT‚ÄëWEIGHT LOCAL LLM EXECUTION                                       #\n",
        "# --------------------------------------------------------------------------- #\n",
        "class OllamaRunner:\n",
        "    \"\"\"\n",
        "    `ollama run ‚Ä¶`\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, default_model: str = \"gemma3:1b\", default_timeout: int = 600):\n",
        "        self.default_model = default_model\n",
        "        self.default_timeout = default_timeout\n",
        "\n",
        "    def run(self, prompt: str, model: str | None = None, timeout: int | None = None) -> str:\n",
        "        model = model or self.default_model\n",
        "        timeout = timeout or self.default_timeout\n",
        "        print(f\"üß† Running Ollama: {model}\")\n",
        "\n",
        "        try:\n",
        "            proc = subprocess.run(\n",
        "                [\"ollama\", \"run\", model],\n",
        "                input=prompt.encode(\"utf-8\"),\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                timeout=timeout,\n",
        "            )\n",
        "\n",
        "            raw_output = proc.stdout.decode(\"utf-8\").strip()\n",
        "            return re.sub(r\"<think>.*?</think>\", \"\", raw_output, flags=re.DOTALL).strip()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Ollama execution failed: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "# üåê  LOOK‚ÄëUP ENGINE                                                          #\n",
        "# --------------------------------------------------------------------------- #\n",
        "class LookupEngine:\n",
        "    \"\"\"\n",
        "    ‚Äì Validates every URL first\n",
        "    ‚Äì Google results exclude Yelp & Reddit and are content‚Äëchecked\n",
        "    ‚Äì Yelp & Reddit results are *also* content‚Äëchecked before ‚Äòbattling‚Äô\n",
        "    ‚Äì At most one Yelp URL & one Reddit URL are returned\n",
        "    ‚Äì Wikipedia returns at most one page (auto_suggest)\n",
        "    \"\"\"\n",
        "    def __init__(self,  ollama_runner: OllamaRunner | None = None):\n",
        "\n",
        "        self.llm_config = LLMConfig(provider=\"ollama/gemma3:1b\")\n",
        "        self.ollama = ollama_runner or OllamaRunner()\n",
        "        self.crawler_manager = self.CrawlerManager()\n",
        "    async def initialize(self):\n",
        "        await self.crawler_manager.start()\n",
        "\n",
        "\n",
        "    # ---------------------  LOW‚ÄëLEVEL HELPERS  ----------------------------- #\n",
        "    class CrawlerManager:\n",
        "        def __init__(self):\n",
        "            self.crawler = None\n",
        "\n",
        "\n",
        "\n",
        "        async def crawl(self, url: str,browserconfig, config: CrawlerRunConfig):\n",
        "\n",
        "\n",
        "            try:\n",
        "                async with AsyncWebCrawler(config=browserconfig) as crawler:\n",
        "                    result = await crawler.arun(url=url, config=config)\n",
        "                    return result\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå crawl error for {url}: {e}\")\n",
        "                return None\n",
        "\n",
        "\n",
        "        async def _normalize_and_validate_url(self, url: str) -> str | None:\n",
        "          try:\n",
        "              url = url.lower()\n",
        "              if not url.startswith((\"http://\", \"https://\")):\n",
        "                  url = \"https://\" + url\n",
        "\n",
        "              parsed = urlparse(url)\n",
        "              if (\n",
        "                  parsed.scheme not in [\"http\", \"https\"]\n",
        "                  or not parsed.netloc\n",
        "                  or \".\" not in parsed.netloc\n",
        "                  or \" \" in parsed.netloc\n",
        "                  or \"/http\" in parsed.netloc\n",
        "              ):\n",
        "                  return None  # Invalid URL\n",
        "\n",
        "              # ‚úÖ Check if the URL is reachable using httpx (this was incorrectly indented before)\n",
        "              async with httpx.AsyncClient(timeout=5) as client:  # Timeout should be in seconds, not 800ms\n",
        "                  try:\n",
        "                      response = await client.head(url, follow_redirects=True)\n",
        "                      if response.status_code < 400:\n",
        "                          return url\n",
        "                      # Some servers reject HEAD, fallback to GET\n",
        "                      response = await client.get(url, follow_redirects=True)\n",
        "                      if response.status_code < 400:\n",
        "                          return url\n",
        "                      return False  # URL unreachable\n",
        "                  except httpx.RequestError as e:\n",
        "                      print(f\"‚ùå HTTP check failed: {url} ({e})\")\n",
        "                      return None\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"‚ùå URL normalization failed: {e}\")\n",
        "              return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    async def _extract_snippet(self, url, max_chars, min_chars):\n",
        "        try:\n",
        "            async with async_playwright() as p:\n",
        "                browser = await p.chromium.launch(headless=True)\n",
        "                context = await browser.new_context(\n",
        "                    user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\",\n",
        "                    locale=\"en-US\",\n",
        "                    java_script_enabled=False,\n",
        "                    permissions=[\"geolocation\"],\n",
        "\n",
        "    viewport={\"width\": 1280, \"height\": 720}\n",
        "                )\n",
        "                page = await context.new_page()\n",
        "\n",
        "                print(f\"Fetching: {url}\")\n",
        "                await page.goto(url, wait_until=\"domcontentloaded\", timeout=20000)\n",
        "\n",
        "                await page.wait_for_timeout(random.randint(1000, 3000))  # Randomized delay\n",
        "\n",
        "\n",
        "                title = await page.title()\n",
        "                desc = None\n",
        "\n",
        "                # Try standard meta description first\n",
        "                try:\n",
        "                    desc = await page.locator('meta[name=\"description\"]').get_attribute('content')\n",
        "                except Exception:\n",
        "                    # Try OpenGraph and Twitter metadata as fallback\n",
        "                    for meta_tag in [\"meta[property='og:description']\", \"meta[name='twitter:description']\"]:\n",
        "                        try:\n",
        "                            desc = await page.locator(meta_tag).get_attribute('content')\n",
        "                            if desc:\n",
        "                                break\n",
        "                        except Exception:\n",
        "                            continue\n",
        "                    if not desc:\n",
        "                        print(f\"No meta description found for {url}\")\n",
        "\n",
        "\n",
        "                parts = []\n",
        "                if title:\n",
        "                  parts.append(\"Title: \" + title.strip())\n",
        "                if desc:\n",
        "                  parts.append(\"Desc: \" + desc.strip())\n",
        "\n",
        "                await browser.close()\n",
        "\n",
        "                result = \"\\n\".join(parts)\n",
        "\n",
        "\n",
        "                print(\"ddj\")\n",
        "                if len(str(result)) < min_chars:\n",
        "                    print(f\"‚ùå Extracted content too short: {len(result)} chars\")\n",
        "                    return None\n",
        "                result = result[:max_chars]\n",
        "                return f\"Snippet From {url}:\\n{result[:max_chars]}\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Playwright extraction failed for {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    async def _basic_url_checker(self, snippet,url: str, shop_name: str, shop_type: str, state: str, city:str) -> bool:\n",
        "        try:\n",
        "\n",
        "            \"\"\"\n",
        "            Improved relevance check for a URL:\n",
        "            1. Uses Crawl4AI's ContentRelevanceFilter for semantic similarity.\n",
        "            2. Falls back to LLM prompt decision if semantic check is inconclusive.\n",
        "            \"\"\"\n",
        "\n",
        "            if not url:\n",
        "                return False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Step 2: Fallback ‚Äî Quick Snippet and LLM Yes/No Decision\n",
        "\n",
        "            if not snippet:\n",
        "                print(\"no snippet\")\n",
        "                return False\n",
        "\n",
        "            prompt = (\n",
        "                f\"Is the website {url} related at all to '{shop_name}' or in any way related to the place its located in, {state}, {city}? \"\n",
        "                f\"Be super lient and allow all websites related to {shop_type} to pass. If the name {shop_name} or {state}, {city} is present or any related info like the town, the category, ect then let it pass. Be super lient. Answer only `true` or `false` or `none`.\\n\\n{snippet}\"\n",
        "            )\n",
        "\n",
        "            decision = self.ollama.run(prompt, 'qwen3:0.6b')\n",
        "            print(decision)\n",
        "            print(prompt)\n",
        "            if \"true\" in decision.lower():\n",
        "                print(f\"‚úÖ LLM confirmed relevance for: {url}\")\n",
        "                return True\n",
        "\n",
        "            print(f\"‚ö†Ô∏è URL deemed irrelevant: {url}\")\n",
        "            return False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"error in basicc checker\", {url}, {shop_name}, {shop_type}, e)\n",
        "            return False\n",
        "    @staticmethod\n",
        "    def get_semantic_query(shop_type, shop_name):\n",
        "        queries = {\n",
        "            \"church\": f\"{shop_type}, {shop_name}, history, review, hours, muslim, phone, church, christian, church events, holiday schedules, mass times, sermons, church history, community programs, accessibility options, FAQs, donation methods, parking, contact information\",\n",
        "            \"plasma_center\": f\"{shop_type}, {shop_name}, history, review, stocked brands, review, hours, phone, plasma, plasma donation requirements, compensation rates, donor reward, donor eligibility, contact details, operating hours, health guidelines, FAQ, appointment scheduling, safety procedures\",\n",
        "            \"thrift_store\": f\"{shop_type}, {shop_name}, history, review, stocked brands, second hand,  review, hours, phone, thrift, store hours, donation guidelines, accepted items, discounts, sales events, store history, accessibility, contact info, volunteer programs, reviews\",\n",
        "            \"dog_park\": f\"{shop_type}, {shop_name}, history, review, water, shade, agility equipment, park, review, hours, phone, dog, dog park hours, leash rules, pet-friendly areas, dog-friendly facilities, park amenities, accessibility options, entry fees, safety tips, events, pet policies, reviews\",\n",
        "        }\n",
        "        return queries.get(shop_type.lower(), \"business information, contact details, operating hours, reviews, FAQs, history\")\n",
        "\n",
        "    async def _get_site_content(\n",
        "\n",
        "            self,\n",
        "\n",
        "            url: str,\n",
        "            shop_name: str,\n",
        "            shop_type: str,\n",
        "state:str, city:str\n",
        "            ) -> str | None:\n",
        "        try:\n",
        "            print(\"scraping\")\n",
        "        # Step 1: Semantic Filter Based on Shop Type\n",
        "            #semantic_query = self.get_semantic_query(shop_type, shop_name)\n",
        "\n",
        "\n",
        "            prune_filter = PruningContentFilter(\n",
        "                        threshold=0.05,\n",
        "                        threshold_type=\"dynamic\",  # or \"dynamic\"\n",
        "                        min_word_threshold=5\n",
        "                    )\n",
        "\n",
        "            md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)\n",
        "\n",
        "\n",
        "            crawl_config = CrawlerRunConfig(\n",
        "\n",
        "                  markdown_generator=md_generator,\n",
        "\n",
        "\n",
        "                  excluded_tags=[\"style\", \"script\", \"footer\"],\n",
        "  cache_mode=CacheMode.BYPASS,\n",
        "    page_timeout=60000,\n",
        "\n",
        "\n",
        "            )\n",
        "            bconfig = BrowserConfig(\n",
        "            headless=True,\n",
        "            viewport_width=1280,\n",
        "                    viewport_height=720,\n",
        "                   user_agent_mode=\"random\",\n",
        "                    text_mode=True\n",
        "            )\n",
        "            result = await self.crawler_manager.crawl(url, bconfig,config=crawl_config)\n",
        "\n",
        "\n",
        "            if result.success:\n",
        "\n",
        "                  print(result.markdown.fit_markdown)\n",
        "                  return result.markdown.fit_markdown\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to parse site content: {e}\")\n",
        "            return None\n",
        "    # ---------------------  LOOK‚ÄëUP ROUTINES  ------------------------------ #\n",
        "    async def wikipedia_lookup(self, name: str, city: str, shop_type: str) -> str | None:\n",
        "        try:\n",
        "            query = f\"{name} {city} {shop_type}\".strip()\n",
        "            print(f\"üìö Wikipedia lookup ‚Üí {query}\")\n",
        "            page = wikipedia.page(query, auto_suggest=True)\n",
        "            content = page.content\n",
        "\n",
        "            if len(content) <= 2000:\n",
        "                return content\n",
        "            chunks = [content[i:i + 500] for i in range(0, len(content), 500)]\n",
        "            # Fallback-safe middle extraction\n",
        "            if len(chunks) > 6:\n",
        "                middle = chunks[2:-2]  # Remove first and last 2 chunks\n",
        "                if not middle:\n",
        "                    middle = chunks  # If middle is empty, fallback to all chunks\n",
        "            else:\n",
        "                middle = chunks\n",
        "\n",
        "            # Intelligent selection\n",
        "            if len(middle) > 6:\n",
        "                selected = random.sample(middle, 6)  # Randomly select 6 if too many\n",
        "            else:\n",
        "                selected = middle  # Take all available if 6 or fewer\n",
        "            formatted_chunks = [f\"\\nWIKI CHUNK {idx + 1}:\\n{chunk}\" for idx, chunk in enumerate(selected)]\n",
        "\n",
        "            return f\"ALL EXTRACTED WIKIPEDIA SEARCH INFO FOR {name}:\\n\" + \"\\n\".join(formatted_chunks)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Wikipedia fetch failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    async def search_lookup(self,  name: str,  shop_type: str, query: str, placename: str, amount: int,state:str, city:str, isyelp: bool) -> str | None:\n",
        "        try:\n",
        "            print(f\"üîé {placename} search ‚Üí {query}\")\n",
        "            print(\"dd\")\n",
        "            raw = list(search(query, amount))\n",
        "            if not isyelp:\n",
        "                print(\"ddf\")\n",
        "                candidate_urls = [\n",
        "                      u for u in raw\n",
        "                      if all(excl not in str(u).lower() for excl in [\"yelp\", \"reddit\", \"wiki\", \"nearestdoor\", \"facebook\", \"twitter\"])\n",
        "                  ]\n",
        "                print(\"bfd\")\n",
        "            else:\n",
        "                print(\"rgrg\")\n",
        "                candidate_urls = [\n",
        "                      u for u in raw\n",
        "                      if (\"yelp\" in str(u).lower() or \"reddit\" in str(u).lower())\n",
        "                      and all(excl not in str(u).lower() for excl in [\"wiki\", \"nearestdoor\", \"facebook\", \"twitter\"])\n",
        "                  ]\n",
        "                print(\"frg\")\n",
        "\n",
        "            good_content = []\n",
        "\n",
        "            print(\"ddd\")\n",
        "            for i, url in enumerate(candidate_urls):\n",
        "                print(url, \"url canidate\")\n",
        "                urld = await self.crawler_manager._normalize_and_validate_url(url)\n",
        "                if not urld:\n",
        "                    continue\n",
        "                snippet = await self._extract_snippet(url, 750, 40)\n",
        "                if not await self._basic_url_checker(snippet, urld, name, shop_type, state, city):\n",
        "                    continue\n",
        "\n",
        "                content = await self._get_site_content(urld, name, shop_type,state, city)\n",
        "                if content:\n",
        "                    good_content.append(f\"\\n‚Üê¬†{placename} SEARCH¬†DATA¬†SITE {i} FROM: {url}\\n {content}\")\n",
        "\n",
        "            return f\"ALL EXTRACTED {placename} SEARCH DATA FOR {name}, {city}\\n\".join(good_content) if good_content else None\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to parse {placename} search lookup results: {e}\")\n",
        "            return None\n",
        "    # ---------------------  PUBLIC ENTRY POINT  ---------------------------- #\n",
        "    async def combined_search(self, name: str, city: str, state: str, shop_type: str, website_url: str) -> tuple[bool, str | None, None]:\n",
        "        print(\"üåê Starting combined search‚Ä¶\")\n",
        "\n",
        "        Google_query = f\"{name} {city} {state} {shop_type} \"\n",
        "\n",
        "        if website_url:\n",
        "            Official_query = f\"{name} site: {website_url} \"\n",
        "\n",
        "        g_res = await self.search_lookup(name, shop_type, Google_query, \"Google\", 10,state, city,  False)\n",
        "        w_res = await self.wikipedia_lookup(name, city, shop_type)\n",
        "        o_res = None\n",
        "        if website_url:\n",
        "            o_res = await self.search_lookup(name, shop_type, Official_query, f\"Official Website of {name}\", 5,state, city, True)\n",
        "        main = \"\"\n",
        "\n",
        "\n",
        "        if g_res:\n",
        "            main += g_res\n",
        "        if w_res:\n",
        "            main += w_res\n",
        "        if o_res:\n",
        "            main += o_res\n",
        "        print('sss')\n",
        "        if len(main) < 500:\n",
        "            print(\"‚ùå Not enough content gathered.\")\n",
        "            return False, None, None\n",
        "\n",
        "        print(\"‚úÖ Combined search complete.\")\n",
        "        return True, main, None\n",
        "# --------------------------------------------------------------------------- #\n",
        "# üì¶  HIGH‚ÄëLEVEL CONTENT¬†SUMMARIZER                                          #\n",
        "# --------------------------------------------------------------------------- #\n",
        "class ContentSummarizer:\n",
        "    \"\"\"\n",
        "    Reduce a large blob of text about a specific business down to ‚â§‚ÄØmax_final_chars\n",
        "    while preserving high‚Äëvalue facts. Uses multi-stage LLM summarisation with\n",
        "    chunk filtering and escalation if necessary.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        ollama_runner: OllamaRunner,\n",
        "        shop_name: str,\n",
        "        shop_type: str,\n",
        "        city: str | None = None,\n",
        "        state: str | None = None,\n",
        "        max_final_chars: int = 6000,\n",
        "        min_final_chars: int = 500,\n",
        "    ):\n",
        "        self.ollama = ollama_runner\n",
        "        self.shop_name = shop_name\n",
        "        self.shop_type = shop_type\n",
        "        self.city = city or \"\"\n",
        "        self.state = state or \"\"\n",
        "        self.max_final_chars = max_final_chars\n",
        "        self.min_final_chars = min_final_chars\n",
        "\n",
        "    # ----------------- Internal Helpers ----------------- #\n",
        "    def _clean_raw_content(self, content: str) -> str:\n",
        "        lines = content.splitlines()\n",
        "        cleaned, seen = [], set()\n",
        "        NOISE = [\n",
        "            \"cookie policy\", \"all rights reserved\", \"subscribe\", \"advertisement\",\n",
        "            \"accept cookies\", \"privacy policy\", \"terms of service\", \"sign in\", \"cookie\"\n",
        "        ]\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            lo = line.lower()\n",
        "            if len(line) < 30 or lo in seen:\n",
        "                continue\n",
        "            if any(noise in lo for noise in NOISE):\n",
        "                continue\n",
        "            seen.add(lo)\n",
        "            cleaned.append(line)\n",
        "        return \"\\n\".join(cleaned)\n",
        "\n",
        "    def _chunk_text(self, text: str, chunk_size: int) -> list[str]:\n",
        "        return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "\n",
        "    def _filter_chunks(self, chunks: list[str], model: str = \"gemma3:1b\") -> list[str]:\n",
        "        try:\n",
        "            good = []\n",
        "            print(f\"Filtering {len(chunks)} chunks...\")\n",
        "            for chunk in chunks:\n",
        "                prompt = (\n",
        "                    f\"Is the following content related at all to {self.shop_type}, {self.city} {self.state}\"\n",
        "                    f\"'{self.shop_name}'. Be super lient. Only reply 'true' or 'false'.\\n\\n{chunk}\"\n",
        "                )\n",
        "                print(\"prompt\", prompt)\n",
        "                decision = self.ollama.run(prompt, model=model).strip().lower()\n",
        "                print(\"decision\", decision)\n",
        "                if \"true\" in decision:\n",
        "                    good.append(chunk)\n",
        "\n",
        "            return good\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to filter chunk: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _build_prompt(self, text_chunk: str) -> str:\n",
        "        return (\n",
        "            f\"You are creating a SHORT unformatted summary for \"\n",
        "            f\"{self.shop_type} **{self.shop_name}** \"\n",
        "            f\"{'in ' + self.city if self.city else ''}{', ' + self.state if self.state else ''}.\\n\\n\"\n",
        "            f\"KEEP USEFUL DATA\"\n",
        "            \"DO NOT USE ASTERISKS OR * OR **\"\n",
        "            \"KEEP SERVICES.\\n\"\n",
        "            f\"KEEP URLS ONLY WHEN IT IS THE LITERAL {self.shop_name}'s WEBSITE.\\n\"\n",
        "            f\"KEEP ALL GOOD INFO AND HISTORY, FACTS, INFO, Extract all usefull info,\\n\"\n",
        "\n",
        "            f\" remove weird info.\\n\"\n",
        "\n",
        "            f\"--- SOURCE TEXT START ---\\n{text_chunk}\\n--- SOURCE TEXT END ---\"\n",
        "\n",
        "        )\n",
        "\n",
        "    def _summarize_with_ollama(self, text: str, model: str) -> str:\n",
        "        try:\n",
        "            prompt = self._build_prompt(text)\n",
        "            print(\"prompt summarize\", prompt)\n",
        "            return self.ollama.run(prompt, model=model)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to summarize w ollama: {e}\")\n",
        "            return None\n",
        "    def summarize_chunks(\n",
        "        self,\n",
        "        content: str,\n",
        "\n",
        "        initial_model: str = \"gemma3:1b\",\n",
        "\n",
        "    ) -> str:\n",
        "        try:\n",
        "            print(content)\n",
        "            if len(content) < self.min_final_chars:\n",
        "                print(\"filtered less than min\")\n",
        "                return None\n",
        "            if len(content) > self.max_final_chars:\n",
        "\n",
        "                chunks = self._chunk_text(content, 1000)\n",
        "\n",
        "                content = self._filter_chunks(chunks)\n",
        "                if not content:\n",
        "                    content = chunks\n",
        "                content = ''.join(content)\n",
        "\n",
        "            if len(content) < self.min_final_chars:\n",
        "                print(\"filtered less than min2\")\n",
        "                return None\n",
        "            chunks = self._chunk_text(content, 3000)\n",
        "\n",
        "            summarized_chunks = []\n",
        "\n",
        "            for idx, chunk in enumerate(chunks, start=1):\n",
        "                print(f\"üìö Summarizing chunk {idx}/{len(chunks)}...\")\n",
        "                summary = self._summarize_with_ollama(chunk, model=initial_model)\n",
        "                print(\"summary\", summary)\n",
        "                if not summary or len(summary) < 50:\n",
        "                    print(f\"‚ö†Ô∏è Failed to summarize chunk {idx}, keeping raw content.\")\n",
        "                    summary = chunk  # Fallback to raw content if summary failed\n",
        "\n",
        "                summarized_chunks.append(f\"### CHUNK {idx} SUMMARY:\\n{summary}\")\n",
        "\n",
        "                # Early exit: check if adding all remaining raw chunks without summarizing fits within limit\n",
        "                combined_so_far = \"\\n\\n\".join(summarized_chunks)\n",
        "                remaining_raw = \"\".join(chunks[idx:])  # Remaining chunks after current one\n",
        "\n",
        "                if len(combined_so_far) + len(remaining_raw) <= self.max_final_chars:\n",
        "                    print(f\"‚úÖ Early exit: current summary + remaining raw fits within limit. Skipping further summarization.\")\n",
        "                    for r_idx, remaining_chunk in enumerate(chunks[idx:], start=idx + 1):\n",
        "                        summarized_chunks.append(f\"### CHUNK {r_idx} (Raw):\\n{remaining_chunk}\")\n",
        "                    break\n",
        "\n",
        "            combined_summary = \"\\n\\n\".join(summarized_chunks)\n",
        "            if len(combined_summary) < self.min_final_chars:\n",
        "                print(\"smmariez less than min\")\n",
        "                return None\n",
        "            # Final trim if absolutely necessary\n",
        "            if len(combined_summary) > self.max_final_chars:\n",
        "                print(\"‚ö†Ô∏è Final combined summary exceeds character limit. Trimming result.\")\n",
        "                return combined_summary[:self.max_final_chars]\n",
        "\n",
        "            return combined_summary\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to sumarrize chunks: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    def summarize_content(self, raw_content: str) -> str:\n",
        "        print(\"üßπ Cleaning raw content...\")\n",
        "        cleaned = self._clean_raw_content(raw_content)\n",
        "        print(cleaned)\n",
        "\n",
        "        print(\"summarizing\")\n",
        "        final_summary = self.summarize_chunks(\n",
        "        content=cleaned,           # The large raw text content you want to reduce\n",
        "               # Size of each chunk before summarizing\n",
        "        initial_model=\"gemma3:1b\",      # Start with the lightweight model\n",
        "\n",
        "    )\n",
        "        print(final_summary)\n",
        "        if final_summary is None:\n",
        "            print(\"‚ùå Summarization failed. Returning none instead.\")\n",
        "            return '', False\n",
        "\n",
        "        print(\"üéâ Final summarization complete.\")\n",
        "        return final_summary, True\n",
        "\n",
        "import re\n",
        "import json\n",
        "\n",
        "class Smartypants:\n",
        "    def __init__(self, ollama_runner: OllamaRunner):\n",
        "        self.ollama = ollama_runner\n",
        "\n",
        "    def _run4(self, prompt: str) -> str:\n",
        "        return self.ollama.run(prompt, model=\"gemma3:4b\",)\n",
        "    def _run1(self, prompt: str) -> str:\n",
        "        return self.ollama.run(prompt, model=\"gemma3:1b\")\n",
        "\n",
        "    # ------------ PLAN ------------ #\n",
        "    def create_plan(self, aggregate: str,shop_name: str, shop_type: str,  city: str, state: str) -> tuple[bool, list[str]]:\n",
        "        prompt = (\n",
        "            \"You have an aggregated summary about a \"\n",
        "\n",
        "            f\"place called {shop_name} {shop_type} in {city}, {state}\\n\\n\"\n",
        "            f\"{aggregate}\\n\\n\"\n",
        "            \"Which content sections can confidently be generated based on this?\\n\"\n",
        "            \"Options: article, faq, history.\\n\"\n",
        "            \"Reply with a correct python comma-separated list of available sections to write about, nothing else, Options: article, faq, history.\"\n",
        "        )\n",
        "        try:\n",
        "            count = 0\n",
        "            resp = self._run4(prompt).lower()\n",
        "            valid = {\"article\", \"faq\", \"history\"}\n",
        "            print(resp)\n",
        "            for s in valid:\n",
        "                if s not in resp:\n",
        "                    count +=1\n",
        "            if count == 3:\n",
        "                return False, []\n",
        "\n",
        "            return True, [s for s in valid if s in resp]\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå create_plan error: {e}\")\n",
        "            return False, []\n",
        "\n",
        "    def check_aggregate_quality(self, shop_name: str, aggregate: str,shop_type: str,  city: str, state: str) -> bool:\n",
        "        prompt = (\n",
        "            f\"Check if this content contains usefull info about {shop_name} {shop_type} in {city}, {state}.\\n\\n{aggregate}\\n\\n\"\n",
        "            \"Reply only `true` or `false`.\"\n",
        "        )\n",
        "        try:\n",
        "            return \"true\" in self._run1(prompt).lower()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå quality check error: {e}\")\n",
        "            return False\n",
        "\n",
        "    # ------------ SECTION VALIDATION / FIX ------------ #\n",
        "    def validate_section_html(self, shop_name: str, section: str, text: str) -> bool:\n",
        "        prompt = (\n",
        "\n",
        "            f\"Validate the following text for section '{section}'. It is supposed to be about '{shop_name}'.\\n\\n{text}\\n\\n\"\n",
        "            \"Rules: DO NOT ALLOW ASTERISKS, DO NOT ALLOW * OR **.\\n- No HTML.\\n- No irrelevant info. Is it useful and no format or weird characters? Nothing Else, Nothing before or after our content\\n\"\n",
        "            \"- Only factual, structured, and clear content.\\n- Reply `true` or `false` only.\"\n",
        "        )\n",
        "        print(\"validate html\", prompt)\n",
        "        return \"true\" in self._run1(prompt).lower()\n",
        "\n",
        "    def fix_section_html(self, shop_name: str, section: str, text: str) -> str | None:\n",
        "        prompt = (\n",
        "            f\"Clean and fix this section '{section}'s format. It is about '{shop_name}'.\\n\\n{text}\\n\\n\"\n",
        "            \"Rules:DO NOT USE ASTERISKS, DO NOT USE * OR **. \\n- No HTML, Is it useful and and no format or weird characters? asterisks, or irrelevant info.\\n\"\n",
        "            \"Return only the final cleaned text for consumers on nearestdoor.com to read, no junk, no explanations, nothing else, nothing before or after our content. Only return the corrected text.\"\n",
        "        )\n",
        "        print(\"fixing html\",prompt)\n",
        "        return self._run4(prompt).strip()\n",
        "\n",
        "    # ------------ JSON & FIELD EXTRACTION ------------ #\n",
        "\n",
        "    def extract_clean_json_structure(self,text: str, field_name: str = None) -> dict | list | None:\n",
        "        try:\n",
        "            # ‚úÖ Extract content inside ```json ... ```\n",
        "            json_block = re.search(r\"```json\\s*(.*?)\\s*```\", text, re.IGNORECASE | re.DOTALL)\n",
        "            if json_block:\n",
        "                text = json_block.group(1).strip()\n",
        "            else:\n",
        "                text = text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "            # ‚úÖ Extract JSON object or array\n",
        "            match = re.search(r\"(\\{.*?\\}|\\[.*?\\])\", text, re.DOTALL)\n",
        "            if not match:\n",
        "                return None\n",
        "\n",
        "            json_ready = match.group(0)\n",
        "            match_text_lower = json_ready.lower()\n",
        "\n",
        "            # ‚úÖ Exclude meaningless content\n",
        "            exclusion_keywords = ['n/a', 'n-a', 'none', 'false', 'na', 'cant', 'not', 'found', 'unable', '{{', '()', 'unavailable']\n",
        "            if any(bad in match_text_lower for bad in exclusion_keywords):\n",
        "                return None\n",
        "\n",
        "            # ‚úÖ Clean JSON formatting issues\n",
        "            json_ready = json_ready.replace(\"'\", '\"')\n",
        "            json_ready = re.sub(r\",\\s*([\\]}])\", r\"\\1\", json_ready)\n",
        "\n",
        "            parsed = json.loads(json_ready)\n",
        "\n",
        "            # ‚úÖ Handle List: Deduplicate and Title Case\n",
        "            if isinstance(parsed, list):\n",
        "                seen = set()\n",
        "                cleaned_list = []\n",
        "                for item in parsed:\n",
        "                    if isinstance(item, str):\n",
        "                        cleaned_item = item.strip().title()\n",
        "                        if cleaned_item and cleaned_item.lower() not in exclusion_keywords and cleaned_item not in seen:\n",
        "                            seen.add(cleaned_item)\n",
        "                            cleaned_list.append(cleaned_item)\n",
        "                result = cleaned_list if cleaned_list else None\n",
        "\n",
        "            # ‚úÖ Handle Dict: Clean keys and values\n",
        "            elif isinstance(parsed, dict):\n",
        "                cleaned_dict = {}\n",
        "                for k, v in parsed.items():\n",
        "                    if isinstance(v, str):\n",
        "                        v_clean = v.strip().lower()\n",
        "                        if v_clean in exclusion_keywords:\n",
        "                            continue\n",
        "                        cleaned_dict[k.title()] = v.title()\n",
        "                    else:\n",
        "                        cleaned_dict[k.title()] = v\n",
        "                result = cleaned_dict if cleaned_dict else None\n",
        "\n",
        "            else:\n",
        "                result = None\n",
        "\n",
        "            # ‚úÖ Final Validation Using FIELD_VALIDATORS\n",
        "            if field_name and field_name in FIELD_VALIDATORS:\n",
        "                validator = FIELD_VALIDATORS[field_name]\n",
        "                if not validator(result):\n",
        "                    print(f\"‚ùå Validation failed for field: {field_name} with value: {result}\")\n",
        "                    return None\n",
        "\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå JSON extraction failed: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "    def extract_available_fields(self, aggregate: str,shop_name: str, shop_type: str,  city: str, state: str) -> tuple[bool, list[str]]:\n",
        "        try:\n",
        "            field_list = list(FIELD_EXTRACTORS.keys())\n",
        "            field_str = ', '.join(field_list)\n",
        "            prompt = (\n",
        "                f\"Analyze the content:\\n\\n{aggregate}\\n\\n\"\n",
        "                f\"Whos data we want {shop_type}, {shop_name}. \"\n",
        "\n",
        "                f\"Which of these fields can be confidently extracted from the content?\\n{field_str}\\n\"\n",
        "                \"Reply ONLY with the field keys that are present in the text, as ONLY the correct format requested. no junk. Nothing else. \"\n",
        "                \"If none, reply exactly 'none'.\"\n",
        "            )\n",
        "            response = self._run4(prompt).strip().lower()\n",
        "            if \"none\" in response:\n",
        "                return True, []\n",
        "            detected = [field for field in field_list if field.lower() in response]\n",
        "            return True, detected\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå extract_available_fields error: {e}\")\n",
        "            return False, []\n",
        "    def validate_extracted_field_value(self, field: str,fieldextractprompt, value) -> bool:\n",
        "        \"\"\"\n",
        "        Validate the extracted field value using LLM and manual schema checks.\n",
        "\n",
        "        - If it's a JSON list, remove invalid entries.\n",
        "        - If it's invalid after cleaning, return False.\n",
        "        \"\"\"\n",
        "        # LLM-Based Validation Prompt\n",
        "        prompt = (\n",
        "            f\"Validate this extracted value for field '{field}':\\n\\n{value}\\n\\n, {fieldextractprompt} \"\n",
        "            \"Is this a valid and correct value and format for the specified field requested? Is it weird for the field or contain none values? Reply ONLY `true` or `false`.\"\n",
        "        )\n",
        "        print(\"validate field\", prompt)\n",
        "        llm_decision = \"true\" in self._run1(prompt).lower()\n",
        "        print(\"field valid decision\", llm_decision)\n",
        "        # If LLM says it's invalid, fail immediately\n",
        "        if not llm_decision:\n",
        "            print(f\"‚ùå LLM validation failed for field '{field}'.\")\n",
        "            return False\n",
        "\n",
        "\n",
        "\n",
        "        # Final check for singular values\n",
        "        return True\n",
        "\n",
        "    def extract_fields(\n",
        "        self, aggregate: str, available_fields: list[str],\n",
        "        shop_name: str, shop_type: str, city: str, state: str\n",
        "    ) -> tuple[bool, dict]:\n",
        "        extracted = {}\n",
        "        try:\n",
        "            for field in available_fields:\n",
        "                try:\n",
        "                    print(field)\n",
        "                    prompt = (\n",
        "                        f\"{FIELD_EXTRACTORS[field]}\\n\\nContent:\\n{aggregate}\\n\\n\"\n",
        "                        f\"Return ONLY the valid structure requested. Respond with nothing but the correct format requested. \"\n",
        "                        f\"If none, say 'none'. Nothing else, nothing before or after our content. Data about {shop_type}, {shop_name}.\"\n",
        "                    )\n",
        "                    print(\"aggregate\",aggregate)\n",
        "                    print(\"extracting fields\",prompt )\n",
        "                    raw_value = self._run4(prompt).strip()\n",
        "                    print(\"raw field\", raw_value)\n",
        "                    final_value = self.extract_clean_json_structure(raw_value, field)\n",
        "                    if final_value is None:\n",
        "                        print(\"final value none\")\n",
        "                        continue\n",
        "                    if self.validate_extracted_field_value(field,{FIELD_EXTRACTORS[field]}, final_value):\n",
        "                        print(\"final value\", final_value)\n",
        "                        extracted[field] = final_value\n",
        "                    else:\n",
        "                        print(\"final value failed validate\")\n",
        "                        continue\n",
        "                except Exception as inner_e:\n",
        "                    print(f\"‚ö†Ô∏è Field extraction failed for '{field}': {inner_e}\")\n",
        "                    continue\n",
        "\n",
        "            return True, extracted if extracted else {}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è extract_fields failed: {e}\")\n",
        "            return False, {}\n",
        "\n",
        "\n",
        "    # ------------ SECTION GENERATION ------------ #\n",
        "    def create_sections(\n",
        "        self, shop_name: str, shop_type: str, aggregate: str,\n",
        "        approved_sections: list[str], city: str | None = None, state: str | None = None\n",
        "    ) -> tuple[bool, dict]:\n",
        "        def _generate(section: str, prompt: str) -> str | None:\n",
        "            raw = self._run4(prompt).strip()\n",
        "            if self.validate_section_html(shop_name, section, raw):\n",
        "                print(\"validated\")\n",
        "                return raw\n",
        "            fixed = self.fix_section_html(shop_name, section, raw)\n",
        "            print(\"fixed html\", fixed)\n",
        "\n",
        "            return fixed if fixed and self.validate_section_html(shop_name, section, fixed) else None\n",
        "\n",
        "        location = f\"in {city}, {state}\" if city or state else \"\"\n",
        "        base_instr = (\n",
        "            f\"You are writing for nearestdoor.com about our listing, about the {shop_type} '{shop_name}' {location}. \"\n",
        "            f\"You will get a summary of this place and write useful information according to your assignment which consumers will read as you write it on the nearestdoor.com website listing page for {shop_name}, no bad info or bad formatting. Urls will be https://example.com formatted.\"\n",
        "            \"Be factual, SEO-friendly, help the users learn use this place and learn about it. no unrelated info, no HTML and no asterisks. NO ASTERISKS, NO **, nothing else, nothing before or after our content. DO NOT USE * \"\n",
        "        )\n",
        "\n",
        "        sections = {}\n",
        "        try:\n",
        "            print(\"approved sections\", approved_sections)\n",
        "            if \"article\" in approved_sections:\n",
        "                prompt = f\"{base_instr}\\n\\nContent:\\n{aggregate}\\n\\nAssignment: Write a detailed article. DO NOT USE ASTERISKS, DO NOT USE * OR **. Write an article about {shop_name} for nearestdoor.com.\"\n",
        "                result = _generate(\"article\", prompt)\n",
        "\n",
        "                if result:\n",
        "                    print(\"article\", result)\n",
        "                    sections[\"article\"] = result\n",
        "\n",
        "            if \"faq\" in approved_sections:\n",
        "                prompt = f\"{base_instr}\\n\\nContent:\\n{aggregate}\\n\\nAssignment: Write a detailed FAQ. DO NOT USE ASTERISKS, DO NOT USE * OR **. Write an FAQ about {shop_name} for nearestdoor.com.\"\n",
        "                result = _generate(\"faq\", prompt)\n",
        "                if result:\n",
        "                    print(\"faq\", result)\n",
        "                    sections[\"faq\"] = result\n",
        "\n",
        "            if \"history\" in approved_sections:\n",
        "                prompt = f\"{base_instr}\\n\\nContent:\\n{aggregate}\\n\\nAssignment: DO NOT USE ASTERISKS. DO NOT USE * OR **. Write the history section about {shop_name} for nearestdoor.com.\"\n",
        "                result = _generate(\"history\", prompt)\n",
        "                if result:\n",
        "                    print(\"history\", result)\n",
        "                    sections[\"history\"] = result\n",
        "\n",
        "            return True, sections\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå create_sections error: {e}\")\n",
        "            return False, {}\n",
        "\n",
        "    # ------------ FULL WORKFLOW ------------ #\n",
        "    def process(\n",
        "        self, shop_name: str, shop_type: str, aggregate: str,\n",
        "        city: str | None = None, state: str | None = None\n",
        "    ) -> dict:\n",
        "        result = {\"plan\": [], \"sections\": {}, \"fields\": {}}\n",
        "        print(\"got aggregate checking quality\", aggregate)\n",
        "        if not self.check_aggregate_quality(shop_name, aggregate, shop_type, city, state):\n",
        "            print(\"‚ùå Aggregate failed quality check.\")\n",
        "            return None\n",
        "        print(\"making plan\")\n",
        "        ok, plan = self.create_plan(aggregate, shop_name, shop_type, city, state)\n",
        "        if not ok or not plan:\n",
        "            print(\"‚ùå No sections can be created.\")\n",
        "            return None\n",
        "        print(\"plan\", plan)\n",
        "        result[\"plan\"] = plan\n",
        "        print(\"making sections\")\n",
        "        ok, sections = self.create_sections(shop_name, shop_type, aggregate, city, state)\n",
        "        if ok:\n",
        "            result[\"sections\"] = sections\n",
        "        print('sections', sections)\n",
        "        print(\"making fields\")\n",
        "        ok, available = self.extract_available_fields(aggregate, shop_name, shop_type, city, state)\n",
        "        print(\"avaliabale\", available)\n",
        "        if ok and available:\n",
        "            print(\"making fields\")\n",
        "            ok, fields = self.extract_fields(aggregate, available,shop_name, shop_type,city, state)\n",
        "\n",
        "            if ok:\n",
        "                print(\"fields\", fields)\n",
        "                result[\"fields\"] = fields\n",
        "\n",
        "        return result\n",
        "\n",
        "import re\n",
        "import json\n",
        "\n",
        "\n",
        "def is_non_empty_string(value) -> bool:\n",
        "    return isinstance(value, str) and len(value.strip()) > 0\n",
        "\n",
        "def is_valid_json(value) -> bool:\n",
        "    if isinstance(value, (dict, list)):\n",
        "        return True\n",
        "    try:\n",
        "        json.loads(value)\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def is_valid_phone(value) -> bool:\n",
        "    if not isinstance(value, str):\n",
        "        return False\n",
        "    return bool(re.fullmatch(r\"\\d{3}-\\d{3}-\\d{4}\", value.strip()))\n",
        "\n",
        "def is_valid_email(value) -> bool:\n",
        "    if not isinstance(value, str):\n",
        "        return False\n",
        "    return bool(re.fullmatch(r\"[^@\\s]+@[^@\\s]+\\.[a-zA-Z0-9]+\", value.strip()))\n",
        "\n",
        "def is_valid_url(value) -> bool:\n",
        "    return isinstance(value, str) and value.strip().lower().startswith(\"http\")\n",
        "\n",
        "def is_valid_dict(value) -> bool:\n",
        "    if isinstance(value, dict):\n",
        "        return True\n",
        "    try:\n",
        "        return isinstance(json.loads(value), dict)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def is_valid_list(value) -> bool:\n",
        "    if isinstance(value, list):\n",
        "        return True\n",
        "    try:\n",
        "        return isinstance(json.loads(value), list)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def is_positive_integer_or_string(value) -> bool:\n",
        "    try:\n",
        "        return int(str(value).strip()) > 0\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "FIELD_VALIDATORS = {\n",
        "    # Contact Info\n",
        "    \"extract_phone\": is_valid_phone,\n",
        "    \"extract_email\": is_valid_email,\n",
        "    \"extract_website\": is_valid_url,\n",
        "\n",
        "    # Structured Fields\n",
        "    \"extract_categories\": is_valid_list,\n",
        "    \"extract_operating_hours\": is_valid_dict,\n",
        "    \"extract_holiday_hours\": is_valid_dict,\n",
        "    \"extract_delivery_services\": is_valid_list,\n",
        "    \"extract_social_media\": is_valid_dict,\n",
        "    \"extract_stocked_brands\": is_valid_list,\n",
        "    \"extract_inventory_categories\": is_valid_dict,\n",
        "    \"extract_customer_reviews\": is_valid_list,\n",
        "\n",
        "    # Event / Misc\n",
        "    \"extract_admission\": is_non_empty_string,\n",
        "    \"extract_date_available\": is_non_empty_string,\n",
        "    \"extract_attendance_amount\": is_positive_integer_or_string,\n",
        "    \"extract_exhibitor_amount\": is_positive_integer_or_string,\n",
        "}\n",
        "FIELD_EXTRACTORS = {\n",
        "    # Contact Information\n",
        "    \"extract_phone\": (\n",
        "        \"Extract ONLY the phone number in this format: 727-237-2132. \"\n",
        "        \"Return ONLY the number, no quotes, no text, no comments, no markup.\"\n",
        "    ),\n",
        "    \"extract_email\": (\n",
        "        \"Extract ONLY the email address. Example: example@mail.com. \"\n",
        "        \"Return ONLY the email address, no quotes, no text, no extras.\"\n",
        "    ),\n",
        "    \"extract_website\": (\n",
        "        \"Extract ONLY the official website URL. Must be the offical url for this business/place or return none. Example: https://website.com. \"\n",
        "        \"Return ONLY the URL, no quotes, no text, no markup.\"\n",
        "    ),\n",
        "\n",
        "    # JSON / Structured Fields\n",
        "    \"extract_categories\": (\n",
        "        \"Extract ONLY the SHORT product/service categories in JSON list format. \"\n",
        "        \"Example: ['Thrift Store', 'Charity']. Return ONLY the JSON array.\"\n",
        "    ),\n",
        "    \"extract_operating_hours\": (\n",
        "        \"Extract ONLY weekly operating hours in JSON dictionary format. \"\n",
        "        \"Example: {'monday': '9:00 AM - 5:00 PM', 'sunday': 'Closed'}. \"\n",
        "        \"Return ONLY the JSON object.\"\n",
        "    ),\n",
        "    \"extract_holiday_hours\": (\n",
        "        \"Extract ONLY holiday-specific hours in JSON dictionary format. \"\n",
        "        \"Example: {'2024-12-25': 'Closed', '2024-12-31': '10:00 AM - 4:00 PM'}. \"\n",
        "        \"Return ONLY the JSON object.\"\n",
        "    ),\n",
        "    \"extract_delivery_services\": (\n",
        "        \"Extract ONLY available delivery services in JSON list format. \"\n",
        "        \"Example: ['Uber Eats', 'Self Delivery']. Return ONLY the JSON array.\"\n",
        "    ),\n",
        "    \"extract_social_media\": (\n",
        "        \"Extract ONLY social media links in JSON dictionary format. \"\n",
        "        \"Example: {'facebook': 'https://facebook.com/example', 'instagram': 'https://instagram.com/example'}. \"\n",
        "        \"Return ONLY the JSON object.\"\n",
        "    ),\n",
        "    \"extract_stocked_brands\": (\n",
        "        \"Extract ONLY stocked brands in JSON list format. \"\n",
        "        \"Example: ['Nike', 'Adidas']. Return ONLY the JSON array.\"\n",
        "    ),\n",
        "    \"extract_inventory_categories\": (\n",
        "        \"Extract ONLY inventory categories in JSON dictionary format. \"\n",
        "        \"Example: {'Apparel': ['Shirts', 'Hoodies']}. Return ONLY the JSON object.\"\n",
        "    ),\n",
        "    \"extract_customer_reviews\": (\n",
        "        \"Extract ONLY customer reviews in JSON list format. \"\n",
        "        \"Example: [{'user': 'John', 'comment': 'Great store!', 'rating': 5}]. \"\n",
        "        \"Return ONLY the JSON array.\"\n",
        "    ),\n",
        "\n",
        "    # Event / Scheduling\n",
        "    \"extract_admission\": (\n",
        "        \"Extract ONLY the admission cost or entry fee. Return ONLY the plain text, no prefixes or suffixes.\"\n",
        "    ),\n",
        "    \"extract_date_available\": (\n",
        "        \"Extract ONLY the available date range or date description. \"\n",
        "        \"Example: 'Available from May 1st to June 30th'. Return ONLY the plain text.\"\n",
        "    ),\n",
        "    \"extract_attendance_amount\": (\n",
        "        \"Extract ONLY the expected attendance as a number. Example: 500. Return ONLY the number or numeric string.\"\n",
        "    ),\n",
        "    \"extract_exhibitor_amount\": (\n",
        "        \"Extract ONLY the expected number of exhibitors. Example: 12. Return ONLY the number or numeric string.\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "class NearestDoorClient:\n",
        "    def __init__(self, smartypants, lookup_engine, ollama,  client_id=CLIENT_ID, api_base=API_BASE):\n",
        "        self.client_id = client_id\n",
        "        self.api_base = api_base\n",
        "        self.ollama = ollama\n",
        "        self.lookup_engine = lookup_engine\n",
        "\n",
        "        self.last_heartbeat = 0\n",
        "\n",
        "\n",
        "        self.smartypants = smartypants\n",
        "\n",
        "    def _api_get(self, endpoint, params=None):\n",
        "        try:\n",
        "            print(f\"üì° GET ‚Üí {endpoint}\")\n",
        "            response = requests.get(f\"{self.api_base}{endpoint}\", params=params or {}, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            return response\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"‚ùå GET failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _api_post(self, endpoint, data):\n",
        "        try:\n",
        "\n",
        "            print(f\"üì° POST ‚Üí {endpoint} {data}\")\n",
        "            response = requests.post(f\"{self.api_base}{endpoint}\", json=data, timeout=30)\n",
        "            response.raise_for_status()\n",
        "            return response\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"‚ùå POST failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_task(self):\n",
        "        res = self._api_get(\"/next-task\", params={\"client_id\": self.client_id})\n",
        "        if res and res.status_code == 200:\n",
        "            task = res.json()\n",
        "            if isinstance(task, dict) and \"task_id\" in task:\n",
        "                return task\n",
        "            print(f\"‚ö†Ô∏è Invalid task structure received: {task}\")\n",
        "        return None\n",
        "\n",
        "    def send_heartbeat(self, current_task_id=None):\n",
        "        data = {\"client_id\": self.client_id}\n",
        "        if current_task_id:\n",
        "            data[\"task_id\"] = current_task_id\n",
        "        self._api_post(\"/heartbeat\", data)\n",
        "        print(\"ü´Ä Heartbeat sent.\")\n",
        "\n",
        "    async def handle_task(self, task):\n",
        "        task_id =task.get(\"task_id\")\n",
        "        task_type =task.get(\"task_type\")\n",
        "        if not task_id or not task_type:\n",
        "            print(\"‚ùå Invalid task format.\")\n",
        "            return\n",
        "\n",
        "        print(f\"‚ñ∂Ô∏è Handling task {task_type} (ID: {task_id})\")\n",
        "\n",
        "        result, summary, mainstring, images = False, None, None, None\n",
        "        aggregateplan, createdinfo, extractedfields, foundfields = None, None, None, None\n",
        "        print(task)\n",
        "        name = task['target'].get(\"name\")\n",
        "\n",
        "        slug = task['target'].get(\"slug\")\n",
        "        city = task['target'].get(\"city\")\n",
        "        state = task['target'].get(\"state\")\n",
        "\n",
        "        print(\"SHOP SLUG\",{city}, {state}, {slug})\n",
        "        website_url = task['target'].get(\"website\", None)\n",
        "\n",
        "        shop_type = task['target'].get(\"shop_type\")\n",
        "        aggregate = task['target'].get(\"aggregate\", \"\")\n",
        "        plan = task['target'].get(\"plan\", [])\n",
        "        fields = task['target'].get(\"fields\", [])\n",
        "\n",
        "        match task_type:\n",
        "            case \"search\":\n",
        "                result, mainstring, images = await self.lookup_engine.combined_search(name, city, state, shop_type, website_url)\n",
        "                result = str(result)\n",
        "            case \"aggregate\":\n",
        "\n",
        "                summarizer = ContentSummarizer(self.ollama, name, shop_type, city, state)\n",
        "                summary, result = summarizer.summarize_content(aggregate)\n",
        "\n",
        "\n",
        "            case \"createplan\":\n",
        "                print(\"creating plan\", aggregate)\n",
        "                result, aggregateplan = self.smartypants.create_plan(aggregate, name, shop_type, city, state)\n",
        "                print(\"plan\", aggregateplan)\n",
        "            case \"create\":\n",
        "                print(\"creating\", aggregate)\n",
        "                result, createdinfo = self.smartypants.create_sections(name, shop_type, aggregate, plan, city, state)\n",
        "                print(\"created\", createdinfo)\n",
        "            case \"find_available_fields\":\n",
        "                print(\"finding fields\", aggregate)\n",
        "                if aggregate != '':\n",
        "                  result, foundfields = self.smartypants.extract_available_fields(aggregate, name, shop_type, city, state)\n",
        "                else:\n",
        "\n",
        "                  print(\"NONE AGGREGATE FINDfIELDS\")\n",
        "                  result = False\n",
        "                  foundfields = []\n",
        "                print(\"fields\", foundfields)\n",
        "            case \"extract_fields_from_aggregate\":\n",
        "                print(\"extracting fields\", aggregate)\n",
        "                if aggregate != '':\n",
        "\n",
        "                  result, extractedfields = self.smartypants.extract_fields(aggregate, fields, name, shop_type, city, state)\n",
        "                else:\n",
        "                  print(\"NONE AGGREGATE EXTRACTFIELDS\")\n",
        "                  result = False\n",
        "                  extractedfields = {}\n",
        "                print(\"extracted\", extractedfields)\n",
        "            case _:\n",
        "                print(f\"‚ùå Unknown task type: {task_type}\")\n",
        "                return\n",
        "        if result:\n",
        "                print(f\"üì§ Submitting result for {task_type} ({task_id})\")\n",
        "                try:\n",
        "                    if task_type == 'search':\n",
        "                        res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"mainstring\": mainstring, \"client_id\": CLIENT_ID})\n",
        "\n",
        "                    if task_type == 'aggregate':\n",
        "                        if summary:\n",
        "                            res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"summary\": summary, \"client_id\": CLIENT_ID})\n",
        "                        else:\n",
        "                            print(\"nosummary\")\n",
        "                            res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"fail\", \"client_id\": CLIENT_ID})\n",
        "\n",
        "\n",
        "                    if task_type == 'createplan':\n",
        "                        res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"aggregateplan\": aggregateplan, \"client_id\": CLIENT_ID})\n",
        "                    if task_type == 'create':\n",
        "                        res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"createdinfo\":createdinfo, \"client_id\": CLIENT_ID})\n",
        "                    if task_type == 'find_available_fields':\n",
        "\n",
        "                        res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"foundfields\": foundfields, \"client_id\": CLIENT_ID})\n",
        "                    if task_type == 'extract_fields_from_aggregate':\n",
        "                        res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"success\", \"extractedfields\": extractedfields, \"client_id\": CLIENT_ID})\n",
        "\n",
        "                    print(f\"Server responded: {res.status_code} - {res.text}\")\n",
        "                    if res:\n",
        "                      if res.status_code == 200:\n",
        "                          print(f\"‚úÖ Submitted: {task_type}\")\n",
        "                      else:\n",
        "                          print(f\"‚ùå Submit failed: {task_type} - {res.status_code}\")\n",
        "                    else:\n",
        "                      print(f\"‚ùå Submit failedd: {task_type} - {res.status_code}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Submit exception: {e}\")\n",
        "        else:\n",
        "\n",
        "            print(f\"Submit Failure {task_type}\")\n",
        "            res = self._api_post(f\"/submit/{task_id}\", {\"status\": \"fail\", \"client_id\": CLIENT_ID})\n",
        "\n",
        "            if res:\n",
        "                if res.status_code == 200:\n",
        "                          print(f\"failed: {task_type}\")\n",
        "                else:\n",
        "                          print(f\"‚ùå couldnt failed: {task_type} - {res.status_code}\")\n",
        "            else:\n",
        "                print(\"NO RES\")\n",
        "\n",
        "    async def run(self):\n",
        "\n",
        "\n",
        "        try:\n",
        "            while True:\n",
        "                task = self.get_task()\n",
        "                if task:\n",
        "                    now = time.time()\n",
        "                    if now - self.last_heartbeat > HEARTBEAT_INTERVAL:\n",
        "                        self.send_heartbeat(task.get(\"task_id\"))\n",
        "                        self.last_heartbeat = now\n",
        "                    await self.handle_task(task)\n",
        "                else:\n",
        "                    print(\"‚è≥ No task available, sleeping...\")\n",
        "                    await asyncio.sleep(10)\n",
        "        finally:\n",
        "            print(\"main error\")\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    import asyncio\n",
        "    import nest_asyncio\n",
        "\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "    ollama = OllamaRunner()\n",
        "    smartypants = Smartypants(ollama)\n",
        "    lookup_engine = LookupEngine( ollama)  # Proper initialization\n",
        "\n",
        "    async def main():\n",
        "\n",
        "        client = NearestDoorClient(smartypants, lookup_engine, ollama)\n",
        "        await client.run()\n",
        "\n",
        "    try:\n",
        "\n",
        "        asyncio.run(main())\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nüõë Shutting down gracefully...\")\n",
        "        sys.exit(0)"
      ],
      "metadata": {
        "id": "g-2rqy_rMlo7",
        "outputId": "5f8454e5-4b18-4b3a-ee1c-b7884525fe55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            "üì° GET ‚Üí /next-task\n",
            "üì° POST ‚Üí /heartbeat {'client_id': 'client001', 'task_id': 31160}\n",
            "ü´Ä Heartbeat sent.\n",
            "‚ñ∂Ô∏è Handling task aggregate (ID: 31160)\n",
            "{'task_id': 31160, 'task_type': 'aggregate', 'object_type': 'shop', 'data': {}, 'target': {'id': 32297, 'name': 'Giant springs state park', 'city': 'Great Falls', 'state': 'Montana', 'website': None, 'slug': 'giant-springs-state-park', 'shop_type': 'Dog Park', 'aggregate': '{\\'search\\': {\\'mainstring\\': \\'\\\\n‚Üê\\\\xa0Google SEARCH\\\\xa0DATA\\\\xa0SITE 2 FROM: https://fwp.mt.gov/giant-springs\\\\n Bring the whole family to enjoy this park\\\\\\'s unique geologic features and abundant recreational opportunities. First recorded by the Lewis and Clark Expedition in 1805, it is one of the largest freshwater springs in the country. \\\\n### Reserve a Day Use Facility\\\\nCome marvel at this day-use (no camping available) park‚Äôs remarkable features. Located just outside of Great Falls and encompassing nearly 14 miles of Missouri River shoreline, Giant Springs State Park has something for everyone. The park provides opportunities for hiking, biking, picnicking, photography, fishing, hunting, boating, and bird watching. Experience the Giant Springs, which is one of the largest freshwater springs in the county, producing over 156 million gallons of water each day. Listen to the tranquil sounds of the water cascading into the Roe River, once listed in the Guinness Book of World Records as the world‚Äôs shortest river. Visit the Giant Springs Fish Hatchery and take a walk through the visitor center or feed the fish in the show pond. Find a place to picnic among the towering silver poplars, mature blue spruce or on the manicured lawn. Bring your binoculars or camera and enjoy the park‚Äôs exceptional birding opportunities.\\\\nIf you are looking for something a little more active, Giant Springs provides excellent fishing opportunities along the Missouri River as well as a separate fishing pond. Take a walk, go for a run or ride your bike along the park‚Äôs 30+ miles of paved and dirt single-track trails, which range in difficulty from beginner to expert. The trails are all part of the larger System, which consists of nearly 60 miles of trail around the Great Falls area. Take a walk or drive to the four Missouri River waterfalls (five, if you count the now buried Colter Falls) that are all located within the park. Giant Springs is rich with history that includes the Lewis and Clark Expedition, silver smelting and Paris Gibson, the founder of Great Falls.\\\\nWhether you come to picnic, hike the trails, fish the Missouri, or simply admire the springs, Giant Springs State Park has exciting opportunities for the whole family!\\\\n[View an overview map of Giant Springs State Park, with points of interest, here.](https://fwp.mt.gov/binaries/content/assets/fwp/stateparks/maps/giant-springs-map-with-interest-points-7.31.19.pdf)\\\\n[Spring 2025 Programs Event Schedule](https://fwp.mt.gov/binaries/content/assets/fwp/stateparks/giant-springs/spring-programs-event-schedule.pdf)\\\\n\\\\n\\\\n[Spring 2025 Programs Event Schedule](https://fwp.mt.gov/binaries/content/assets/fwp/stateparks/giant-springs/spring-programs-event-schedule.pdf)\\\\nSome amenities are seasonal. Check with the park for availability.\\\\n\\\\n\\\\n[Information on Hosting Weddings in Giant Springs State Park](https://myfwp.mt.gov/getRepositoryFile?objectID=106241) (PDF)\\\\nHours listed below are normal operating hours and may not apply when there is a special restriction or closure. Check Alerts and Closures in the tab below.\\\\nDAY USE ONLY Open all year Sunrise - Sunset\\\\n[Visit the State Parks Fees & Regulations page](https://fwp.mt.gov/buyandapply/fees-and-general-information) for park rules.\\\\nSee a current list of on State Parks and other FWP lands that may be affected by floods, fire, drought or major maintenance activities. Other Restrictions\\\\n**Trapping is not permitted in Giant Springs State Park** , this includes the Lewis and Clark Heritage Greenway Conservation Easement. \\\\n**Commercial photography requires a commercial use permit**.\\\\n  * For more information, (PDF)\\\\n  * To apply, complete this (PDF)\\\\n\\\\n\\\\n  * [View the Spring 2025 Giant Springs Volunteer Event Schedule](https://fwp.mt.gov/binaries/content/assets/fwp/stateparks/giant-springs/gssp-spring-2025-volunteer-event-schedule.pdf) (PDF)\\\\n\\\\n\\\\nContact Park Manager, Steffen Janikula, with questions.\\\\nGiant Springs State Park 4803 Giant Springs Rd Great Falls, MT 59405\\\\nFor complete position descriptions, application forms, and details about Montana State Parks volunteer programs, \\\\n  * Montana residents who pay the $9 state parks fee with their annual vehicle registration have no daily entrance fees to state parks. For residents who don\\\\\\'t include this in their vehicle registration, non-resident day use fees apply.\\\\n\\\\n\\\\n  * Day use entrance fee with a vehicle: \\\\n  * Day use entrance fee as a walk-in, bicycle or bus passenger: \\\\n\\\\n\\\\nGiant Springs State Park 4803 Giant Springs Rd Great Falls, MT 59405\\\\nEnjoy this aerial tour of Giant Springs State Park\\\\nExperience the Giant Springs, which is one of the largest freshwater springs in the county, producing over 156 million gallons of water each day. Dip your toes in the bubbling pool on a hot summer day or witness the foggy mist rise off the water in the depths of winter. Listen to the tranquil sounds of the water cascading into the Roe River, once listed in the Guinness Book of World Records as the world‚Äôs shortest river.\\\\n[ View the Spring 2025 Giant Springs Program Schedule  ](https://fwp.mt.gov/binaries/content/assets/fwp/stateparks/giant-springs/spring-programs-event-schedule.pdf)\\\\n4803 Giant Springs Rd.Great Falls, MT 59405Latitude/Longitude:(47.534 / -111.227)\\\\nView the park map to learn more about amenities, parking and camping details\\\\nLearn about fees and regulations for all Montana State Parks\\\\nSteffen Janikula is the manager of Ackley Lake and Giant Springs state parks. Steffen attended college at Jamestown College, in Jamestown North Dakota as well as at the University of Montana. Steffen brings over 18 years of experience from the City of Great Falls where he worked in Parks and Recreation, Forestry, Natural Resources, and the Trails Division. Steffen spent the last 7+ years as the City of Great Falls‚Äô Trails Coordinator where he helped establish the Trails Division and oversaw a significant portion of the Rivers Edge Trail System.\\\\nFWP Region 4 Headquarters 4600 Giant Springs Rd Great Falls, MT 59405\\\\nALL EXTRACTED Google SEARCH DATA FOR Giant springs state park, Great Falls\\\\n\\\\n‚Üê\\\\xa0Google SEARCH\\\\xa0DATA\\\\xa0SITE 3 FROM: https://www.dogpackapp.com/parks/united-states/montana/great-falls/giant-springs-state-park\\\\n \\\\n\\\\nSee Gallery (2)\\\\nGQMC+QF Great Falls, Montana, USA\\\\nGiant Springs State Park in Great Falls, Montana, is a fantastic destination for dog owners looking for a day of adventure with their furry friends. This dog-friendly park offers a spacious off-leash area where your pup can run freely and socialize with other dogs. You\\\\\\'ll find beautiful shaded spots to relax, as well as numerous dog-friendly trails for you and your four-legged companion to explore together. With free entry and convenient free parking nearby, accessing the park is hassle-free, allowing you to focus on enjoying your time outdoors. In addition to the ample open fields and scenic trails, the park features a swimming area, perfect for those warmer days when your dog could use a refreshing dip. While there are no specific amenities for separating big and small dogs or mandatory...  Giant Springs State Park in Great Falls, Montana, is a fantastic destination for dog owners looking for a day of adventure with their furry friends. This dog-friendly park offers a spacious off-leash area where your pup can run freely and socialize with other dogs. You\\\\\\'ll find beautiful shaded spots to relax, as well as numerous dog-friendly trails for you and your four-legged companion to explore together. With free entry and convenient free parking nearby, accessing the park is hassle-free, allowing you to focus on enjoying your time outdoors. In addition to the ample open fields and scenic trails, the park features a swimming area, perfect for those warmer days when your dog could use a refreshing dip. While there are no specific amenities for separating big and small dogs or mandatory leashing, the open environment encourages fun and play. Remember to bring your own dog toys, as well as any necessities like poop bags, since these may not be readily available at the park. Giant Springs State Park is not only a great spot for dogs, but it also has rich history and beautiful natural features, including one of the largest springs in the country. The park\\\\\\'s lush landscapes and abundant wildlife make for an engaging outing, whether you\\\\\\'re taking a leisurely stroll or camping under the stars. Make sure to soak in the views of the Missouri River while you‚Äôre there‚Äîit\\\\\\'s an experience you and your pup won\\\\\\'t forget! \\\\nGiant Springs State Park in Great Falls, Montana, is a fantastic destination for dog owners looking for a day of adventure with their furry friends. T...  Giant Springs State Park in Great Falls, Montana, is a fantastic destination for dog owners looking for a day of adventure with their furry friends. This dog-friendly park offers a spacious off-leash area where your pup can run freely and socialize with other dogs. You\\\\\\'ll find beautiful shaded spots to relax, as well as numerous dog-friendly trails for you and your four-legged companion to explore together. With free entry and convenient free parking nearby, accessing the park is hassle-free, allowing you to focus on enjoying your time outdoors. In addition to the ample open fields and scenic trails, the park features a swimming area, perfect for those warmer days when your dog could use a refreshing dip. While there are no specific amenities for separating big and small dogs or mandatory leashing, the open environment encourages fun and play. Remember to bring your own dog toys, as well as any necessities like poop bags, since these may not be readily available at the park. Giant Springs State Park is not only a great spot for dogs, but it also has rich history and beautiful natural features, including one of the largest springs in the country. The park\\\\\\'s lush landscapes and abundant wildlife make for an engaging outing, whether you\\\\\\'re taking a leisurely stroll or camping under the stars. Make sure to soak in the views of the Missouri River while you‚Äôre there‚Äîit\\\\\\'s an experience you and your pup won\\\\\\'t forget! \\\\nVote on the Available Amenities\\\\nShare with your friends ‚ù§Ô∏è\\\\nAre you sure you want to delete this Review?\\\\nAre you sure you want to report this Review?\\\\nWooded area of Marcel Laurin Park\\\\nTo navigate, press the arrow keys.\\\\nGPCR+9G Great Falls, Montana, USA ( 2.70 KM away )\\\\nGP9C+9W Great Falls, Montana, USA ( 4.08 KM away )\\\\n### [Pacific Steel & Recycling Trailside Dog Park ](https://www.dogpackapp.com/parks/united-states/montana/great-falls/pacific-steel-recycling-trailside-dog-park)\\\\nGP84+G6 Great Falls, Montana, USA ( 5.32 KM away )\\\\n### [Pacific Steel & Recycling Dog Park ](https://www.dogpackapp.com/parks/united-states/montana/great-falls/pacific-steel-recycling-dog-park)\\\\n800 River Dr N, Great Falls, MT 59401, United States ( 5.35 KM away )\\\\nFMVR+W6 Great Falls, Montana, USA ( 7.48 KM away )\\\\nReport [Name Park] as inaccurate Please include as much details as possible\\\\nThe pin is in the wrong location\\\\nThe park is closed down.\\\\nThis park is no longer dog-friendly\\\\nWe use cookies and similar technologies to personalize content, tailor and measure ads, analyze site performance, and enhance your experience. You can choose which cookies you allow or manage your preferences. By clicking \"Accept All,\" you agree to the use of cookies as described in our \\\\nALL EXTRACTED Google SEARCH DATA FOR Giant springs state park, Great Falls\\\\n\\\\n‚Üê\\\\xa0Google SEARCH\\\\xa0DATA\\\\xa0SITE 5 FROM: https://visitmt.com/listings/general/state-park/giant-springs-state-park\\\\n RECREATION AREA, RIVER, STATE PARK, TRAIL | \\\\nGiant Springs State Park was discovered by the Lewis & Clark Expedition in 1805 and is one of the largest freshwater springs in the county. The spring flows at a rate of 156 million gallons of crystal clear water from the Madison Aquifer per day, the water originates from the Little Belt Mountains and takes approximately 26 years to reach the springs. Amazingly the water is a constant 54 degrees year round, during the cold winter months water vapor covers the air and soaks the trees in the park, which then forms beautiful ice covered trees shining in the sunlight.\\\\nOnce you visit this magnificent State Park you will realize why it is the most visited State Park in Montana, with over 300,000 visits a year. The springs is amazing to witness and view, but there are over 4,500 acres of front and back county to visit as well. The Park encompasses land on the North and South shores of the ‚ÄúMight Mo‚Äù (Missouri River). We have over 20 miles of amazing single track trail to bike, hike and enjoy plus miles of paved trail for the casual bike/walk/birding adventure. The park also has one of the most amazing features of Montana, the ‚ÄúGreat Falls‚Äù. The Great Falls of the Missouri is one of 4 waterfalls within the Park and we also have 5 hydroelectric dams owned by Northwestern Energy. All of the waterfalls and most of the dams are viewable by road and overlooks throughout the Park. The overlooks contain informational kiosks, vault toilets and access to the trails.\\\\nThe Park also contains Giant Springs State Fish Hatchery; Giant Springs State Park is the only State Park with a fish hatchery located within its boundaries. The Hatchery uses water from the springs to fill their raceways. Stop in and learn about the hatchery raised fish and view the monster trout in the show pond and you can even feed them.\\\\nRecreation opportunities are endless within the park, you can: fish, hunt, hike, bike, bird watch or just have a relaxing picnic under the trees. The fishing is phenomenal from Roe Island; you have the springs bubbling out into the Missouri on one side and the springs running down the Roe River on the other side. The Roe River was once named the world‚Äôs shortest river by Guinness Book of World Records. If river fishing is not for you, there is the Family Fishing Pond with 5 docks for easy access and great fishing. If you are a hunter, there are acres of land that you can access, thanks to a conservation easement from Northwestern Energy. There is the opportunity to hunt; waterfowl, upland game birds and deer. If you just like to get out for some peace and quiet, take a stroll down one of the miles of paved or dirt trails, beautiful views of the Missouri around every corner.\\\\nGiant Springs State Park has something for everybody! Stop in today to explore the park and all that is has to offer. We even have reservable group use areas, if you looking to have a wedding, family gathering, birthday party or any type of get together contact us for more information. Hope to see you soon at one of Montana‚Äôs gems.\\\\n  * 4803 Giant Springs Road \\\\n\\\\n\\\\n\\\\n\\\\n\\\\n\\\\n  * Giant Springs State Park is located off US Highway 87 in Great Falls, then 1 mile east on River Drive to Giant Springs Road.Giant Springs State Park is located off US Highway 87 in Great Falls, then 1 mile east on River Drive to Giant Springs Road.\\\\n\\\\n\\\\n### [Sleep Inn and Suites Great Falls ](https://visitmt.com/listing/sleep-inn-and-suites-great-falls)\\\\n### [Double Barrel Coffee House Caf√© ](https://visitmt.com/listing/double-barrel-coffee-house-cafe)\\\\n  * [ Fire Warning Signs & Info ](https://visitmt.com/plan-your-trip/fire-information-for-travelers)\\\\n\\\\n\\\\nThis website uses cookies.\\\\nCookies are small text files that can be used by websites to make a user\\\\\\'s experience more efficient. The law states that we can store cookies on your device if they are strictly necessary for the operation of this site. For all other types of cookies we need your permission. This site uses different types of cookies. Some cookies are placed by third party services that appear on our pages. You can at any time change or withdraw your consent from the Privacy Policy on our website. Learn more about who we are, how you can contact us and how we process personal data in our Privacy Policy. \\\\n  * These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.\\\\n  * These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant adverts on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.\\\\n  * These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly.\\\\n  * These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information.\\\\n\\\\n\\\\nALL EXTRACTED Google SEARCH DATA FOR Giant springs state park, Great Falls\\\\n\\\\n‚Üê\\\\xa0Google SEARCH\\\\xa0DATA\\\\xa0SITE 6 FROM: https://greatfallsmt.net/recreation/trailside-dog-park\\\\n Large and Small Dog Park\\\\n  *     * [Park & Rec Advisory Board](https://greatfallsmt.net/boc-prb)\\\\n  * [Scheels Aim High Big Sky Aquatic and Recreation Center](https://greatfallsmt.net/recreation/scheels-aim-high-big-sky-aquatic-and-recreation-center)\\\\n    * [Aim High Big Sky Fees](https://greatfallsmt.net/recreation/aim-high-big-sky-fees \"Fees for Scheels Aim High Big Sky Aquatic and Recreation Center\")\\\\n    * [Party at the Aquatic and Recreation Center](https://greatfallsmt.net/recreation/party-aquatic-and-recreation-center \"Party Packages at Scheels Aim High Big Sky\")\\\\n  *     * [Electric City Winter Classic Basketball](https://greatfallsmt.net/recreation/electric-city-winter-classic-basketball-tournament \"Electric City Winter Classic Basketball Tournament\")\\\\n  *   *     * [Storm-Damaged Trees -- What You Need To Know](https://greatfallsmt.net/recreation/storm-damaged-trees-what-you-need-know)\\\\n  *   *     *       * [Electric City Water Park Fees](https://greatfallsmt.net/recreation/electric-city-water-park-fees \"Electric City Water Park Fees\")\\\\n  *   *     * [Interactive River\\\\\\'s Edge Trail Map](https://greatfallsmt.net/recreation/interactive-rivers-edge-trail-map)\\\\n    * [Mayhem & South Shore Trails](https://greatfallsmt.net/recreation/mayhem-and-south-shore-trails \"Mayhem and South Shore Trails\")\\\\n\\\\n\\\\n1700 River Drive North P.O. Box 5021 Great Falls, Montana 59403 (406) 771-1265\\\\n> [Great Falls Park and Recreation](https://www.facebook.com/GreatFallsParkandRecreation)\\\\nCity of Great Falls | P.O. Box 5021 | Great Falls, MT 59403\\\\nALL EXTRACTED Google SEARCH DATA FOR Giant springs state park, Great Falls\\\\n\\\\n‚Üê\\\\xa0Google SEARCH\\\\xa0DATA\\\\xa0SITE 7 FROM: https://montanastateparksfoundation.org/parks/giant-springs-state-park/\\\\n Located just outside of Great Falls lies Giant Springs State Park.\\\\nOriginally discovered by Lewis & Clark in 1805, Giant Springs is one of the largest freshwater springs in the country.\\\\nCome marvel at this day-use park\\\\\\'s remarkable features and view the variety of birdlife. Take part in their special events, picnic by the Missouri River, visit the fish hatchery and visitor center, walk along the Rivers Edge Trail, view the nearby Rainbow Falls overlook, or visit the neighboring Lewis and Clark Interpretive Center operated by the U.S. Forest Service.\\\\nThe spring flows at a rate of 156 million gallons of water per day and is always 54 degrees Fahrenheit! Multiple bridges cross the crystal clear water that makes up Giant Springs, allowing visitors to peer in and see the growing vegetation and even an occasional fish!\\\\nWhile you won‚Äôt find any camping at Giant Springs State Park, you won‚Äôt find the park lacking in activities! The Roe River (also found in Giant Springs State Park) was once listed in the Guinness Book for World Records as the world‚Äôs shortest river!\\\\n  * Visit the fish hatchery and visitor center.\\\\n  * Take a walk on the Rivers Edge Trail.\\\\n  * Go on a hike along the many trails hugging the Missouri River.\\\\n  * Take in the Rainbow Falls at the overlook.\\\\n  * Visit the Lewis and Clark Interpretive Center, located nearby and operated by the U.S. Forest Service.\\\\n\\\\n\\\\nThe water found in Giant Springs comes from the Madison Aquifer under the Little Belt Mountains. Because the water stays around 54 degrees year-round, Giant Springs State Park is great to visit regardless of the season!\\\\nIn the winter, the steam rises off of the unfrozen water and birds flock around the warm water, while in the summer, the park is, on average, 20 degrees cooler than the nearby city of Great Falls.\\\\n##  Curious what other visitors have had to say about the park?\\\\n\"This is a must see attraction if you visit Great Falls. The trout hatchery, the kids pond, the amazing views, and excellent fishing right from the park. We had our engagement photos done here as well. Bring a picnic lunch and spend the day.\"\\\\n\"Absolutley beautiful place top visit. Especially during spring and summer time. Trail access is easy from here and is a must see for anyone paying through or visiting.\"\\\\n\"This is by far the best park in Great Falls and has a lot of good history posted around the park. There is a very clear natural spring, a playground for the kids, lots of barbecues and picnic areas. Just don\\\\\\'t leave a mess like some of the tourists do.. throw your trash away and keep this place looking nice.\"\\\\n\"Turned out to be a great Sunday to visit the park. Sun was shining, no wind, 38 degrees out but it felt great. Picnickers we\\\\\\'re about. Nice \"museum\" for the hatchery. Got to feed some really big rainbow trout.\"\\\\n\"This is a must see attraction if you visit Great Falls. The trout hatchery, the kids pond, the amazing views, and excellent fishing right from the park. We had our engagement photos done here as well. Bring a picnic lunch and spend the day.\"\\\\n\"Absolutley beautiful place top visit. Especially during spring and summer time. Trail access is easy from here and is a must see for anyone paying through or visiting.\"\\\\n[ Click Here to View Google Reviews ](https://www.google.com/search?gs_ssp=eJzj4tLP1TcwKknKNSswYLRSNagwNTYxMk60NE02MDJLTjY2tgIKpZkmphikmVsYWpobm1omeUmkZybmlSgUFxRl5qUXKxSXJJakKhQkFmUDAD48FyQ&q=giant+springs+state+park&oq=giant&aqs=chrome.1.69i57j46l5j69i60j69i61.2226j0j9&sourceid=chrome&ie=UTF-8#lrd=0x53423a95c026cc33:0x5f5ad0f78197359b,1,,,)\\\\n  * DAY USE ONLYOpen all yearSunrise - Sunset\\\\n  * 4803 Giant Springs Road Great Falls, MT 59405\\\\n\\\\n\\\\n# Parks That Others Are Visiting\\\\nMedia error: Format(s) not supported or source(s) not found\\\\n[Use Up/Down Arrow keys to increase or decrease volume.](javascript:void\\\\\\\\(0\\\\\\\\);)\\\\nWe use cookies on our website to give you the most relevant experience by remembering your preferences and repeat visits. By clicking ‚ÄúAccept‚Äù, you consent to the use of ALL the cookies.\\\\nDo not sell my personal information.\\\\nThis website uses cookies to improve your experience while you navigate through the website. Out of these, the cookies that are categorized as necessary are stored on your browser as they are essential for the working of basic functionalities of the ...\\\\nNecessary cookies are absolutely essential for the website to function properly. These cookies ensure basic functionalities and security features of the website, anonymously. The cookie is used by cdn services like CloudFare to identify individual clients behind a shared IP address and apply security settings on a per-client basis. It does not correspond to any user ID in the web application and does not store any personally identifiable information.  \\\\n---  \\\\nThis cookie is set by Stripe payment gateway. This cookie is used to enable payment on the website without storing any patment information on a server.  \\\\nThis cookie is set by Stripe payment gateway. This cookie is used to enable payment on the website without storing any patment information on a server.  \\\\nThis cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category \"Analytics\".  \\\\nThe cookie is set by GDPR cookie consent to record the user consent for the cookies in the category \"Functional\".  \\\\nThis cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category \"Other.  \\\\nThe cookie is set by GDPR cookie consent to record the user consent for the cookies in the category \"Advertisement\".  \\\\nThis cookie is set by GDPR Cookie Consent plugin. The cookies is used to store the user consent for the cookies in the category \"Necessary\".  \\\\nThis cookie is set by GDPR Cookie Consent plugin. The cookie is used to store the user consent for the cookies in the category \"Performance\".  \\\\nThis cookie is native to PHP applications. The cookie is used to store and identify a users\\\\\\' unique session ID for the purpose of managing user session on the website. The cookie is a session cookies and is deleted when all the browser windows are closed.  \\\\nThe cookie is set by the GDPR Cookie Consent plugin and is used to store whether or not user has consented to the use of cookies. It does not store any personal data.  \\\\nFunctional cookies help to perform certain functionalities like sharing the content of the website on social media platforms, collect feedbacks, and other third-party features. This cookie is set by the provider Eventbrite. This cookie is used for the functionality of website chat-box function.  \\\\n---  \\\\nThis cookie is set by Tawk.to which is a live chat functionality. The cookie is used to remember users so that previous chats can be linked together to provide better and improved service.  \\\\nPerformance cookies are used to understand and analyze the key performance indexes of the website which helps in delivering a better user experience for the visitors. \\\\nAnalytical cookies are used to understand how visitors interact with the website. These cookies help provide information on metrics the number of visitors, bounce rate, traffic source, etc. This cookie is installed by Google Analytics. The cookie is used to calculate visitor, session, campaign data and keep track of site usage for the site\\\\\\'s analytics report. The cookies store information anonymously and assign a randomly generated number to identify unique visitors.  \\\\n---  \\\\nThis cookie is installed by Google Analytics. The cookie is used to store information of how visitors use a website and helps in creating an analytics report of how the website is doing. The data collected including the number visitors, the source where they have come from, and the pages visted in an anonymous form.  \\\\nAdvertisement cookies are used to provide visitors with relevant ads and marketing campaigns. These cookies track visitors across websites and collect information to provide customized ads. This cookie is set by Facebook to deliver advertisement when they are on Facebook or a digital platform powered by Facebook advertising after visiting this website.  \\\\n---  \\\\nThe cookie is set by Facebook to show relevant advertisments to the users and measure and improve the advertisements. The cookie also tracks the behavior of the user across the web on sites that have Facebook pixel or Facebook social plugin.  \\\\nThis cookie is set by the provider Mautic.This cookie is used for identifying visitor across visits and devices. Mautic cookies are used for supporting marketing activities.  \\\\nThis cookie is set by the provider Mautic.This cookie is used for setting unique ID for visitor, to track visitor across multiple websites inorder to serve them with relevant advertisements. Mautic cookies are used for supporting marketing activities.  \\\\nThis cookie is set by the provider Mautic.This cookie is used for setting unique ID for visitor, to track visitor across multiple websites inorder to serve them with relevant advertisements. Mautic cookies are used for supporting marketing activities.  \\\\nThis cookie is used to a profile based on user\\\\\\'s interest and display personalized ads to the users.  \\\\nOther uncategorized cookies are those that are being analyzed and have not been classified into a category as yet.   \\\\n16 years 8 months 2 days 11 hours\\\\nALL EXTRACTED Google SEARCH DATA FOR Giant springs state park, Great Falls\\\\n\\\\n‚Üê\\\\xa0Google SEARCH\\\\xa0DATA\\\\xa0SITE 8 FROM: https://www.gopetfriendly.com/activities/mt/great-falls/\\\\n It‚Äôs Been A Great Ride! On To The Next Adventure ‚Ä¶\\\\nWhen we launched GoPetFriendly sixteen years ago, our goal was to make it easy for people to travel with their pets. Perhaps you remember what it was like back then ‚Ä¶ when finding places to stay and things to do with your pets was HARD. \\\\nIt wasn‚Äôt that the idea of taking your pets with you on vacation was a new concept. Some people had been doing it for years. But they were the exception. Most people believed that traveling with a pet was extravagant, or weird, or just too difficult. \\\\nThings certainly have changed! Perhaps creating GoPetFriendly helped sparked a shift in thinking. But, over the years, millions of pet lovers have visited the site to plan trips with their furry family members, and that absolutely delights me!\\\\nEventually, as more people began traveling with their pets, businesses recognized the opportunity. Soon hotels, restaurants, and even entire cities began touting the fact that they were pet friendly. They added pet policies and pet friendly directories to their websites. They splashed it on billboards. And hung signs in their windows. \\\\nWalk around most cities these days and you‚Äôre likely to find as many businesses that are pet friendly as those that aren‚Äôt. Maybe even more! It‚Äôs exactly what we hoped for when we started the site.\\\\nAnd it means there‚Äôs not as much need for GoPetFriendly‚Äôs directory of pet friendly businesses.\\\\n## Things Change ‚Ä¶ And So Do We\\\\nAs the world began embracing pet travel, we changed, too. After twelve fantastic years in the motorhome, we‚Äôre happily settled in Bisbee, Arizona. (It‚Äôs very pet friendly, if anyone is looking for a !)\\\\nWith all the time I‚Äôd spent driving the RV suddenly freed up, I went back to school. Two months ago, I completed a certificate program in AutoCAD (the software designers, drafters, and architects use to draw building plans). I also picked up several courses in residential construction while I was there. It was a blast!\\\\nBest first day of school EVER!Cochise College Residential Construction Program\\\\nIt wasn‚Äôt clear where all of these courses would take me at the time. It was just fun to learn more about things that had always fascinated me. And while I was focusing on homework and exams, the online world where GoPetFriendly exists was changing dramatically.\\\\nYou‚Äôd have to be living under a rock to have missed the recent advances in Artificial Intelligence (AI). Admittedly, I have concerns about how it will impact humanity. On the other hand, I also remember seeing a self-checkout at the grocery store for the first time. When I turned and swore to Rod that I‚Äôd never use one, he replied, ‚ÄúWe might be a little young to stop adopting new technology.‚Äù Of course, he was right. Though I still use the self-checkout sparingly. \\\\nBut the fact that AI is already changing how people plan trips is undeniable. The number of human beings landing on GoPetFriendly has dropped significantly during the past year. Why spend your time hunting for options when typing a simple request will get the answer?\\\\nSeriously, how convenient is that?! Of course, you still have to verify that the information provided is correct. But you‚Äôd have to confirm the pet policies found on GoPetFriendly, too! \\\\n### So ‚Äî as they say ‚Äî all good things come to an end.\\\\nIt‚Äôs clear that there‚Äôs now an way to plan trips with pets. So we‚Äôre shutting down the directory of pet friendly places on GoPetFriendly at the end of the month. It served its purpose, and it will free up time for something new!\\\\nWe‚Äôre keeping the blog in place for now. Partially because it‚Äôs home to the millions of memories we made with Ty and Buster. But also because it‚Äôs not something AI can do better. Our paws-on-the-ground research, though a bit dated, still has value. Hopefully it will continue to inspire more people to travel with their pets. And, who knows? Maybe I‚Äôll post occasionally, if I find something especially interesting. \\\\nSeveral years ago, Rod and I bought a 3-acre lot on a mountaintop in southern Colorado. When we found a fantastic house for sale in Bisbee and decided not to build a home here, the dream of building on that lot started taking shape.\\\\nSuddenly, everything I was learning in my classes became useful! And months of research went into finding the best way to build on a slope, in a remote location, where summer only lasts for a few months ‚Ä¶ and winter can be brutal.\\\\nWe‚Äôre planning to break ground in May ‚Äî depending on the weather, of course. The panelized house will be delivered in June, and a crew will complete the exterior of the home. Then we‚Äôll take over, doing the parts we can, and hiring sub-contractors to do what we can‚Äôt.\\\\nMy Sketch Up drawing of what the house will look like.\\\\nI know many of you reading this post have been with us for years. The fact that you‚Äôre still here amazes me. I‚Äôm truly touched by the friendships we‚Äôve made. And I hope you‚Äôll join us on this next adventure! \\\\nI‚Äôve launched a to share our progress. The went up yesterday, explaining why we chose a prefab home. And I‚Äôll be posting a new video every week. And at the very end of each one, I‚Äôm including a little ‚Äútreat‚Äù especially for my GoPetFriendly peeps. \\\\n**Warning:_The video is not good!_** But you long-timers will remember that the first blog posts I wrote weren‚Äôt good either. Everything improves with practice. \\\\n(Visited 79,010 times, 222 visits today)\\\\n* I came for the dog road rtip from palm springs to Berkeley but noticed what you learned while in school. I‚Äôm a carpenter in my 50‚Äôs that is just now needing to commit to deeper learning with autocad or sketchup and get some certs. I wonder how long that took you for that to be professionalized and what your thoughts are now? If you‚Äôre up for sharing‚Ä¶lots of workers are faced with needing to learn computer based drafting. Even if we are good drafts people and drawings are still important, the drafting apps are the standard.Any thoughts or reflections on what you actually use.\\\\n  * Hi there! Thanks for your note. Yes, it seems so many people are in a state of transition now. I took the courses to get my certification in CAD at my local community college and it took three semesters. My classes were in the evenings, which made it easy to work around, and the resident tuition was reasonable. I‚Äôd do it again in a heartbeat. I loved the classes and the opportunities that are now open to me. \\\\nI learned SketchUp on my own, and it‚Äôs my true passion. I find the program very intuitive and easy to use. It‚Äôs not as robust at AutoCAD, and that makes it easier to learn. There‚Äôs nothing I enjoy more than building a 3D model in SketchUp. But, there aren‚Äôt as many job prospects for a construction illustrator as there are for CAD operators. \\\\nI hope that helps you and wish you the very best of luck!\\\\n  * Thanks for your reply! Super helpful! I feel like you could do a blog about skill development and learning and loving sketchup and I‚Äôd subscribe to it. Especially if it was of a project you are going to do. Your house design looks smart and well thought out. In other news, my partner went to Western Herbalism school in Bisbee and I visited briefly. what a great place.\\\\n    * YAY! I‚Äôm happy to help. And yes, Bisbee is truly a gem. We‚Äôre delighted to have landed here. I did do a video on how to use SketchUp to visualize your floor plan. You might get a kick out of it >> Waggin‚Äô trails!\\\\n\\\\n\\\\n* Best wishes and THANK YOU! Your blog posts have been incredibly helpful to me!\\\\n  * I‚Äôm so glad, Sharri. Thanks for your note, and waggin‚Äô trails to you!\\\\n\\\\n\\\\n* Best of luck in all your new adventures! I have loved following you all these years!\\\\n  * Thank you, Doreen! Meeting fantastic folks like you has been one of the best parts of GoPetFriendly!\\\\n\\\\n\\\\n* I have been following you and Rod for a long time when you had Ty and Buster. Then , you found Myles and still following in all your adventures! ‚ù§Ô∏è It Is very understandable you wanna find your passion in doing something else while all this AI is taking over the world! . It was a joyful journey and I really appreciate you letting us know about this hard but necessary transition! Wishing you the best! ‚ù§Ô∏è Thanks for being our most effective and informative tool for so many years! ! Many blessings for all of you!!\\\\n  * Thank you so very much, Awilda. It truly was a joyful journey, and I believe the next one will be as well. Sending all my best wishes to you. <3\\\\n\\\\n\\\\n* Wow! This is big news! It has taken me several days to wrap my head around this. I have to admit, my initial thoughts were very selfish: Will I never get to see pictures of cute Myles again?! Here I am finally retired and now I won‚Äôt have all of Amy‚Äôs great tips and insights on where to go with my boys (my hubby and our 2 Goldens). If only I had taken the time to drive down to Bisbee and introduce myself‚Ä¶‚Ä¶ Finally, my thoughts turned to gratitude for finding your posts so many years ago (I don‚Äôt even remember how long I have been following you!). Your tip on the Deerfield Lodge at Heavenly in Lake Tahoe became one of our favorite spots with our ‚Äúkids‚Äù, CJ & Simon. Through to recently, all the info you posted about Mount Rushmore and the surrounding area helped us on our latest trip checking off a bucket list item. We, too, recently (a year ago January) made a bog move going from spending our entire lives in the Valley of the Sun (Phoenix metro area) to moving up to the relatively small town of Show Low, in Arizona‚Äôs White Mountains. Let me close by saying that I hope your move will make you and Rod and Myles as happy as our move has made us. Thank you for all you have shared over the years!\\\\n  * Congratulations to you, Cathy! We stayed in Show Low a few years ago and really enjoyed the area. Have no fear ‚Ä¶ the blog is staying put. So, all of our travel tips, product reviews, and destination guides will be there for you to enjoy. They might be a little dated. But honestly, how much is the Grand Canyon going to change in our lifetime?! =D I really appreciate your note, and wish you a lovely retirement and many years of happy travels with your pups. If you ever get over to Bisbee, let me know. It would be fun to meet up for coffee.\\\\n    * I would love that! How long will you guys be in Bisbee before your big move? One of the benefits of being retired is that I have a lot more flexibility with my time now.\\\\n      * Hi Cathy! We‚Äôll be leaving Bisbee at the end of May to spend the summer in Colorado completing the house. We‚Äôre planning to be back in Bisbee around the the middle of October to settle back in for the winter and spring. Between now and when we leave, we do have a couple of trip plans. So let me know if it works out for you to come over. My email is \\\\n\\\\n\\\\n* Michael and I are so excited and happy for you & Rod. I can‚Äôt wait to see what happens next! We‚Äôll be watching and cheering from the sidelines! Getting to hang out with you guys for an evening on your trip to OK was a rare treat for us and one of our fondest memories. Hugs from your friends, the Rennels Pack (Bonnie, Michael, Sunday & Cache, R.I.P. Abby)\\\\n  * Thank you, Bonnie! Rod and I very much enjoyed meeting up with you and Michael and the pups as well. I‚Äôm sorry to see that Abby has passed. I hope that the wonderful memories you made together help to heal your hearts. My best wishes that all is going well in Texas. Waggin‚Äô trails!\\\\n\\\\n\\\\n* Congratulations on your next adventure! We are moving to Western Colorado this year and we can‚Äôt wait!\\\\n  * That‚Äôs fantastic, Ashely! Congratulations! I hope your move goes smoothly, and that Colorado is even more fun than you‚Äôre imagining!\\\\n\\\\n\\\\n* I found you almost 15 years ago when my husband and I adopted our first dog. We loved following your adventures with Ty and Buster and used many of your recommendations! Our little Doodle passed a few years ago but now we have Ginger, our spunky mini schnauzer, who loves to travel! Thank you for all the hard work you put into making this blog and community a blessing to us!\\\\n  * Thanks so much for your note, Nichole. This is definitely a bitter-sweet transition. We had so much fun while we were traveling, blogging, and connecting with other pet lovers. And we‚Äôre hoping the next adventure brings us just as much joy. Happy travels with Ginger! There‚Äôs nothing more precious than the memories we make with our pets.\\\\n\\\\n\\\\n* Wow, this is sad, but we get it. We‚Äôve been blogging for fourteen years and our focus has changed over time. We toy with the idea of stopping too. The past few years we have posted a lot about traveling with dogs but nothing like what you have done. We have used your blog as a resource here and there. We met at BlogPaws, I don‚Äôt remember which one(s). We wish you all the best in your new chapter!\\\\n  * Yes, I remember! Though, I don‚Äôt know which one either. Maybe Atlanta? Thanks for your note. Running this site has been such a fun and interesting experience. But after so many years, our life has changed. Myles doesn‚Äôt really like traveling. Rod and I are loving having a home again after being on-the-go for twelve years. And it‚Äôs time to learn something new. I‚Äôm looking forward to the challenge. I wish you all the best with your adventures, too.\\\\n\\\\n\\\\n* I love having my 2 sheltie with me when traveling. I gained so much information from your blog to aid in my travels!! I can‚Äôt say how many times I referenced your information. I will definitely miss the guidance. Thank you for all those years and I wish you the best.\\\\n  * Thank you, Judy. Running GoPetFriendly was an absolute blast, and I‚Äôm so grateful to know that we helped other pet lovers make memories with their furry travel buddies. I appreciate your note and wish you safe travels and waggin‚Äô trails.\\\\n\\\\n\\\\n* Oh my goodness! So happy for you and Rod! We are Arizonians who have happily followed you through your journeys. God bless and safe travels.\\\\n  * Thank you, Carrie! It‚Äôs an exciting new chapter and we‚Äôre really looking forward to it.\\\\n\\\\n\\\\n* Congratulations! We are still working to hit every spot in your book. Only hit 30 out of 49. It‚Äôs a little harder now we aren‚Äôt in the RV full time though! It has been fun when we have tried to recreate Ty and Buster‚Äôs photos with some of our dogs that have since passed. Now we are making new memories with our current pups! Good luck with building the new home! -Roxy, Molly, Pepper, Beacon, Vivian, Shawn and Tricia\\\\n  * Thank you all! And making it to 30 of 49 places is quite an accomplishment! Wishing you happy and safe travels.\\\\n\\\\n\\\\n* Congratulations ‚Äì hope you have incredible success moving forward!!\\\\n  * Thanks so much, Audrey! The support we‚Äôve received from this community over the years has been such a blessing. I‚Äôm excited to see where it takes us next!\\\\n\\\\n\\\\n* Love this for you! Looking forward to following along via YouTube. Congratulations on your new adventure!\\\\n  * Thanks so much, Sue! My motto is ‚ÄúLife is an adventure or nothing at all.‚Äù The thought of learning all the new things in front of me is so exciting. We‚Äôre fastening our seatbelts and seeing what happens next.\\\\n\\\\n\\\\n* Congratulations!! This sounds like a wonderful life! ‚ù§Ô∏è\\\\n  * Thank you, Judy! The one thing I detest more than anything else is being bored. Fortunately, there no chance of that happening! =D\\\\n\\\\n\\\\n* I have lived vicariously through you for a number of years and will continue to. I look forward to your next adventure and hope you love Colorado. My travel buddy crossed the rainbow bridge last week and I can‚Äôt imagine traveling without him, but I hope to find another travel buddy in time.\\\\n  * Ann, I‚Äôm so very sorry. They take a piece of us with them when they go, and my heart breaks for you being so close to that loss. When the time is right, your next travel buddy will be one of the lucky ones. Sending my love to you!\\\\n\\\\n\\\\n* Congrats to you all!!! We can always count on you both, and your critters, to keep us informed, educated and looking forward to new things to do and try. We are excited to see what the next chapter brings! Who knows what that will inspire in all of us this time!! We can‚Äôt wait to tune in! Bruce and Chris\\\\n  * Thanks so much, Bruce and Chris! Perhaps next we‚Äôll all be building pet friendly prefab homes! =D\\\\n\\\\n\\\\n* Amy and Rod! A person from the past. GPF is and always will be a favorite here. Looking forward to the channel and watching the build. What a great adventure! Matt (if you don‚Äôt remember from Carrie‚Äôs TalesAndTails).\\\\n  * Oh my goodness! Of course I remember you, Matt! How wonderful to hear from you. I appreciate the long-time loyalty, and hope you‚Äôre doing well. Sending a big hug and looking forward to connecting again on YouTube.\\\\n\\\\n\\\\n* Wishing you and your little family all the best. Thanks for all of your posts over the years. Like you we have sold our RV and our sweet Bailey crossed the rainbow bridge one year ago. We are still trying to decide if we can go through that kind of loss again.\\\\n  * Oh Donna, I‚Äôm so sorry to hear about Bailey‚Äôs passing. There are no words for that heartbreak. I do hope that the wonderful memories you made together bring you peace. And, if you decide at some point to open your heart to another dog ‚Ä¶ he or she will be one of the lucky ones! Thanks for the note.\\\\n\\\\n\\\\n* Thank you Amy and Rod for sharing experiences, adventure, love, loss and LIFE! Congratulations on your new spot in Bisbee. What a cool spot. XO Erin T\\\\n  * Thank you, Erin! Yes, we‚Äôre absolutely loving Bisbee. If you ever get down this way, we should get together for coffee and catch up!\\\\n\\\\n\\\\n* Go Pet Friendly was such a valuable resource when we drove across country and back with our dog. We really appreciated your info and tips. Good luck on your next adventure!!!\\\\n  * Thank you, Gail! The time I‚Äôve spent on GoPetFriendly has been some of the best of my life. =)\\\\n\\\\n\\\\n* Thanks for helping us get started travelling with our whole furamily! It was you guys that inspired us to go on our 1st big trip from WI to OH, NY & PA! We have been on MANY adventures since & have many more in the works. Best wishes to you all!!!\\\\n  * Woohoo!! Thanks for the note, Casey. It‚Äôs been so rewarding to know that we‚Äôre helping people create wonderful memories with their pets. And seeing how things have changed over time has been fantastic. There is nothing else I would have enjoyed more.\\\\n\\\\n\\\\n* Thank you, Amy! We will look forward to your weekly videos on YouTube. We are interested in doing something similar, though not so remotely. All the best to you, Rod and Myles.\\\\n* Wow! I feel like I‚Äôve know you, or at least followed your adventures, forever! Congratulations on your new adventure! Sounds so exciting!\\\\n  * Thanks so much, Diane! I‚Äôm really excited and am looking forward to learning so many new things!\\\\n\\\\n\\\\n* Congrats on your new chapter! We love YouTube and will be checking out your new space.hopefully you‚Äôll keep including your 4 paw family in some of your uploads\\\\n  * Thanks, Bonnie! And we absolutely will be including puppy paws on the channel. (Myles doesn‚Äôt know it yet, but we have a dog harness for the GoPro, so he‚Äôs going to be taking video, too.) =D\\\\n\\\\n\\\\n* Wow, Amy, Rod and Myles! Sounds great! So you smarties will have a cool spot for summers up there on the mountain top. I have lucked into a summer spot also: in Eugene. Then we can appreciate our warm AZ winters. Looking forward to your YouTube house process. My Callie and I had great car camping adventures in the Forester last summer, will try to send you a photo of her where we camped on the Pacific Coast Trail in CA. All best from former Bisbonian Berta\\\\n  * Berta! It‚Äôs so nice to hear from you! How wonderful that you‚Äôre spending summers in Eugene ‚Äì it‚Äôs a lovely place. Especially in the summer. And I love hearing that you and Callie have been car camping. There are so many great places to visit, and every one is even sweeter with your pet. Safe travels to you two!\\\\n\\\\n\\\\nCategory: \\\\nMake sure to subscribe to our newsletter and be the first to know our latest news and pet friendly activities.\\\\n[ Red Roof¬Æ And GoPetFriendly Partner To Make Traveling With Pets Easier This Summer ](https://www.gopetfriendly.com/blog/red-roof-and-gopetfriendly-partnership/)\\\\n[ What Ever Happened With The Blog Infringement Case? ](https://www.gopetfriendly.com/blog/blog-infringement/)\\\\nThe login page will open in a new tab. After logging in you can close it and return to this page.\\\\nALL EXTRACTED WIKIPEDIA SEARCH INFO FOR Giant springs state park:\\\\n\\\\nWIKI CHUNK 1:\\\\nGiant Springs is a large first-magnitude spring located near Great Falls, Montana and is the central feature of Giant Springs State Park.  Its water has a constant temperature of 54 ¬∞F (12 ¬∞C) and originates from snowmelt in the Little Belt Mountains, 60 miles (97 km) away.  According to chlorofluorocarbon dating, the water takes about 3,000 years to travel underground before returning to the surface at the springs.\\\\nGiant Springs is formed by an opening in a part of the Madison aquifer, a vast a\\\\n\\\\nWIKI CHUNK 2:\\\\nquifer underlying 5 U.S. States and 3 Canadian Provinces. The conduit between the mountains and the spring is the geological stratum found in parts of the northwest United States called the Madison Limestone.  Although some of the underground water from the Little Belt Mountains escapes to form Giant Springs, some stays underground and continues flowing, joining sources from losing streams in the Black Hills, Big Horn Mountains and other areas.  The aquifer eventually surfaces in Canada. Giant S\\\\n\\\\nWIKI CHUNK 3:\\\\nprings has an average discharge of 242 cubic feet (6.9 m3) of water per second or 150 million gallons per day. \\\\n\\\\nThe spring outlet is located in Giant Springs State Park, just downstream and northeast of Great Falls, Montana on the east bank of the Missouri River.  Giant Springs was first described by Lewis and Clark during their exploration of the Louisiana Purchase in 1805. Before that, the Blackfeet people utilized the springs as an easy-to-access water source in the winter. The springs were \\\\n\\\\nWIKI CHUNK 4:\\\\nmostly ignored by settlers until 1884 when the town of Great Falls was established and the springs became the place for Sunday recreational activities. In the mid-1970s the park was established as a Montana State Park. \\\\nToday, some of the spring water is bottled annually for human consumption and some of the discharge is used for a trout hatchery. The hatchery is a Montana state trout hatchery named Giant Springs Trout Hatchery and raises mostly Rainbow Trout. The spring serves as the headwaters\\\\n\\\\nWIKI CHUNK 5:\\\\n of the 200-foot (61 m)-long Roe River, once listed as the shortest river in the world according to Guinness Book of World Records.  The river flows into the Missouri River which is near the spring and borders its state park.\\\\n\\\\n\\\\n== See also ==\\\\nSpring (hydrosphere)\\\\n\\\\n\\\\n== References ==\\\\n\\\\n\\\\n== External links ==\\\\n Media related to Giant Springs State Park at Wikimedia Commons\\\\nState of Montana: Official Giant Springs State Park website\\\\nYoutube: Giant Springs 1\\\\nYoutube: Giant Springs 2\\\\nYoutube: Giant Sprin\\\\n\\\\nWIKI CHUNK 6:\\\\ngs 3\\'}}None'}}\n",
            "SHOP SLUG {'Great Falls'} {'Montana'} {'giant-springs-state-park'}\n",
            "üßπ Cleaning raw content...\n",
            "\n",
            "summarizing\n",
            "\n",
            "filtered less than min\n",
            "None\n",
            "‚ùå Summarization failed. Returning none instead.\n",
            "Submit Failure aggregate\n",
            "üì° POST ‚Üí /submit/31160 {'status': 'fail', 'client_id': 'client001'}\n",
            "‚ùå POST failed: 400 Client Error: Bad Request for url: https://www.nearestdoor.com/submit/31160\n",
            "NO RES\n",
            "üì° GET ‚Üí /next-task\n",
            "‚ñ∂Ô∏è Handling task createplan (ID: 31161)\n",
            "{'task_id': 31161, 'task_type': 'createplan', 'object_type': 'shop', 'data': {}, 'target': {'id': 32297, 'name': 'Giant springs state park', 'city': 'Great Falls', 'state': 'Montana', 'website': None, 'slug': 'giant-springs-state-park', 'shop_type': 'Dog Park', 'aggregate': ''}}\n",
            "SHOP SLUG {'Great Falls'} {'Montana'} {'giant-springs-state-park'}\n",
            "creating plan \n",
            "üß† Running Ollama: gemma3:4b\n",
            "article, faq, history\n",
            "plan ['faq', 'history', 'article']\n",
            "üì§ Submitting result for createplan (31161)\n",
            "üì° POST ‚Üí /submit/31161 {'status': 'success', 'aggregateplan': ['faq', 'history', 'article'], 'client_id': 'client001'}\n",
            "Server responded: 200 - {\"status\": \"success\", \"slug\": \"giant-springs-state-park\"}\n",
            "‚úÖ Submitted: createplan\n",
            "üì° GET ‚Üí /next-task\n",
            "‚ñ∂Ô∏è Handling task create (ID: 31162)\n",
            "{'task_id': 31162, 'task_type': 'create', 'object_type': 'shop', 'data': {}, 'target': {'id': 32297, 'name': 'Giant springs state park', 'city': 'Great Falls', 'state': 'Montana', 'website': None, 'slug': 'giant-springs-state-park', 'shop_type': 'Dog Park', 'aggregate': '', 'plan': ['faq', 'history', 'article']}}\n",
            "SHOP SLUG {'Great Falls'} {'Montana'} {'giant-springs-state-park'}\n",
            "creating \n",
            "approved sections ['faq', 'history', 'article']\n",
            "üß† Running Ollama: gemma3:4b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "main error\n",
            "\n",
            "üõë Shutting down gracefully...\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-34-48410947fab6>\", line 1255, in <cell line: 0>\n",
            "    asyncio.run(main())\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-34-48410947fab6>\", line 1251, in main\n",
            "    await client.run()\n",
            "  File \"<ipython-input-34-48410947fab6>\", line 1231, in run\n",
            "    await self.handle_task(task)\n",
            "  File \"<ipython-input-34-48410947fab6>\", line 1148, in handle_task\n",
            "    result, createdinfo = self.smartypants.create_sections(name, shop_type, aggregate, plan, city, state)\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-34-48410947fab6>\", line 854, in create_sections\n",
            "    result = _generate(\"article\", prompt)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-34-48410947fab6>\", line 833, in _generate\n",
            "    raw = self._run4(prompt).strip()\n",
            "          ^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-34-48410947fab6>\", line 608, in _run4\n",
            "    return self.ollama.run(prompt, model=\"gemma3:4b\",)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-34-48410947fab6>\", line 59, in run\n",
            "    proc = subprocess.run(\n",
            "           ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 550, in run\n",
            "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 1209, in communicate\n",
            "    stdout, stderr = self._communicate(input, endtime, timeout)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/subprocess.py\", line 2115, in _communicate\n",
            "    ready = selector.select(timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-34-48410947fab6>\", line 1258, in <cell line: 0>\n",
            "    sys.exit(0)\n",
            "SystemExit: 0\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1671, in getframeinfo\n",
            "    lineno = frame.f_lineno\n",
            "             ^^^^^^^^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'f_lineno'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-48410947fab6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                     \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/events.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSystemExit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-48410947fab6>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNearestDoorClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmartypants\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_engine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mollama\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m         \u001b[0;32mawait\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-48410947fab6>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1230\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_heartbeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m                     \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-48410947fab6>\u001b[0m in \u001b[0;36mhandle_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"creating\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreatedinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmartypants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_sections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshop_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"created\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreatedinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-48410947fab6>\u001b[0m in \u001b[0;36mcreate_sections\u001b[0;34m(self, shop_name, shop_type, aggregate, approved_sections, city, state)\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{base_instr}\\n\\nContent:\\n{aggregate}\\n\\nAssignment: Write a detailed article. DO NOT USE ASTERISKS, DO NOT USE * OR **. Write an article about {shop_name} for nearestdoor.com.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"article\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-48410947fab6>\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(section, prompt)\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msection\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m             \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_section_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-48410947fab6>\u001b[0m in \u001b[0;36m_run4\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mollama\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gemma3:4b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-48410947fab6>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, prompt, model, timeout)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             proc = subprocess.run(\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0;34m\"ollama\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1208\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m                 \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                     \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-48410947fab6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüõë Shutting down gracefully...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mSystemExit\u001b[0m: 0",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    }
  ]
}